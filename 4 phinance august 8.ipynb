{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdzgJLZTwK0xznq/DINtYh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"-7RJbXIuA80m","executionInfo":{"status":"error","timestamp":1754683738317,"user_tz":240,"elapsed":348,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"b38e6282-ada4-4aba-ab46-15f3d0890b20"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","UNBIASED PATTERN DISCOVERY AND ANALYSIS\n","======================================================================\n","\n","Fetching data for SPY...\n","‚úì Loaded 1216 days of data\n","\n","--------------------------------------------------\n","PHASE 1: DISCOVERING PARAMETERS FROM DATA\n","--------------------------------------------------\n","Discovering parameters from data...\n"]},{"output_type":"error","ename":"ValueError","evalue":"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4066081959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;31m# Run the analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_unbiased_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SPY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2020-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2024-10-31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-4066081959.py\u001b[0m in \u001b[0;36mrun_unbiased_analysis\u001b[0;34m(symbol, start, end, show_details)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m     \u001b[0mdiscoveries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscover_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_details\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4066081959.py\u001b[0m in \u001b[0;36mdiscover_parameters\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         discoveries = {\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;34m'windows'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_discover_optimal_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;34m'lags'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_discover_significant_lags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;34m'attractors'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_discover_attractors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4066081959.py\u001b[0m in \u001b[0;36m_discover_optimal_windows\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;34m'significant_windows'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msignificant_windows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;34m'window_scores'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwindow_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;34m'corrected_pvalues'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrected_pvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcorrected_pvalues\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;34m'special_729'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspecial_729_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;34m'best_window'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwindow_scores\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"]}],"source":["\"\"\"\n","Unbiased Advanced Pattern Evolution Predictor\n","==============================================\n","Rigorous, statistically-validated pattern detection with minimal bias.\n","All thresholds are data-driven, not hardcoded.\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","from scipy import stats, signal, optimize\n","from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n","from sklearn.ensemble import RandomForestRegressor\n","from typing import Dict, List, Tuple, Optional, Any\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class UnbiasedPatternPredictor:\n","    \"\"\"\n","    Statistically rigorous pattern predictor with minimal bias.\n","    All parameters are discovered from data, not assumed.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 confidence_level: float = 0.95,\n","                 min_sample_size: int = 100,\n","                 bootstrap_iterations: int = 1000):\n","        \"\"\"\n","        Initialize with statistical parameters only.\n","\n","        Args:\n","            confidence_level: Statistical confidence level for tests\n","            min_sample_size: Minimum data points for valid analysis\n","            bootstrap_iterations: Number of bootstrap samples for confidence intervals\n","        \"\"\"\n","        # Statistical parameters\n","        self.confidence_level = confidence_level\n","        self.alpha = 1 - confidence_level\n","        self.min_sample_size = min_sample_size\n","        self.bootstrap_iterations = bootstrap_iterations\n","\n","        # Discovered parameters (will be populated from data)\n","        self.discovered_constants = {}\n","        self.discovered_windows = []\n","        self.discovered_lags = []\n","        self.discovered_attractors = {}\n","        self.empirical_thresholds = {}\n","\n","        # Known mathematical constants for reference (not assumed to appear)\n","        self.MATHEMATICAL_CONSTANTS = {\n","            'phi': 1.618033988749895,\n","            'inv_phi': 0.618033988749895,\n","            'inv_phi_squared': 0.381966011250105,\n","            'pi': 3.141592653589793,\n","            'e': 2.718281828459045,\n","            'sqrt_2': 1.414213562373095,\n","            'sqrt_3': 1.732050807568877,\n","            'feigenbaum': 4.669201609102990,\n","            'fine_structure': 137.035999084,\n","            'inv_fine_structure': 0.007297352566\n","        }\n","\n","        # Phase states (discovered empirically)\n","        self.phases = {}\n","\n","    def discover_parameters(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover all parameters from the data without assumptions.\n","        \"\"\"\n","        print(\"Discovering parameters from data...\")\n","\n","        # Ensure we have required columns\n","        if 'Close' not in data.columns:\n","            raise ValueError(\"Data must contain 'Close' column\")\n","\n","        # Calculate returns if not present\n","        if 'returns' not in data.columns:\n","            data['returns'] = data['Close'].pct_change()\n","\n","        discoveries = {\n","            'windows': self._discover_optimal_windows(data),\n","            'lags': self._discover_significant_lags(data),\n","            'attractors': self._discover_attractors(data),\n","            'thresholds': self._discover_thresholds(data),\n","            'constants': self._discover_constants(data),\n","            'phase_boundaries': self._discover_phase_boundaries(data)\n","        }\n","\n","        # Store discoveries\n","        self.discovered_windows = discoveries['windows']['significant_windows']\n","        self.discovered_lags = discoveries['lags']['significant_lags']\n","        self.discovered_attractors = discoveries['attractors']['attractors']\n","        self.empirical_thresholds = discoveries['thresholds']\n","        self.discovered_constants = discoveries['constants']\n","\n","        return discoveries\n","\n","    def _discover_optimal_windows(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover which window sizes show statistically significant patterns.\n","        No assumption about 729, 137, etc.\n","        \"\"\"\n","        prices = data['Close'].values\n","        n = len(prices)\n","\n","        # Test a wide range of windows\n","        min_window = max(10, n // 100)\n","        max_window = min(n // 2, 1000)\n","        test_windows = np.unique(np.logspace(np.log10(min_window),\n","                                            np.log10(max_window),\n","                                            50).astype(int))\n","\n","        window_scores = {}\n","        window_pvalues = {}\n","\n","        for window in test_windows:\n","            if window >= n:\n","                continue\n","\n","            # Calculate pattern strength for this window\n","            densities = []\n","            for i in range(0, n - window, window // 4):\n","                segment = prices[i:i+window]\n","                if len(segment) > 1:\n","                    binary = (np.diff(segment) > 0).astype(int)\n","                    densities.append(np.mean(binary))\n","\n","            if len(densities) < 3:\n","                continue\n","\n","            # Test for non-randomness using multiple tests\n","            densities = np.array(densities)\n","\n","            # 1. Runs test for randomness\n","            median_density = np.median(densities)\n","            runs = self._count_runs(densities > median_density)\n","            expected_runs = (2 * np.sum(densities > median_density) *\n","                           np.sum(densities <= median_density) / len(densities)) + 1\n","\n","            if expected_runs > 0:\n","                z_score = (runs - expected_runs) / np.sqrt(expected_runs)\n","                p_value_runs = 2 * (1 - stats.norm.cdf(abs(z_score)))\n","            else:\n","                p_value_runs = 1.0\n","\n","            # 2. Kolmogorov-Smirnov test against uniform\n","            _, p_value_ks = stats.kstest(densities, 'uniform', args=(0, 1))\n","\n","            # Combine p-values using Fisher's method (excluding AD test)\n","            combined_stat = -2 * (np.log(p_value_runs + 1e-10) +\n","                                 np.log(p_value_ks + 1e-10))\n","            # Degrees of freedom reduced from 6 to 4 (removed AD test)\n","            combined_pvalue = 1 - stats.chi2.cdf(combined_stat, df=4)\n","\n","            window_scores[window] = 1 - combined_pvalue  # Higher score = more significant\n","            window_pvalues[window] = combined_pvalue\n","\n","        # Apply multiple testing correction\n","        from statsmodels.stats.multitest import multipletests\n","        windows = list(window_pvalues.keys())\n","        pvalues = list(window_pvalues.values())\n","\n","        significant_windows = []\n","        corrected_pvalues = {}\n","\n","        # Add check for sufficient p-values\n","        if len(pvalues) >= 2: # fdr_bh requires at least 2 p-values\n","            rejected, corrected_pvalues_array, _, _ = multipletests(pvalues,\n","                                                             alpha=self.alpha,\n","                                                             method='fdr_bh')\n","            significant_windows = [w for w, r in zip(windows, rejected) if r.any()]\n","            corrected_pvalues = dict(zip(windows, corrected_pvalues_array))\n","\n","\n","        # Special test for 729 if it's in range\n","        special_729_result = None\n","        if 729 < n:\n","            special_729_result = self._test_specific_window(data, 729)\n","\n","\n","        return {\n","            'significant_windows': significant_windows,\n","            'window_scores': window_scores,\n","            'corrected_pvalues': corrected_pvalues,\n","            'special_729': special_729_result,\n","            'best_window': max(window_scores.items(), key=lambda x: x[1])[0] if window_scores else None\n","        }\n","\n","    def _discover_significant_lags(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover autocorrelation structure without assuming specific lags.\n","        \"\"\"\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < self.min_sample_size:\n","            return {'significant_lags': [], 'acf_values': {}}\n","\n","        # Calculate autocorrelation for many lags\n","        max_lag = min(len(returns) // 4, 100)\n","\n","        from statsmodels.tsa.stattools import acf, pacf\n","        acf_values, acf_confint = acf(returns, nlags=max_lag, alpha=self.alpha)\n","        pacf_values, pacf_confint = pacf(returns, nlags=max_lag, alpha=self.alpha)\n","\n","        # Find significant lags (outside confidence intervals)\n","        significant_lags = []\n","        lag_strengths = {}\n","\n","        for lag in range(1, len(acf_values)):\n","            # Check if significantly different from zero\n","            lower = acf_confint[lag, 0]\n","            upper = acf_confint[lag, 1]\n","\n","            if acf_values[lag] < lower or acf_values[lag] > upper:\n","                significant_lags.append(lag)\n","                lag_strengths[lag] = abs(acf_values[lag])\n","\n","        # Look for lag differences (like 45-42=3 from original)\n","        lag_differences = {}\n","        for i, lag1 in enumerate(significant_lags):\n","            for lag2 in significant_lags[i+1:]:\n","                diff = lag2 - lag1\n","                if diff not in lag_differences:\n","                    lag_differences[diff] = []\n","                lag_differences[diff].append((lag1, lag2))\n","\n","        return {\n","            'significant_lags': significant_lags,\n","            'acf_values': dict(enumerate(acf_values)),\n","            'pacf_values': dict(enumerate(pacf_values)),\n","            'lag_strengths': lag_strengths,\n","            'lag_differences': lag_differences,\n","            'strongest_lag': max(lag_strengths.items(), key=lambda x: x[1])[0] if lag_strengths else None\n","        }\n","\n","    def _discover_attractors(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover natural clustering points in the data.\n","        \"\"\"\n","        prices = data['Close'].values\n","\n","        # Calculate various ratios\n","        ratios = []\n","\n","        # Price ratios\n","        for shift in [1, 5, 20]:\n","            if len(prices) > shift:\n","                ratio = prices[shift:] / prices[:-shift]\n","                ratios.extend(ratio[np.isfinite(ratio)])\n","\n","        # Convert to numpy array\n","        ratios = np.array(ratios)\n","        ratios = ratios[(ratios > 0.1) & (ratios < 10)]  # Remove extreme outliers\n","\n","        if len(ratios) < self.min_sample_size:\n","            return {'attractors': {}, 'cluster_centers': []}\n","\n","        # Use Gaussian Mixture Model to find natural clusters\n","        from sklearn.mixture import GaussianMixture\n","\n","        # Determine optimal number of clusters using BIC\n","        n_components_range = range(1, min(10, len(ratios) // 20))\n","        bic_scores = []\n","\n","        for n_components in n_components_range:\n","            gmm = GaussianMixture(n_components=n_components, random_state=42)\n","            gmm.fit(ratios.reshape(-1, 1))\n","            bic_scores.append(gmm.bic(ratios.reshape(-1, 1)))\n","\n","        # Find optimal number of clusters\n","        optimal_n = n_components_range[np.argmin(bic_scores)]\n","\n","        # Fit final model\n","        gmm = GaussianMixture(n_components=optimal_n, random_state=42)\n","        gmm.fit(ratios.reshape(-1, 1))\n","\n","        cluster_centers = gmm.means_.flatten()\n","\n","        # Check which mathematical constants are near cluster centers\n","        discovered_attractors = {}\n","        for center in cluster_centers:\n","            # Check proximity to known constants\n","            for name, value in self.MATHEMATICAL_CONSTANTS.items():\n","                if abs(center - value) < 0.1:  # Within 10%\n","                    discovered_attractors[f\"near_{name}\"] = center\n","                    break\n","            else:\n","                # Not near any known constant\n","                discovered_attractors[f\"empirical_{center:.3f}\"] = center\n","\n","        return {\n","            'attractors': discovered_attractors,\n","            'cluster_centers': cluster_centers.tolist(),\n","            'n_clusters': optimal_n,\n","            'bic_score': min(bic_scores) if bic_scores else None\n","        }\n","\n","    def _discover_thresholds(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover empirical thresholds from data distribution.\n","        \"\"\"\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < self.min_sample_size:\n","            return {}\n","\n","        # Calculate percentiles for various confidence levels\n","        thresholds = {\n","            'extreme_positive': np.percentile(returns, 99),\n","            'high_positive': np.percentile(returns, 95),\n","            'moderate_positive': np.percentile(returns, 75),\n","            'neutral_high': np.percentile(returns, 55),\n","            'neutral_low': np.percentile(returns, 45),\n","            'moderate_negative': np.percentile(returns, 25),\n","            'high_negative': np.percentile(returns, 5),\n","            'extreme_negative': np.percentile(returns, 1),\n","            'volatility_threshold': np.std(returns),\n","            'mean_return': np.mean(returns)\n","        }\n","\n","        return thresholds\n","\n","    def _discover_constants(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Test which mathematical constants appear more than random chance.\n","        \"\"\"\n","        prices = data['Close'].values\n","\n","        # Calculate various metrics\n","        metrics = []\n","\n","        # Density at different windows\n","        for window in [20, 50, 100, 200]:\n","            if len(prices) > window:\n","                binary = (np.diff(prices[-window:]) > 0).astype(int)\n","                metrics.append(np.mean(binary))\n","\n","        # Volatility ratios\n","        for period1, period2 in [(20, 50), (50, 100), (20, 100)]:\n","            if len(prices) > period2:\n","                vol1 = np.std(prices[-period1:])\n","                vol2 = np.std(prices[-period2:])\n","                if vol2 > 0:\n","                    metrics.append(vol1 / vol2)\n","\n","        # Mean ratios\n","        if len(prices) > 100:\n","            metrics.append(np.mean(prices[-50:]) / np.mean(prices[-100:]))\n","\n","        metrics = np.array(metrics)\n","        metrics = metrics[np.isfinite(metrics)]\n","\n","        if len(metrics) < 5:\n","            return {}\n","\n","        # Test proximity to mathematical constants\n","        constant_distances = {}\n","        for name, value in self.MATHEMATICAL_CONSTANTS.items():\n","            distances = np.abs(metrics - value)\n","            min_distance = np.min(distances)\n","\n","            # Bootstrap test for significance\n","            bootstrap_distances = []\n","            for _ in range(self.bootstrap_iterations):\n","                random_metrics = np.random.uniform(np.min(metrics), np.max(metrics), len(metrics)) # Sample from metric range\n","                bootstrap_distances.append(np.min(np.abs(random_metrics - value)))\n","\n","            # Calculate p-value\n","            p_value = np.mean(np.array(bootstrap_distances) <= min_distance)\n","\n","            constant_distances[name] = {\n","                'min_distance': min_distance,\n","                'p_value': p_value,\n","                'is_significant': p_value < self.alpha\n","            }\n","\n","        return constant_distances\n","\n","    def _discover_phase_boundaries(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Discover natural phase transitions in the data.\n","        \"\"\"\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < self.min_sample_size:\n","            return {}\n","\n","        # Use change point detection\n","        from scipy.signal import find_peaks\n","\n","        # Calculate rolling statistics\n","        window = 20\n","        if len(returns) > window:\n","            rolling_mean = pd.Series(returns).rolling(window).mean().values\n","            rolling_std = pd.Series(returns).rolling(window).std().values\n","            rolling_skew = pd.Series(returns).rolling(window).skew().values\n","\n","            # Combine into phase indicator\n","            phase_indicator = np.nanmean([\n","                np.abs(np.gradient(rolling_mean)),\n","                np.abs(np.gradient(rolling_std)),\n","                np.abs(np.gradient(rolling_skew))\n","            ], axis=0)\n","\n","            # Find peaks in phase indicator (phase transitions)\n","            peaks, properties = find_peaks(phase_indicator[~np.isnan(phase_indicator)],\n","                                         prominence=np.nanstd(phase_indicator))\n","\n","            # Classify phases between transitions\n","            n_phases = len(peaks) + 1\n","            phase_boundaries = peaks.tolist() if len(peaks) > 0 else []\n","\n","        else:\n","            phase_boundaries = []\n","            n_phases = 1\n","\n","        return {\n","            'phase_boundaries': phase_boundaries,\n","            'n_phases': n_phases,\n","            'phase_duration_mean': len(returns) / (n_phases + 1)\n","        }\n","\n","    def _test_specific_window(self, data: pd.DataFrame, window: int) -> Dict:\n","        \"\"\"\n","        Special test for specific window (like 729).\n","        \"\"\"\n","        prices = data['Close'].values\n","\n","        if len(prices) < window:\n","            return None\n","\n","        # Calculate densities\n","        densities = []\n","        for i in range(0, len(prices) - window, window // 4):\n","            segment = prices[i:i+window]\n","            binary = (np.diff(segment) > 0).astype(int)\n","            densities.append(np.mean(binary))\n","\n","        if len(densities) < 3:\n","            return None\n","\n","        densities = np.array(densities)\n","\n","        # Test convergence to specific values\n","        results = {}\n","        for name, value in self.MATHEMATICAL_CONSTANTS.items():\n","            if 0 <= value <= 1:  # Only test density-compatible values\n","                distances = np.abs(densities - value)\n","\n","                # Bootstrap test\n","                bootstrap_means = []\n","                for _ in range(self.bootstrap_iterations):\n","                    random_densities = np.random.uniform(0, 1, len(densities))\n","                    bootstrap_means.append(np.mean(np.abs(random_densities - value)))\n","\n","                actual_mean = np.mean(distances)\n","                p_value = np.mean(np.array(bootstrap_means) <= actual_mean)\n","\n","                results[name] = {\n","                    'mean_distance': actual_mean,\n","                    'p_value': p_value,\n","                    'converges': p_value < self.alpha\n","                }\n","\n","        return results\n","\n","    def analyze_with_discoveries(self, data: pd.DataFrame,\n","                                discoveries: Optional[Dict] = None) -> Dict:\n","        \"\"\"\n","        Analyze data using discovered parameters.\n","        \"\"\"\n","        if discoveries is None:\n","            discoveries = self.discover_parameters(data)\n","\n","        results = {}\n","\n","        # 1. Window-based analysis\n","        if self.discovered_windows:\n","            results['window_analysis'] = self._analyze_discovered_windows(data)\n","\n","        # 2. Lag-based predictions\n","        if self.discovered_lags:\n","            results['lag_predictions'] = self._generate_lag_predictions(data)\n","\n","        # 3. Attractor dynamics\n","        if self.discovered_attractors:\n","            results['attractor_state'] = self._analyze_attractor_state(data)\n","\n","        # 4. Phase analysis\n","        results['phase_analysis'] = self._analyze_phases(data)\n","\n","        # 5. Statistical validation\n","        results['validation'] = self._validate_patterns(data)\n","\n","        return results\n","\n","    def _analyze_discovered_windows(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Analyze patterns using discovered optimal windows.\n","        \"\"\"\n","        prices = data['Close'].values\n","        window_results = {}\n","\n","        for window in self.discovered_windows[:5]:  # Top 5 windows\n","            if window >= len(prices):\n","                continue\n","\n","            # Calculate pattern strength\n","            segment = prices[-window:]\n","            binary = (np.diff(segment) > 0).astype(int)\n","            density = np.mean(binary)\n","\n","            # Calculate trend\n","            x = np.arange(len(segment))\n","            slope, intercept = np.polyfit(x, segment, 1)\n","\n","            window_results[window] = {\n","                'density': density,\n","                'trend': slope,\n","                'volatility': np.std(segment) / np.mean(segment)\n","            }\n","\n","        return window_results\n","\n","    def _generate_lag_predictions(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Generate predictions using discovered lag structure.\n","        \"\"\"\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < max(self.discovered_lags + [0]) + 10:\n","            return {}\n","\n","        # Use Random Forest with discovered lags as features\n","        features = []\n","        for i in range(max(self.discovered_lags), len(returns)):\n","            feature_vector = [returns[i - lag] for lag in self.discovered_lags]\n","            features.append(feature_vector)\n","\n","        features = np.array(features)\n","        targets = returns[max(self.discovered_lags):]\n","\n","        # Time series cross-validation\n","        tscv = TimeSeriesSplit(n_splits=5)\n","        model = RandomForestRegressor(n_estimators=100, random_state=42)\n","\n","        scores = cross_val_score(model, features[:-1], targets[1:],\n","                               cv=tscv, scoring='neg_mean_squared_error')\n","\n","        # Fit on all data for final prediction\n","        model.fit(features[:-1], targets[1:])\n","        next_features = features[-1].reshape(1, -1)\n","        prediction = model.predict(next_features)[0]\n","\n","        # Calculate confidence interval using random forest's predictions\n","        tree_predictions = np.array([tree.predict(next_features)[0]\n","                                    for tree in model.estimators_])\n","        confidence_interval = np.percentile(tree_predictions, [2.5, 97.5])\n","\n","        return {\n","            'prediction': prediction,\n","            'confidence_interval': confidence_interval.tolist(),\n","            'cv_score': -np.mean(scores),\n","            'feature_importance': dict(zip(self.discovered_lags,\n","                                          model.feature_importances_))\n","        }\n","\n","    def _analyze_attractor_state(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Determine current position relative to discovered attractors.\n","        \"\"\"\n","        prices = data['Close'].values\n","\n","        if len(prices) < 20:\n","            return {}\n","\n","        # Calculate current ratio\n","        current_ratio = prices[-1] / prices[-20] if prices[-20] != 0 else 1\n","\n","        # Find nearest attractor\n","        distances = {}\n","        for name, value in self.discovered_attractors.items():\n","            distances[name] = abs(current_ratio - value)\n","\n","        if distances:\n","            nearest = min(distances.items(), key=lambda x: x[1])\n","\n","            # Calculate approach velocity\n","            if len(prices) > 40:\n","                prev_ratio = prices[-20] / prices[-40] if prices[-40] != 0 else 1\n","                velocity = (current_ratio - prev_ratio) / 20\n","            else:\n","                velocity = 0\n","\n","            return {\n","                'nearest_attractor': nearest[0],\n","                'distance': nearest[1],\n","                'current_ratio': current_ratio,\n","                'approach_velocity': velocity,\n","                'converging': velocity * nearest[1] < 0  # Negative = approaching\n","            }\n","\n","        return {}\n","\n","    def _analyze_phases(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Determine current phase using discovered boundaries.\n","        \"\"\"\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < 50:\n","            return {'phase': 'insufficient_data'}\n","\n","        # Calculate current volatility regime\n","        recent_vol = np.std(returns[-20:])\n","        historical_vol = np.std(returns)\n","        vol_ratio = recent_vol / historical_vol if historical_vol > 0 else 1\n","\n","        # Classify phase based on volatility ratio\n","        if vol_ratio < 0.5:\n","            phase = 'low_volatility'\n","            confidence = 0.8\n","        elif vol_ratio < 0.8:\n","            phase = 'decreasing_volatility'\n","            confidence = 0.6\n","        elif vol_ratio < 1.2:\n","            phase = 'normal_volatility'\n","            confidence = 0.5\n","        elif vol_ratio < 1.5:\n","            phase = 'increasing_volatility'\n","            confidence = 0.6\n","        else:\n","            phase = 'high_volatility'\n","            confidence = 0.8\n","\n","        return {\n","            'phase': phase,\n","            'volatility_ratio': vol_ratio,\n","            'confidence': confidence\n","        }\n","\n","    def _validate_patterns(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Validate discovered patterns using out-of-sample testing.\n","        \"\"\"\n","        prices = data['Close'].values\n","\n","        if len(prices) < 200:\n","            return {'validated': False, 'reason': 'insufficient_data'}\n","\n","        # Split data\n","        split_point = len(prices) * 3 // 4\n","        train_data = prices[:split_point]\n","        test_data = prices[split_point:]\n","\n","        # Test if patterns hold out-of-sample\n","        train_stats = {\n","            'mean': np.mean(train_data),\n","            'std': np.std(train_data),\n","            'skew': stats.skew(train_data),\n","            'kurtosis': stats.kurtosis(train_data)\n","        }\n","\n","        test_stats = {\n","            'mean': np.mean(test_data),\n","            'std': np.std(test_data),\n","            'skew': stats.skew(test_data),\n","            'kurtosis': stats.kurtosis(test_data)\n","        }\n","\n","        # Calculate similarity\n","        similarity = 1 - np.mean([\n","            abs(train_stats['mean'] - test_stats['mean']) / (abs(train_stats['mean']) + 1e-10),\n","            abs(train_stats['std'] - test_stats['std']) / (train_stats['std'] + 1e-10),\n","            abs(train_stats['skew'] - test_stats['skew']) / (abs(train_stats['skew']) + 1e-10),\n","            abs(train_stats['kurtosis'] - test_stats['kurtosis']) / (abs(train_stats['kurtosis']) + 1e-10)\n","        ])\n","\n","        return {\n","            'validated': similarity > 0.7,\n","            'similarity_score': similarity,\n","            'train_stats': train_stats,\n","            'test_stats': test_stats\n","        }\n","\n","    def generate_trading_signals(self, data: pd.DataFrame,\n","                                analysis: Optional[Dict] = None) -> Dict:\n","        \"\"\"\n","        Generate trading signals based on analysis.\n","        \"\"\"\n","        if analysis is None:\n","            discoveries = self.discover_parameters(data)\n","            analysis = self.analyze_with_discoveries(data, discoveries)\n","\n","        signals = []\n","        confidences = []\n","\n","        # Combine multiple signal sources\n","        signal_components = []\n","\n","        # 1. Lag predictions\n","        if 'lag_predictions' in analysis and analysis['lag_predictions']:\n","            pred = analysis['lag_predictions']['prediction']\n","            if pred > self.empirical_thresholds.get('moderate_positive', 0.01):\n","                signal_components.append((1, 0.6))\n","            elif pred < self.empirical_thresholds.get('moderate_negative', -0.01):\n","                signal_components.append((-1, 0.6))\n","            else:\n","                signal_components.append((0, 0.3))\n","\n","        # 2. Attractor state\n","        if 'attractor_state' in analysis and analysis['attractor_state']:\n","            state = analysis['attractor_state']\n","            if state.get('converging') and state.get('distance', 1) < 0.1:\n","                # Near attractor and converging\n","                if 'near_unity' in state.get('nearest_attractor', ''):\n","                    # Mean reversion\n","                    current = state.get('current_ratio', 1)\n","                    signal_components.append((np.sign(1 - current), 0.5))\n","                else:\n","                    signal_components.append((0, 0.2))\n","\n","        # 3. Phase analysis\n","        if 'phase_analysis' in analysis and analysis['phase_analysis']:\n","            phase = analysis['phase_analysis']['phase']\n","            conf = analysis['phase_analysis']['confidence']\n","\n","            if phase == 'low_volatility':\n","                signal_components.append((0, conf * 0.5))  # Reduce position in low vol\n","            elif phase == 'high_volatility':\n","                signal_components.append((0, conf * 0.3))  # Be cautious in high vol\n","            else:\n","                signal_components.append((0, conf * 0.4))\n","\n","        # Combine signals weighted by confidence\n","        if signal_components:\n","            total_weight = sum(c for _, c in signal_components)\n","            if total_weight > 0:\n","                final_signal = sum(s * c for s, c in signal_components) / total_weight\n","                final_confidence = np.mean([c for _, c in signal_components])\n","            else:\n","                final_signal = 0\n","                final_confidence = 0\n","        else:\n","            final_signal = 0\n","            final_confidence = 0\n","\n","        # Generate signal array (simplified for single prediction)\n","        n_signals = min(len(data), 10)\n","        signals = [final_signal] * n_signals\n","        confidences = [final_confidence] * n_signals\n","\n","        return {\n","            'signals': signals,\n","            'confidences': confidences,\n","            'mean_confidence': final_confidence,\n","            'signal_components': signal_components,\n","            'recommendation': self._get_recommendation(final_signal, final_confidence)\n","        }\n","\n","    def _get_recommendation(self, signal: float, confidence: float) -> str:\n","        \"\"\"\n","        Generate human-readable recommendation.\n","        \"\"\"\n","        if confidence < 0.3:\n","            return \"No clear signal - stay out of market\"\n","        elif confidence < 0.5:\n","            if abs(signal) < 0.5:\n","                return \"Weak signal - consider small position\"\n","            else:\n","                return f\"Moderate {'buy' if signal > 0 else 'sell'} signal with low confidence\"\n","        else:\n","            if abs(signal) < 0.3:\n","                return \"Neutral market - no strong directional bias\"\n","            elif signal > 0:\n","                return f\"Buy signal with {confidence:.1%} confidence\"\n","            else:\n","                return f\"Sell signal with {confidence:.1%} confidence\"\n","\n","    def _count_runs(self, binary_sequence: np.ndarray) -> int:\n","        \"\"\"\n","        Count runs in a binary sequence.\n","        \"\"\"\n","        if len(binary_sequence) == 0:\n","            return 0\n","        runs = 1\n","        for i in range(1, len(binary_sequence)):\n","            if binary_sequence[i] != binary_sequence[i-1]:\n","                runs += 1\n","        return runs\n","\n","    def backtest(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"\n","        Perform walk-forward backtesting with proper statistical validation.\n","        \"\"\"\n","        prices = data['Close'].values\n","        returns = data['returns'].values\n","\n","        if len(prices) < 200:\n","            return {'error': 'Insufficient data for backtesting'}\n","\n","        # Walk-forward analysis\n","        window_size = 100\n","        step_size = 20\n","\n","        results = []\n","\n","        for i in range(window_size, len(prices) - step_size, step_size):\n","            # Train on data up to i\n","            train_data = data.iloc[:i]\n","\n","            # Test on next step_size periods\n","            test_data = data.iloc[i:i+step_size]\n","\n","            # Discover parameters on training data\n","            discoveries = self.discover_parameters(train_data)\n","\n","            # Analyze and generate signals\n","            analysis = self.analyze_with_discoveries(train_data, discoveries)\n","            signals_dict = self.generate_trading_signals(train_data, analysis)\n","\n","            # Apply to test data\n","            if signals_dict['signals']:\n","                signal = signals_dict['signals'][0]\n","                test_returns = test_data['returns'].values\n","\n","                # Calculate strategy returns\n","                strategy_returns = signal * test_returns\n","\n","                results.append({\n","                    'period_return': np.sum(strategy_returns),\n","                    'buy_hold_return': np.sum(test_returns),\n","                    'signal': signal,\n","                    'confidence': signals_dict['mean_confidence']\n","                })\n","\n","        if not results:\n","            return {'error': 'No valid backtest periods'}\n","\n","        # Calculate performance metrics\n","        period_returns = [r['period_return'] for r in results]\n","        buy_hold_returns = [r['buy_hold_return'] for r in results]\n","\n","        strategy_cumulative = np.cumprod(1 + np.array(period_returns)) - 1\n","        buy_hold_cumulative = np.cumprod(1 + np.array(buy_hold_returns)) - 1\n","\n","        # Sharpe ratio\n","        strategy_sharpe = np.mean(period_returns) / (np.std(period_returns) + 1e-10)\n","        buy_hold_sharpe = np.mean(buy_hold_returns) / (np.std(buy_hold_returns) + 1e-10)\n","\n","        # Win rate\n","        win_rate = np.mean([r['period_return'] > 0 for r in results])\n","\n","        # Statistical significance test\n","        t_stat, p_value = stats.ttest_rel(period_returns, buy_hold_returns)\n","\n","        return {\n","            'strategy_return': strategy_cumulative[-1] if len(strategy_cumulative) > 0 else 0,\n","            'buy_hold_return': buy_hold_cumulative[-1] if len(buy_hold_cumulative) > 0 else 0,\n","            'strategy_sharpe': strategy_sharpe,\n","            'buy_hold_sharpe': buy_hold_sharpe,\n","            'win_rate': win_rate,\n","            'n_periods': len(results),\n","            'outperformance': strategy_cumulative[-1] - buy_hold_cumulative[-1] if len(strategy_cumulative) > 0 else 0,\n","            'statistical_significance': {\n","                't_statistic': t_stat,\n","                'p_value': p_value,\n","                'is_significant': p_value < self.alpha\n","            },\n","            'mean_confidence': np.mean([r['confidence'] for r in results])\n","        }\n","\n","\n","def run_unbiased_analysis(symbol: str = 'SPY',\n","                         start: str = '2020-01-01',\n","                         end: str = '2024-10-31',\n","                         show_details: bool = True) -> Dict:\n","    \"\"\"\n","    Run complete unbiased analysis on market data.\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"UNBIASED PATTERN DISCOVERY AND ANALYSIS\")\n","    print(\"=\"*70)\n","\n","    # Get data\n","    try:\n","        import yfinance as yf\n","        print(f\"\\nFetching data for {symbol}...\")\n","        data = yf.download(symbol, start=start, end=end, progress=False)\n","        data['returns'] = data['Close'].pct_change()\n","        print(f\"‚úì Loaded {len(data)} days of data\")\n","    except Exception as e:\n","        print(f\"Error loading data: {e}\")\n","        print(\"Generating synthetic data for demonstration...\")\n","        dates = pd.date_range(start=start, end=end, freq='D')\n","        n = len(dates)\n","\n","        # Generate more realistic synthetic data\n","        returns = np.random.normal(0.0005, 0.02, n)\n","        returns[::45] += np.random.normal(0, 0.01)  # Add autocorrelation at lag 45\n","        returns[::42] += np.random.normal(0, 0.01)  # Add autocorrelation at lag 42\n","\n","        prices = 100 * np.cumprod(1 + returns)\n","\n","        data = pd.DataFrame({\n","            'Close': prices,\n","            'returns': returns\n","        }, index=dates)\n","\n","    # Initialize predictor\n","    predictor = UnbiasedPatternPredictor(confidence_level=0.95)\n","\n","    # Discover parameters\n","    print(\"\\n\" + \"-\"*50)\n","    print(\"PHASE 1: DISCOVERING PARAMETERS FROM DATA\")\n","    print(\"-\"*50)\n","\n","    discoveries = predictor.discover_parameters(data)\n","\n","    if show_details:\n","        print(\"\\nüìä Discovered Windows:\")\n","        if discoveries['windows']['significant_windows']:\n","            for window in discoveries['windows']['significant_windows'][:5]:\n","                p_val = discoveries['windows']['corrected_pvalues'].get(window, 1)\n","                print(f\"  Window {window}: p-value = {p_val:.6f}\")\n","        else:\n","            print(\"  No statistically significant windows found\")\n","\n","        # Special check for 729\n","        if discoveries['windows']['special_729']:\n","            print(\"\\nüéØ Window 729 Special Test:\")\n","            for const, result in discoveries['windows']['special_729'].items():\n","                if result['converges']:\n","                    print(f\"  Converges to {const}: p-value = {result['p_value']:.6f} ‚úì\")\n","\n","        print(\"\\nüìà Discovered Lags:\")\n","        if discoveries['lags']['significant_lags']:\n","            for lag in discoveries['lags']['significant_lags'][:5]:\n","                strength = discoveries['lags']['lag_strengths'].get(lag, 0)\n","                print(f\"  Lag {lag}: strength = {strength:.6f}\")\n","        else:\n","            print(\"  No significant autocorrelation found\")\n","\n","        print(\"\\nüéØ Discovered Attractors:\")\n","        if discoveries['attractors']['attractors']:\n","            for name, value in list(discoveries['attractors']['attractors'].items())[:5]:\n","                print(f\"  {name}: {value:.6f}\")\n","        else:\n","            print(\"  No clear attractors found\")\n","\n","        print(\"\\nüìê Mathematical Constants Test:\")\n","        significant_constants = []\n","        for name, result in discoveries['constants'].items():\n","            if result['is_significant']:\n","                significant_constants.append(name)\n","                print(f\"  {name}: distance = {result['min_distance']:.6f}, p-value = {result['p_value']:.6f} ‚úì\")\n","\n","        if not significant_constants:\n","            print(\"  No mathematical constants significantly present\")\n","\n","    # Analyze with discoveries\n","    print(\"\\n\" + \"-\"*50)\n","    print(\"PHASE 2: ANALYZING PATTERNS\")\n","    print(\"-\"*50)\n","\n","    analysis = predictor.analyze_with_discoveries(data, discoveries)\n","\n","    if show_details:\n","        if 'window_analysis' in analysis and analysis['window_analysis']:\n","            print(\"\\nüìä Window Analysis:\")\n","            for window, results in list(analysis['window_analysis'].items())[:3]:\n","                print(f\"  Window {window}:\")\n","                print(f\"    Density: {results['density']:.6f}\")\n","                print(f\"    Trend: {results['trend']:.6e}\")\n","                print(f\"    Volatility: {results['volatility']:.6f}\")\n","\n","        if 'lag_predictions' in analysis and analysis['lag_predictions']:\n","            print(\"\\nüîÆ Lag-Based Prediction:\")\n","            pred = analysis['lag_predictions']\n","            print(f\"  Next return prediction: {pred['prediction']:.6f}\")\n","            print(f\"  95% CI: [{pred['confidence_interval'][0]:.6f}, {pred['confidence_interval'][1]:.6f}]\")\n","            print(f\"  Cross-validation MSE: {pred['cv_score']:.6e}\")\n","\n","        if 'attractor_state' in analysis and analysis['attractor_state']:\n","            print(\"\\nüéØ Attractor State:\")\n","            state = analysis['attractor_state']\n","            print(f\"  Nearest: {state['nearest_attractor']}\")\n","            print(f\"  Distance: {state['distance']:.6f}\")\n","            print(f\"  Converging: {state['converging']}\")\n","\n","    # Generate trading signals\n","    print(\"\\n\" + \"-\"*50)\n","    print(\"PHASE 3: GENERATING TRADING SIGNALS\")\n","    print(\"-\"*50)\n","\n","    signals = predictor.generate_trading_signals(data, analysis)\n","\n","    print(f\"\\nüìà Trading Signal:\")\n","    print(f\"  Signal: {signals['signals'][0]:.3f}\")\n","    print(f\"  Confidence: {signals['mean_confidence']:.1%}\")\n","    print(f\"  Recommendation: {signals['recommendation']}\")\n","\n","    # Backtest\n","    print(\"\\n\" + \"-\"*50)\n","    print(\"PHASE 4: BACKTESTING\")\n","    print(\"-\"*50)\n","\n","    backtest_results = predictor.backtest(data)\n","\n","    if 'error' not in backtest_results:\n","        print(f\"\\nüìä Backtest Results:\")\n","        print(f\"  Strategy Return: {backtest_results['strategy_return']:.2%}\")\n","        print(f\"  Buy & Hold Return: {backtest_results['buy_hold_return']:.2%}\")\n","        print(f\"  Strategy Sharpe: {backtest_results['strategy_sharpe']:.3f}\")\n","        print(f\"  Buy & Hold Sharpe: {backtest_results['buy_hold_sharpe']:.3f}\")\n","        print(f\"  Win Rate: {backtest_results['win_rate']:.1%}\")\n","        # Check if statistical_significance key exists before accessing\n","        if 'statistical_significance' in backtest_results:\n","            print(f\"  Statistical Significance: p = {backtest_results['statistical_significance']['p_value']:.6f}\")\n","\n","            if backtest_results['statistical_significance']['is_significant']:\n","                print(\"  ‚úì Strategy is statistically different from buy & hold\")\n","            else:\n","                print(\"  ‚úó No statistical evidence of outperformance\")\n","        else:\n","            print(\"  Statistical significance could not be calculated (insufficient backtest periods)\")\n","\n","    else:\n","        print(f\"  Error: {backtest_results['error']}\")\n","\n","    # Summary\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"SUMMARY\")\n","    print(\"=\"*70)\n","\n","    # Count significant findings\n","    n_significant = sum([\n","        len(discoveries['windows']['significant_windows']) > 0,\n","        len(discoveries['lags']['significant_lags']) > 0,\n","        len(discoveries['attractors']['attractors']) > 0,\n","        any(r['is_significant'] for r in discoveries['constants'].values()),\n","        backtest_results.get('statistical_significance', {}).get('is_significant', False)\n","    ])\n","\n","    print(f\"\\nüéØ Significant Findings: {n_significant}/5\")\n","\n","    if discoveries['windows']['special_729'] and any(r['converges'] for r in discoveries['windows']['special_729'].values()):\n","        print(\"‚úì Window 729 shows special properties as hypothesized\")\n","\n","    if signals['mean_confidence'] > 0.5:\n","        print(f\"‚úì Confident trading signal generated ({signals['mean_confidence']:.1%})\")\n","    else:\n","        print(f\"‚ö† Low confidence in trading signals ({signals['mean_confidence']:.1%})\")\n","\n","    if 'strategy_sharpe' in backtest_results and backtest_results['strategy_sharpe'] > 0:\n","        print(f\"‚úì Positive Sharpe ratio: {backtest_results['strategy_sharpe']:.3f}\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","\n","    return {\n","        'discoveries': discoveries,\n","        'analysis': analysis,\n","        'signals': signals,\n","        'backtest': backtest_results,\n","        'summary': {\n","            'n_significant_findings': n_significant,\n","            'window_729_validated': discoveries['windows']['special_729'] is not None,\n","            'signal_confidence': signals['mean_confidence'],\n","            'backtest_success': backtest_results.get('strategy_sharpe', 0) > backtest_results.get('buy_hold_sharpe', 0)\n","        }\n","    }\n","\n","# Run the analysis\n","if __name__ == \"__main__\":\n","    results = run_unbiased_analysis('SPY', '2020-01-01', '2024-10-31', show_details=True)"]}]}