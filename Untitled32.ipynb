{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKIaFpz0CHfPR1yz9UWaTt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3F-t1C_TE2ni","executionInfo":{"status":"ok","timestamp":1758306183774,"user_tz":240,"elapsed":207555,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"9155aac1-4403-44b3-a98c-1c659b540ecd"},"outputs":[{"output_type":"stream","name":"stdout","text":["…progress lanes: 1,000 chunks, elapsed 35.2s\n","…progress lanes: 2,000 chunks, elapsed 99.4s\n","…progress lanes: 3,000 chunks, elapsed 144.8s\n","…progress lanes: 4,000 chunks, elapsed 180.0s\n","\n","Saved: alpha_best_lanes.csv (rows: 92640 )\n","Saved: alpha_best_per_base.csv\n","Saved: alpha_summary.csv\n","Best lane: {'best_base': 10, 'best_word': 'Fib', 'best_lambda': '0.729', 'best_op': 'S/S2', 'best_median_ppm': 1.4256779972806497, 'best_plateau_adjacent': True}\n","Saved: alpha_lambda_robustness.csv\n","Saved: alpha_perturbation.csv\n","Saved: alpha_offset_invariance.csv\n","Saved: alpha_outlier_score.csv\n","Saved: alpha_lambda_identifiability.csv\n","\n","Done. Small files only:\n"," - alpha_best_lanes.csv 123098.41796875 KB\n"," - alpha_best_per_base.csv 5.75390625 KB\n"," - alpha_lambda_robustness.csv 6.5703125 KB\n"," - alpha_perturbation.csv 0.6240234375 KB\n"," - alpha_offset_invariance.csv 3.5146484375 KB\n"," - alpha_outlier_score.csv 0.14453125 KB\n"," - alpha_lambda_identifiability.csv 0.103515625 KB\n"," - alpha_summary.csv 0.1171875 KB\n"]}],"source":["# ================================================================\n","# LEAN REBUILD (no giant CSVs)\n","# - Finds best α-lanes fairly, then writes only small, reviewer-ready CSVs\n","# - Files written:\n","#     alpha_best_lanes.csv\n","#     alpha_best_per_base.csv\n","#     alpha_lambda_robustness.csv\n","#     alpha_perturbation.csv\n","#     alpha_offset_invariance.csv\n","#     alpha_outlier_score.csv\n","#     alpha_lambda_identifiability.csv\n","#     alpha_summary.csv\n","# ================================================================\n","from decimal import Decimal as D, getcontext, ROUND_HALF_EVEN\n","import math, numpy as np, pandas as pd\n","import itertools, time, os\n","\n","# ---------- Precision & target ----------\n","NS = [12, 27, 81, 243, 729, 2187]\n","getcontext().prec = max(2600, int(max(NS)*1.2) + 200)\n","getcontext().rounding = ROUND_HALF_EVEN\n","ALPHA_INV = D(\"137.035999084\")\n","def ppm(v: D, ref: D = ALPHA_INV) -> D:\n","    return (abs(v - ref) / ref) * D(1_000_000)\n","\n","# ---------- Words ----------\n","def fib_word(n):\n","    a, b = \"0\", \"01\"\n","    while len(b) < n:\n","        a, b = b, b + a\n","    return b[:n]\n","def thue_morse(n):        return ''.join('1' if bin(i).count('1')%2 else '0' for i in range(n))\n","def period_doubling(n):\n","    s=\"0\"\n","    while len(s)<n:\n","        s = s.replace(\"0\",\"X\").replace(\"1\",\"Y\")\n","        s = s.replace(\"X\",\"01\").replace(\"Y\",\"00\")\n","    return s[:n]\n","def rudin_shapiro(n):\n","    out=[]\n","    for i in range(n):\n","        b = bin(i)[2:]\n","        cnt = sum(1 for j in range(len(b)-1) if b[j]=='1' and b[j+1]=='1')\n","        out.append('1' if cnt%2 else '0')\n","    return ''.join(out)\n","def random_word(n, seed=777):\n","    rng = np.random.RandomState(seed)\n","    return ''.join(str(int(x)) for x in rng.randint(0,2,size=n))\n","def fib_shuffled(n, seed=888):\n","    digs = list(fib_word(n))\n","    rng = np.random.RandomState(seed)\n","    rng.shuffle(digs)\n","    return ''.join(digs)\n","\n","WORDS = {\n","    \"Fib\": fib_word,\n","    \"TM\": thue_morse,\n","    \"PD\": period_doubling,\n","    \"RS\": rudin_shapiro,\n","    \"Rand\": random_word,     # deterministic seed\n","    \"FibShuf\": fib_shuffled, # deterministic seed\n","}\n","\n","# ---------- Interpret 0/1 as base-B fraction ----------\n","def word_to_fraction_base(digits_str: str, B: int) -> D:\n","    w = D(0)\n","    weight = D(1) / D(B)\n","    for ch in digits_str:\n","        if ch == '1':\n","            w += weight\n","        weight = weight / D(B)\n","    return w\n","BASES = [10, 3, 5, 2]   # keep same set, in an intuitive order\n","\n","# ---------- Ops (rails) ----------\n","def S1(x): return D(1) - x\n","def S2(x): return None if x == 0 else D(1) / x\n","def S3(x): return None if x == 1 else x / (D(1) - x)\n","def S4(x): return -x\n","\n","OPS = {\"S1\":S1, \"S2\":S2, \"S3\":S3, \"S4\":S4}\n","INV = {\"S1\":S1, \"S2\":S2, \"S3\":(lambda y: None if y == -1 else y/(D(1)+y)), \"S4\":S4}\n","\n","T = {\n","    \"T1\":(\"S2\",\"S1\"), \"T2\":(\"S1\",\"S2\"), \"T3\":(\"S1\",\"S3\"), \"T4\":(\"S3\",\"S2\"),\n","    \"T5\":(\"S3\",\"S1\"), \"T6\":(\"S4\",\"S1\"), \"T7\":(\"S1\",\"S4\"), \"T8\":(\"S4\",\"S3\"),\n","    \"T9\":(\"S3\",\"S4\"), \"T10\":(\"S2\",\"S4\"), \"T11\":(\"S4\",\"S2\"), \"T12\":(\"S2\",\"S3\"),\n","    \"T13\":(\"S3\",\"S3\"), \"T14\":(\"S2\",\"S2\"), \"T15\":(\"S1\",\"S1\"), \"T16\":(\"S4\",\"S4\"),\n","}\n","def apply_seq(x: D, seq) -> D:\n","    y = x\n","    for s in seq:\n","        y = OPS[s](y)\n","        if y is None: return None\n","    return y\n","ALL_OPS = {(\"S\",k):(k,) for k in OPS.keys()}\n","ALL_OPS.update({(\"T\",k):T[k] for k in T.keys()})\n","\n","# ---------- Scale grid (fair, symmetric, modest size) ----------\n","def generate_scales():\n","    # λ = 2^u * 3^v * 5^w / 10^k with small exponents; include 3^6/10^3 explicitly\n","    vals = set()\n","    for u in range(-1,2):      # 2^[-1..1]\n","        for v in range(-6,7):  # 3^[-6..6]\n","            for w in range(-1,2):  # 5^[-1..1]\n","                for k in range(-3,4):  # 10^[-3..3]\n","                    lam = (D(2)**u)*(D(3)**v)*(D(5)**w)/(D(10)**k)\n","                    if D(\"0.03\") <= lam <= D(\"30\"):\n","                        vals.add(lam)\n","    vals.add((D(3)**6)/(D(10)**3))  # 0.729\n","    return sorted(vals)\n","LAMBDAS = generate_scales()\n","\n","# ---------- Complexity proxy ----------\n","def complexity_score(word_name, base, op_kind, op_code, lam: D):\n","    lam_str = format(lam, 'f').rstrip('0').rstrip('.') if '.' in format(lam, 'f') else str(lam)\n","    depth = 1 if op_kind==\"S\" else 2\n","    word_term = {\"Fib\":0,\"TM\":1,\"PD\":1,\"RS\":2,\"Rand\":3,\"FibShuf\":2}[word_name]\n","    base_term = {10:0,3:1,5:1,2:2}[base]\n","    return len(lam_str) + depth + word_term + base_term\n","\n","# ---------- Streaming search: keep only lane summaries (tiny) ----------\n","def run_search():\n","    agg = {}  # key -> {\"ppms\":[], \"complexity\":c}\n","    total = len(BASES)*len(WORDS)*len(NS)*len(LAMBDAS)*len(ALL_OPS)\n","    t0 = time.time(); step=0\n","\n","    for base in BASES:\n","        for word_name, builder in WORDS.items():\n","            # prebuild all N digit strings deterministically\n","            digits_by_N = {}\n","            for N in NS:\n","                if word_name == \"Rand\":\n","                    digits_by_N[N] = builder(N, seed=777)\n","                elif word_name == \"FibShuf\":\n","                    digits_by_N[N] = builder(N, seed=888)\n","                else:\n","                    digits_by_N[N] = builder(N)\n","            # convert once per N\n","            xraw_by_N = {N: word_to_fraction_base(digits_by_N[N], base) for N in NS}\n","\n","            for lam in LAMBDAS:\n","                for (ok, oc), seq in ALL_OPS.items():\n","                    key = (base, word_name, str(lam), ok, oc)\n","                    ppms=[]\n","                    for N in NS:\n","                        y = apply_seq(xraw_by_N[N]*lam, seq)\n","                        val_ppm = float(ppm(y)) if y is not None else float('inf')\n","                        ppms.append(val_ppm)\n","                    agg[key] = {\n","                        \"median_ppm\": float(np.median(ppms)),\n","                        \"plateau_adjacent\": any(ppms[i]<=50.0 and ppms[i+1]<=50.0 for i in range(len(ppms)-1)),\n","                        \"complexity\": complexity_score(word_name, base, ok, oc, lam)\n","                    }\n","                step += 1\n","                if step % 1000 == 0:\n","                    dt = time.time()-t0\n","                    print(f\"…progress lanes: {step:,} chunks, elapsed {dt:.1f}s\")\n","\n","    rows=[]\n","    for (base,word,lam,ok,oc), rec in agg.items():\n","        rows.append({\n","            \"base\": base, \"word\": word, \"lambda\": lam, \"op_kind\": ok, \"op_code\": oc,\n","            \"median_ppm\": rec[\"median_ppm\"],\n","            \"plateau_adjacent\": rec[\"plateau_adjacent\"],\n","            \"complexity\": rec[\"complexity\"],\n","        })\n","    lanes = pd.DataFrame(rows).sort_values([\"median_ppm\",\"complexity\"]).reset_index(drop=True)\n","    return lanes\n","\n","lanes = run_search()\n","lanes.to_csv(\"alpha_best_lanes.csv\", index=False)\n","print(\"\\nSaved: alpha_best_lanes.csv (rows:\", len(lanes), \")\")\n","\n","# ---------- Best per base ----------\n","best_per_base = lanes.groupby(\"base\", as_index=False).first()\n","best_per_base.to_csv(\"alpha_best_per_base.csv\", index=False)\n","print(\"Saved: alpha_best_per_base.csv\")\n","\n","# ---------- Extract overall best (for follow-up tests) ----------\n","best = lanes.iloc[0]\n","best_base = int(best[\"base\"])\n","best_word = best[\"word\"]\n","best_lambda = D(str(best[\"lambda\"]))\n","best_okind = best[\"op_kind\"]; best_oc = best[\"op_code\"]\n","summary = {\n","    \"best_base\": best_base,\n","    \"best_word\": best_word,\n","    \"best_lambda\": str(best_lambda),\n","    \"best_op\": f\"{best_okind}/{best_oc}\",\n","    \"best_median_ppm\": float(best[\"median_ppm\"]),\n","    \"best_plateau_adjacent\": bool(best[\"plateau_adjacent\"]),\n","}\n","pd.DataFrame([summary]).to_csv(\"alpha_summary.csv\", index=False)\n","print(\"Saved: alpha_summary.csv\")\n","print(\"Best lane:\", summary)\n","\n","# ---------- λ-robustness (±20 ppm around best λ) ----------\n","def robustness_sweep(best_lambda, seq, base, word_name, ns=NS, ppm_window=20):\n","    deltas_ppm = np.linspace(-ppm_window, ppm_window, 41)\n","    rows=[]\n","    builder = WORDS[word_name]\n","    for N in ns:\n","        if word_name == \"Rand\":\n","            digits = builder(N, seed=777)\n","        elif word_name == \"FibShuf\":\n","            digits = builder(N, seed=888)\n","        else:\n","            digits = builder(N)\n","        x_raw = word_to_fraction_base(digits, base)\n","        for d in deltas_ppm:\n","            lam = best_lambda * (D(1) + D(d)/D(1_000_000))\n","            y = apply_seq(x_raw * lam, ALL_OPS[(best_okind, best_oc)])\n","            rows.append({\"N\":N, \"d_ppm_on_lambda\": float(d), \"ppm\": float(ppm(y)) if y is not None else float('inf')})\n","    return pd.DataFrame(rows)\n","\n","rob = robustness_sweep(best_lambda, ALL_OPS[(best_okind,best_oc)], best_base, best_word)\n","rob.to_csv(\"alpha_lambda_robustness.csv\", index=False)\n","print(\"Saved: alpha_lambda_robustness.csv\")\n","\n","# ---------- Perturbation sensitivity (mean±sd) ----------\n","def flip_bits(s, rate, seed):\n","    if rate<=0: return s\n","    rng = np.random.RandomState(seed)\n","    n = len(s); k = int(round(rate*n))\n","    idx = rng.choice(n, size=k, replace=False)\n","    arr = np.array(list(s))\n","    arr[idx] = np.where(arr[idx]=='0','1','0')\n","    return ''.join(arr)\n","def perturbation_curve(seq, base, word_name, lam, rates=(0,0.001,0.01,0.05,0.1), seeds=(101,202,303), ns=(81,243,729)):\n","    rows=[]\n","    builder = WORDS[word_name]\n","    for N in ns:\n","        if word_name == \"Rand\":\n","            base_digits = builder(N, seed=777)\n","        elif word_name == \"FibShuf\":\n","            base_digits = builder(N, seed=888)\n","        else:\n","            base_digits = builder(N)\n","        for r in rates:\n","            ppms=[]\n","            for sd in seeds:\n","                pert = flip_bits(base_digits, r, sd)\n","                x_raw = word_to_fraction_base(pert, base)\n","                y = apply_seq(x_raw * lam, seq)\n","                ppms.append(float(ppm(y)) if y is not None else float('inf'))\n","            rows.append({\"N\":N, \"flip_rate\":r, \"ppm_mean\":float(np.mean(ppms)), \"ppm_std\":float(np.std(ppms))})\n","    return pd.DataFrame(rows)\n","pert = perturbation_curve(ALL_OPS[(best_okind,best_oc)], best_base, best_word, best_lambda)\n","pert.to_csv(\"alpha_perturbation.csv\", index=False)\n","print(\"Saved: alpha_perturbation.csv\")\n","\n","# ---------- Offset/window invariance (random start offsets) ----------\n","def offset_invariance(word_name, lam, base, Ns=(81,243,729,2187), samples={81:50,243:40,729:30,2187:15}):\n","    rows=[]\n","    builder = WORDS[word_name]\n","    rng = np.random.RandomState(2025)\n","    for N in Ns:\n","        L = N + 5000\n","        digits_big = builder(L) if word_name not in (\"Rand\",\"FibShuf\") else (builder(L, seed=777) if word_name==\"Rand\" else builder(L, seed=888))\n","        starts = sorted(rng.choice(L - N, size=samples[N], replace=False))\n","        for o in starts:\n","            sub = digits_big[o:o+N]\n","            x0 = word_to_fraction_base(sub, base) * lam\n","            y = S2(x0)\n","            rows.append({\"N\":N, \"offset\":int(o), \"ppm\": float(ppm(y))})\n","    return pd.DataFrame(rows)\n","off = offset_invariance(best_word, best_lambda, best_base)\n","off.to_csv(\"alpha_offset_invariance.csv\", index=False)\n","print(\"Saved: alpha_offset_invariance.csv\")\n","\n","# ---------- Outlier score (empirical p vs all lanes) ----------\n","all_meds = lanes[\"median_ppm\"].values\n","best_med = float(best[\"median_ppm\"])\n","rank = int((all_meds <= best_med).sum())\n","p_emp = rank / len(all_meds)\n","gap = float(lanes.iloc[1][\"median_ppm\"] - best_med) if len(lanes) > 1 else float(\"nan\")\n","pd.DataFrame([{\n","    \"best_median_ppm\": best_med,\n","    \"rank_among_lanes\": rank,\n","    \"num_lanes\": int(len(all_meds)),\n","    \"empirical_p_value\": float(p_emp),\n","    \"gap_to_2nd_best_ppm\": float(gap)\n","}]).to_csv(\"alpha_outlier_score.csv\", index=False)\n","print(\"Saved: alpha_outlier_score.csv\")\n","\n","# ---------- λ identifiability (local quadratic around Δλ=0) ----------\n","local = rob[rob[\"d_ppm_on_lambda\"].abs() <= 4].copy()\n","X = np.vstack([np.ones(len(local)), local[\"d_ppm_on_lambda\"].values, (local[\"d_ppm_on_lambda\"].values**2)]).T\n","y = local[\"ppm\"].values\n","beta, *_ = np.linalg.lstsq(X, y, rcond=None)\n","a,b,c = beta\n","pd.DataFrame([{\"ppm_at_min_approx\":a, \"slope_dppm_dppmLam\":b, \"curvature\":c}]).to_csv(\"alpha_lambda_identifiability.csv\", index=False)\n","print(\"Saved: alpha_lambda_identifiability.csv\")\n","\n","print(\"\\nDone. Small files only:\")\n","for f in [\"alpha_best_lanes.csv\",\"alpha_best_per_base.csv\",\"alpha_lambda_robustness.csv\",\n","          \"alpha_perturbation.csv\",\"alpha_offset_invariance.csv\",\"alpha_outlier_score.csv\",\n","          \"alpha_lambda_identifiability.csv\",\"alpha_summary.csv\"]:\n","    print(\" -\", f, os.path.getsize(f)/1024, \"KB\")\n"]}]}