{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfssLbAiQFM5wa+BYmsNWV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xK9nFEnLFOQy","executionInfo":{"status":"ok","timestamp":1755201356893,"user_tz":240,"elapsed":46581,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"175d41c9-e1fb-4ca8-bde5-ac5baf7d6ff9"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ COMPLETE COMPREHENSIVE EMERGENCE MAPPING\n","üìä Building Tier 2/3 + Full Statistical Analysis + Visualizations\n","üéØ Loading existing Tier 1 results and extending analysis\n","üöÄ STARTING COMPLETE COMPREHENSIVE EMERGENCE MAPPING...\n","‚úÖ Loaded 864 Tier 1 results from previous run\n","üìö Built comprehensive library: 443 constant variations\n","\n","üî¨ Building Tier 2: Dyadic Operations...\n","  Processed 100/864 Tier 1 results...\n","  Processed 200/864 Tier 1 results...\n","  Processed 300/864 Tier 1 results...\n","  Processed 400/864 Tier 1 results...\n","  Processed 500/864 Tier 1 results...\n","  Processed 600/864 Tier 1 results...\n","  Processed 700/864 Tier 1 results...\n","  Processed 800/864 Tier 1 results...\n","‚úÖ Tier 2 complete: 36869 operations generated\n","\n","üî¨ Building Tier 3: Complex Operations...\n","  Processed 50 Tier 2 results...\n","  Processed 100 Tier 2 results...\n","‚úÖ Tier 3 complete: 2503 operations generated\n","\n","üî¨ Building Pathway B: Direct Constant Operations...\n","‚úÖ Pathway B complete: 212 operations\n","\n","üìä Computing Statistical Summaries...\n","‚ùå Error: 'sequence'\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/tmp/ipython-input-4127220272.py\", line 696, in <cell line: 0>\n","    df_tier1, df_tier2, df_tier3, mc_results = run_complete_emergence_analysis()\n","                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-4127220272.py\", line 641, in run_complete_emergence_analysis\n","    tier2_summary = compute_cross_seed_statistics(df_tier2)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-4127220272.py\", line 366, in compute_cross_seed_statistics\n","    grouped = df.groupby(['sequence', 'base', 'k'] if 'k' in df.columns else ['sequence', 'source_base'])\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\", line 9183, in groupby\n","    return DataFrameGroupBy(\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\", line 1329, in __init__\n","    grouper, exclusions, obj = get_grouper(\n","                               ^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/grouper.py\", line 1043, in get_grouper\n","    raise KeyError(gpr)\n","KeyError: 'sequence'\n"]}],"source":["# === COMPLETE COMPREHENSIVE EMERGENCE MAPPING SYSTEM ===\n","# Building Tier 2/3 + Statistical Rigor + Visualizations on existing Tier 1 results\n","# Includes: Dyadic operations, Complex operations, Monte Carlo, Cymatics, Cross-seed stats\n","\n","import math, random, statistics, itertools, os, sys, time\n","from fractions import Fraction\n","from decimal import Decimal, getcontext\n","from collections import Counter, defaultdict\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","from scipy.fft import fft, fftfreq\n","from statsmodels.stats.multitest import fdrcorrection\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# ============= CONFIGURATION =============\n","OUTDIR = \"/content/complete_emergence_mapping\"\n","os.makedirs(OUTDIR, exist_ok=True)\n","\n","# Enhanced parameters for complete analysis\n","K_MAX = 8\n","L_MAX = 16            # Dyadic range for Tier 2\n","MONTE_CARLO_TRIALS = 500\n","N_BOOTSTRAP = 100\n","SEED_BASE = 123456\n","getcontext().prec = 100\n","\n","print(f\"üöÄ COMPLETE COMPREHENSIVE EMERGENCE MAPPING\")\n","print(f\"üìä Building Tier 2/3 + Full Statistical Analysis + Visualizations\")\n","print(f\"üéØ Loading existing Tier 1 results and extending analysis\")\n","\n","# ============= LOAD EXISTING TIER 1 RESULTS =============\n","def load_tier1_results():\n","    \"\"\"Load the successful Tier 1 results from previous run.\"\"\"\n","    try:\n","        df_tier1 = pd.read_csv('tier1_corrected.csv')\n","        print(f\"‚úÖ Loaded {len(df_tier1)} Tier 1 results from previous run\")\n","        return df_tier1\n","    except:\n","        print(\"‚ùå Could not load previous Tier 1 results\")\n","        return None\n","\n","# ============= COMPREHENSIVE CONSTANT LIBRARY =============\n","def build_comprehensive_constants():\n","    \"\"\"Build exhaustive constant library with all variations.\"\"\"\n","    base_constants = {\n","        'fine_structure': Decimal(\"0.0072973525693\"),\n","        'fine_structure_codata': Decimal(1) / Decimal(\"137.035999084\"),\n","        'phi': (Decimal(1) + Decimal(5).sqrt()) / 2,\n","        'pi': Decimal(str(math.pi)),\n","        'e': Decimal(str(math.e)),\n","        'sqrt2': Decimal(str(math.sqrt(2))),\n","        'sqrt3': Decimal(str(math.sqrt(3))),\n","        'feigenbaum_delta': Decimal(\"4.6692016091029906718532038204662\"),\n","        'feigenbaum_alpha': Decimal(\"2.5029078750958928222839028732182\"),\n","        'euler_gamma': Decimal(\"0.5772156649015328606065120900824\"),\n","        'catalan': Decimal(\"0.9159655941772190150546035149324\"),\n","        'apery': Decimal(\"1.2020569031595942853997381615114\"),\n","        'inv137': Decimal(1) / Decimal(137),\n","        'inv9': Decimal(1) / Decimal(9),\n","        'khinchin': Decimal(\"2.6854520010653064453097148354818\"),\n","        'glaisher': Decimal(\"1.2824271291006226368753425688697\"),\n","    }\n","\n","    constants = {}\n","\n","    # Generate ALL variations for each base constant\n","    for name, value in base_constants.items():\n","        if value > 0:\n","            constants[f'{name}'] = value\n","            constants[f'{name}_inv'] = Decimal(1) / value\n","            constants[f'{name}_neg'] = -value\n","            constants[f'{name}_sqrt'] = value.sqrt()\n","            constants[f'{name}_square'] = value * value\n","            constants[f'{name}_half'] = value / 2\n","            constants[f'{name}_double'] = value * 2\n","            constants[f'{name}_triple'] = value * 3\n","\n","            # Complement variations\n","            if value < 1:\n","                constants[f'{name}_complement'] = Decimal(1) - value\n","                if Decimal(1) - value > 0:\n","                    constants[f'{name}_complement_inv'] = Decimal(1) / (Decimal(1) - value)\n","\n","            # Scale variations (your key insight about digit ranges)\n","            for scale in [10, 100, 1000, 10000, 100000, 1000000]:\n","                constants[f'{name}_div{scale}'] = value / scale\n","                constants[f'{name}_x{scale}'] = value * scale\n","\n","            # Subtraction variations (your inverse suggestion)\n","            constants[f'{name}_minus1'] = value - 1 if value > 1 else None\n","            constants[f'{name}_minus2'] = value - 2 if value > 2 else None\n","            constants[f'{name}_plus1'] = value + 1\n","            constants[f'{name}_plus2'] = value + 2\n","\n","            # Root variations (your inverse suggestion)\n","            if value > 0:\n","                constants[f'{name}_cuberoot'] = value ** (Decimal(1)/3)\n","                constants[f'{name}_ninthroot'] = value ** (Decimal(1)/9)\n","                constants[f'{name}_root729'] = value ** (Decimal(1)/729) if value < 10 else None\n","                constants[f'{name}_27throot'] = value ** (Decimal(1)/27) if value < 100 else None\n","\n","    # Remove None values\n","    constants = {k: v for k, v in constants.items() if v is not None}\n","\n","    print(f\"üìö Built comprehensive library: {len(constants)} constant variations\")\n","    return constants\n","\n","# ============= ENHANCED MATHEMATICAL OPERATIONS =============\n","def find_closest_constants(value, constants_lib, max_matches=10):\n","    \"\"\"Find closest constants with ultra-high sensitivity.\"\"\"\n","    matches = []\n","    for name, const_val in constants_lib.items():\n","        try:\n","            if const_val is not None and const_val != 0:\n","                diff = abs(value - const_val)\n","                rel_diff = diff / abs(const_val) if const_val != 0 else float('inf')\n","                matches.append((name, const_val, float(diff), float(rel_diff)))\n","        except:\n","            continue\n","\n","    matches.sort(key=lambda x: x[2])\n","    return matches[:max_matches]\n","\n","# ============= TIER 2: COMPREHENSIVE DYADIC CASCADE =============\n","def tier2_dyadic_operations(base_value, constants_lib, l_max=L_MAX):\n","    \"\"\"Comprehensive dyadic operations + inverse explorations.\"\"\"\n","    operations = []\n","\n","    # Standard dyadic tweaks\n","    for l in range(8, l_max + 1):\n","        factor = Decimal(2) ** (-l)\n","\n","        for sign in [1, -1]:\n","            try:\n","                tweaked = base_value * (1 + sign * factor)\n","                operations.append({\n","                    'operation': f'√ó(1{\"++\" if sign > 0 else \"-\"}2^{-l})',\n","                    'factor': 1 + sign * factor,\n","                    'result': tweaked,\n","                    'l_value': l,\n","                    'operation_type': 'dyadic_tweak'\n","                })\n","            except:\n","                continue\n","\n","    # Transform operations (your inverse suggestions)\n","    transforms = [\n","        ('identity', lambda x: x),\n","        ('negative', lambda x: -x),\n","        ('reciprocal', lambda x: Decimal(1)/x if x != 0 else None),\n","        ('complement', lambda x: Decimal(1) - x if x < 1 and x > 0 else None),\n","        ('sqrt', lambda x: x.sqrt() if x > 0 else None),\n","        ('square', lambda x: x * x),\n","        ('minus1', lambda x: x - 1),\n","        ('minus2', lambda x: x - 2),\n","        ('plus1', lambda x: x + 1),\n","        ('plus2', lambda x: x + 2),\n","        ('cube_root', lambda x: x ** (Decimal(1)/3) if x > 0 else None),\n","        ('ninth_root', lambda x: x ** (Decimal(1)/9) if x > 0 else None),\n","        ('divide_by_3', lambda x: x / 3),\n","        ('divide_by_9', lambda x: x / 9),\n","        ('times_3', lambda x: x * 3),\n","        ('times_9', lambda x: x * 9),\n","        ('times_27', lambda x: x * 27),\n","        ('times_81', lambda x: x * 81),\n","        ('root_729', lambda x: x ** (Decimal(1)/729) if x > 0 and x < 100 else None),\n","        ('times_phi', lambda x: x * ((Decimal(1) + Decimal(5).sqrt()) / 2)),\n","        ('div_phi', lambda x: x / ((Decimal(1) + Decimal(5).sqrt()) / 2)),\n","        ('times_pi', lambda x: x * Decimal(str(math.pi))),\n","        ('div_pi', lambda x: x / Decimal(str(math.pi))),\n","        ('times_e', lambda x: x * Decimal(str(math.e))),\n","        ('div_e', lambda x: x / Decimal(str(math.e))),\n","    ]\n","\n","    for op_name, op_func in transforms:\n","        try:\n","            result = op_func(base_value)\n","            if result is not None and abs(result) < 1000:  # Reasonable bounds\n","                operations.append({\n","                    'operation': op_name,\n","                    'factor': 'transform',\n","                    'result': result,\n","                    'l_value': None,\n","                    'operation_type': 'transform'\n","                })\n","        except:\n","            continue\n","\n","    # Find constants for each operation\n","    enhanced_operations = []\n","    for op in operations:\n","        matches = find_closest_constants(op['result'], constants_lib)\n","        op['top_matches'] = matches[:5]\n","        if matches:\n","            op.update({\n","                'best_match_name': matches[0][0],\n","                'best_match_value': matches[0][1],\n","                'best_match_error': matches[0][2],\n","                'best_match_rel_error': matches[0][3]\n","            })\n","        enhanced_operations.append(op)\n","\n","    return enhanced_operations\n","\n","# ============= TIER 3: COMPLEX OPERATIONS =============\n","def tier3_complex_operations(base_value, constants_lib):\n","    \"\"\"Complex operations for tier 3.\"\"\"\n","    operations = []\n","    phi = (Decimal(1) + Decimal(5).sqrt()) / 2\n","    pi = Decimal(str(math.pi))\n","    e = Decimal(str(math.e))\n","\n","    complex_ops = [\n","        # Exponential operations\n","        ('phi^(pi-2)', lambda x: x * (phi ** (pi - 2))),\n","        ('e^(-x)', lambda x: x * (e ** (-x)) if abs(x) < 5 else None),\n","        ('x^phi', lambda x: x ** phi if x > 0 and x < 5 else None),\n","        ('x^(1/phi)', lambda x: x ** (Decimal(1)/phi) if x > 0 and x < 10 else None),\n","\n","        # Subtractive operations (your analytic formula style)\n","        ('minus_sqrt3', lambda x: x - Decimal(str(math.sqrt(3)))),\n","        ('minus_sqrt2', lambda x: x - Decimal(str(math.sqrt(2)))),\n","        ('minus_pi', lambda x: x - pi),\n","        ('minus_e', lambda x: x - e),\n","        ('minus_phi', lambda x: x - phi),\n","        ('minus_1/phi', lambda x: x - (Decimal(1)/phi)),\n","\n","        # Scale + subtract (like your 101 formula)\n","        ('101x_minus_sqrt3', lambda x: 101 * x - Decimal(str(math.sqrt(3)))),\n","        ('100x_minus_sqrt3', lambda x: 100 * x - Decimal(str(math.sqrt(3)))),\n","        ('102x_minus_sqrt3', lambda x: 102 * x - Decimal(str(math.sqrt(3)))),\n","        ('1000x_minus_pi', lambda x: 1000 * x - pi),\n","        ('137x_minus_1', lambda x: 137 * x - 1),\n","        ('729x_minus_phi', lambda x: 729 * x - phi),\n","\n","        # Logarithmic (safe versions)\n","        ('ln_x', lambda x: x.ln() if x > 0 and x < 100 else None),\n","        ('x_ln_x', lambda x: x * x.ln() if x > 0 and x < 10 else None),\n","\n","        # Trigonometric approximations via series\n","        ('sin_approx', lambda x: x - (x**3)/6 + (x**5)/120 if abs(x) < 2 else None),\n","        ('cos_approx', lambda x: 1 - (x**2)/2 + (x**4)/24 if abs(x) < 2 else None),\n","\n","        # Continued fraction operations\n","        ('x/(1+x)', lambda x: x / (1 + x) if x > -1 and x != 0 else None),\n","        ('1/(1+1/x)', lambda x: 1 / (1 + 1/x) if x != 0 and abs(x) > 0.01 else None),\n","\n","        # Golden ratio relationships\n","        ('phi*x - 1', lambda x: phi * x - 1),\n","        ('x/phi + 1/phi', lambda x: x/phi + 1/phi),\n","    ]\n","\n","    for op_name, op_func in complex_ops:\n","        try:\n","            result = op_func(base_value)\n","            if result is not None and abs(result) < 1000:\n","                matches = find_closest_constants(result, constants_lib)\n","                operations.append({\n","                    'operation': op_name,\n","                    'result': result,\n","                    'top_matches': matches[:3],\n","                    'best_match_name': matches[0][0] if matches else None,\n","                    'best_match_error': matches[0][2] if matches else None,\n","                    'best_match_rel_error': matches[0][3] if matches else None\n","                })\n","        except:\n","            continue\n","\n","    return operations\n","\n","# ============= MONTE CARLO STATISTICAL ANALYSIS =============\n","def monte_carlo_baseline_analysis(df_tier1, constants_lib):\n","    \"\"\"Comprehensive Monte Carlo analysis for statistical baselines.\"\"\"\n","    print(f\"\\nüìä Running Monte Carlo baseline analysis...\")\n","\n","    mc_results = []\n","\n","    # Test key cases with Monte Carlo\n","    key_cases = [\n","        {'sequence': 'F', 'base': 10, 'k': 6},  # Your key discovery\n","        {'sequence': 'F', 'base': 10, 'k': 0},  # Baseline\n","        {'sequence': 'R', 'base': 10, 'k': 0},  # Perfect case\n","        {'sequence': 'TM', 'base': 10, 'k': 0}, # TM baseline\n","    ]\n","\n","    for case in key_cases:\n","        case_results = df_tier1[\n","            (df_tier1['sequence'] == case['sequence']) &\n","            (df_tier1['base'] == case['base']) &\n","            (df_tier1['k'] == case['k'])\n","        ]\n","\n","        if len(case_results) == 0:\n","            continue\n","\n","        observed_error = case_results['best_match_error'].min()\n","\n","        # Generate random sequences with same density\n","        seed_0_result = case_results[case_results['seed_idx'] == 0].iloc[0]\n","        n_digits = 2000  # From our test\n","        sequence_density = case.get('density', 0.618)  # Approximate for F\n","\n","        # Density-matched null\n","        density_errors = []\n","        for trial in range(MONTE_CARLO_TRIALS):\n","            random.seed(SEED_BASE + trial)\n","            random_bits = [1 if random.random() < sequence_density else 0 for _ in range(n_digits)]\n","\n","            # Convert to decimal\n","            D_random = sum(b * (10 ** -(i+1)) for i, b in enumerate(random_bits))\n","\n","            # Apply same triadic scaling\n","            k = case['k']\n","            m = 3 ** k\n","            estimate_random = (m / 1000) * D_random\n","\n","            # Find closest constant\n","            matches = find_closest_constants(Decimal(str(estimate_random)), constants_lib)\n","            if matches:\n","                density_errors.append(matches[0][2])\n","\n","        # Calculate statistics\n","        if len(density_errors) > 0:\n","            p_value = np.mean(np.array(density_errors) <= observed_error)\n","            effect_size = (np.mean(density_errors) - observed_error) / np.std(density_errors) if np.std(density_errors) > 0 else 0\n","\n","            mc_results.append({\n","                'sequence': case['sequence'],\n","                'base': case['base'],\n","                'k': case['k'],\n","                'observed_error': observed_error,\n","                'mc_mean_error': np.mean(density_errors),\n","                'mc_std_error': np.std(density_errors),\n","                'p_value': p_value,\n","                'effect_size': effect_size,\n","                'n_trials': len(density_errors)\n","            })\n","\n","    return pd.DataFrame(mc_results)\n","\n","# ============= STATISTICAL UTILITIES =============\n","def bootstrap_confidence_interval(data, confidence=0.95, n_bootstrap=N_BOOTSTRAP):\n","    \"\"\"Calculate bootstrap confidence interval.\"\"\"\n","    if len(data) <= 1:\n","        mean_val = data[0] if len(data) == 1 else 0\n","        return mean_val, mean_val, mean_val\n","\n","    bootstrap_means = []\n","    for _ in range(n_bootstrap):\n","        sample = np.random.choice(data, size=len(data), replace=True)\n","        bootstrap_means.append(np.mean(sample))\n","\n","    alpha = 1 - confidence\n","    lower = np.percentile(bootstrap_means, 100 * alpha/2)\n","    upper = np.percentile(bootstrap_means, 100 * (1 - alpha/2))\n","    return np.mean(data), lower, upper\n","\n","def compute_cross_seed_statistics(df):\n","    \"\"\"Compute statistics across multiple seeds.\"\"\"\n","    if 'best_match_error' not in df.columns:\n","        return df\n","\n","    grouped = df.groupby(['sequence', 'base', 'k'] if 'k' in df.columns else ['sequence', 'source_base'])\n","\n","    summary_stats = []\n","    for name, group in grouped:\n","        errors = group['best_match_error'].dropna()\n","        if len(errors) > 0:\n","            mean_error, ci_lower, ci_upper = bootstrap_confidence_interval(errors)\n","\n","            summary_stats.append({\n","                'sequence': name[0] if isinstance(name, tuple) else name,\n","                'base': name[1] if isinstance(name, tuple) else None,\n","                'k': name[2] if isinstance(name, tuple) and len(name) > 2 else None,\n","                'mean_error': mean_error,\n","                'std_error': np.std(errors),\n","                'min_error': np.min(errors),\n","                'max_error': np.max(errors),\n","                'ci_lower': ci_lower,\n","                'ci_upper': ci_upper,\n","                'n_seeds': len(errors),\n","                'best_match_name': group['best_match_name'].mode().iloc[0] if 'best_match_name' in group.columns and len(group['best_match_name'].mode()) > 0 else None\n","            })\n","\n","    return pd.DataFrame(summary_stats)\n","\n","# ============= CYMATICS VISUALIZATION =============\n","def generate_comprehensive_cymatics(constants_lib):\n","    \"\"\"Generate comprehensive cymatics visualizations.\"\"\"\n","    print(f\"\\nüéµ Generating cymatics visualizations...\")\n","\n","    # Key constants with their frequencies\n","    key_constants = {\n","        'Fine Structure (Œ±)': 137.035999084,\n","        'Golden Ratio (œÜ)': float((Decimal(1) + Decimal(5).sqrt()) / 2),\n","        'œÄ': math.pi,\n","        'e': math.e,\n","        'Feigenbaum Œ¥': 4.6692016091029906718532038204662,\n","        'Feigenbaum Œ±': 2.5029078750958928222839028732182,\n","        'sqrt(2)': math.sqrt(2),\n","        'sqrt(3)': math.sqrt(3),\n","        'Catalan': 0.9159655941772190150546035149324,\n","    }\n","\n","    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n","    axes = axes.flatten()\n","\n","    for i, (name, freq) in enumerate(key_constants.items()):\n","        if i >= len(axes):\n","            break\n","\n","        # Create cymatics pattern\n","        x = np.linspace(-2, 2, 300)\n","        y = np.linspace(-2, 2, 300)\n","        X, Y = np.meshgrid(x, y)\n","        R = np.sqrt(X**2 + Y**2)\n","\n","        # Multiple frequency components for richer patterns\n","        Z1 = np.sin(freq * R * 2 * np.pi) * np.exp(-R * 1.5)\n","        Z2 = np.sin(freq * R * np.pi / 2) * np.exp(-R * 2)\n","        Z3 = np.cos(freq * R * np.pi) * np.exp(-R * 1.8)\n","\n","        # Combine frequencies\n","        Z = Z1 + 0.5 * Z2 + 0.3 * Z3\n","\n","        im = axes[i].imshow(Z, extent=[-2, 2, -2, 2], cmap='plasma', origin='lower')\n","        axes[i].set_title(f'{name}\\nFreq: {freq:.4f}', fontsize=10)\n","        axes[i].axis('off')\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{OUTDIR}/comprehensive_cymatics.png\", dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","# ============= PRECISION HEATMAPS =============\n","def create_precision_heatmaps(df_tier1, df_tier2, df_tier3):\n","    \"\"\"Create comprehensive precision heatmaps.\"\"\"\n","    print(f\"\\nüìà Creating precision heatmaps...\")\n","\n","    # Tier 1 heatmap\n","    if 'best_match_error' in df_tier1.columns:\n","        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","        # Tier 1: Sequence x Base heatmap\n","        pivot_data = df_tier1.groupby(['sequence', 'base'])['best_match_error'].min().unstack()\n","        log_data = np.log10(pivot_data.fillna(1))\n","\n","        im1 = axes[0,0].imshow(log_data.values, cmap='viridis_r', aspect='auto')\n","        axes[0,0].set_xticks(range(len(pivot_data.columns)))\n","        axes[0,0].set_xticklabels(pivot_data.columns)\n","        axes[0,0].set_yticks(range(len(pivot_data.index)))\n","        axes[0,0].set_yticklabels(pivot_data.index)\n","        axes[0,0].set_xlabel('Base')\n","        axes[0,0].set_ylabel('Sequence')\n","        axes[0,0].set_title('Tier 1: log10(Best Error) by Sequence and Base')\n","\n","        # Mark exceptional results\n","        for i, seq in enumerate(pivot_data.index):\n","            for j, base in enumerate(pivot_data.columns):\n","                if not pd.isna(pivot_data.loc[seq, base]) and pivot_data.loc[seq, base] < 1e-6:\n","                    axes[0,0].text(j, i, '‚òÖ', ha='center', va='center', color='red', fontsize=16)\n","\n","        # Tier 1: Sequence x K heatmap\n","        pivot_k = df_tier1[df_tier1['base'] == 10].groupby(['sequence', 'k'])['best_match_error'].min().unstack()\n","        log_k = np.log10(pivot_k.fillna(1))\n","\n","        im2 = axes[0,1].imshow(log_k.values, cmap='viridis_r', aspect='auto')\n","        axes[0,1].set_xticks(range(len(pivot_k.columns)))\n","        axes[0,1].set_xticklabels(pivot_k.columns)\n","        axes[0,1].set_yticks(range(len(pivot_k.index)))\n","        axes[0,1].set_yticklabels(pivot_k.index)\n","        axes[0,1].set_xlabel('K Value')\n","        axes[0,1].set_ylabel('Sequence')\n","        axes[0,1].set_title('Tier 1: log10(Best Error) by Sequence and K (Base 10)')\n","\n","        # Mark k=6 column\n","        axes[0,1].axvline(x=6, color='red', linestyle='--', alpha=0.7)\n","\n","        # Tier 2 heatmap (if available)\n","        if len(df_tier2) > 0 and 'best_match_error' in df_tier2.columns:\n","            # Group by operation type\n","            tier2_by_op = df_tier2.groupby(['source_sequence', 'operation_type'])['best_match_error'].min().unstack()\n","            log_tier2 = np.log10(tier2_by_op.fillna(1))\n","\n","            im3 = axes[1,0].imshow(log_tier2.values, cmap='plasma_r', aspect='auto')\n","            axes[1,0].set_xticks(range(len(tier2_by_op.columns)))\n","            axes[1,0].set_xticklabels(tier2_by_op.columns, rotation=45)\n","            axes[1,0].set_yticks(range(len(tier2_by_op.index)))\n","            axes[1,0].set_yticklabels(tier2_by_op.index)\n","            axes[1,0].set_xlabel('Operation Type')\n","            axes[1,0].set_ylabel('Source Sequence')\n","            axes[1,0].set_title('Tier 2: log10(Best Error) by Operation Type')\n","\n","        # Error distribution histogram\n","        all_errors = df_tier1['best_match_error'].values\n","        log_errors = np.log10(all_errors[all_errors > 0])\n","\n","        axes[1,1].hist(log_errors, bins=50, alpha=0.7, edgecolor='black')\n","        axes[1,1].set_xlabel('log10(Error)')\n","        axes[1,1].set_ylabel('Count')\n","        axes[1,1].set_title('Tier 1: Error Distribution')\n","        axes[1,1].axvline(x=np.log10(1e-6), color='red', linestyle='--', label='Ultra-precise threshold')\n","        axes[1,1].legend()\n","\n","        plt.tight_layout()\n","        plt.savefig(f\"{OUTDIR}/comprehensive_precision_heatmaps.png\", dpi=300, bbox_inches='tight')\n","        plt.close()\n","\n","# ============= MAIN COMPREHENSIVE ANALYSIS =============\n","def run_complete_emergence_analysis():\n","    \"\"\"Complete comprehensive emergence analysis with all components.\"\"\"\n","    start_time = time.time()\n","\n","    print(f\"üöÄ STARTING COMPLETE COMPREHENSIVE EMERGENCE MAPPING...\")\n","\n","    # Load existing Tier 1 results\n","    df_tier1 = load_tier1_results()\n","    if df_tier1 is None:\n","        print(\"‚ùå Cannot proceed without Tier 1 results\")\n","        return None, None, None, None\n","\n","    # Build comprehensive constants library\n","    constants_lib = build_comprehensive_constants()\n","\n","    # Initialize results containers\n","    all_tier2_results = []\n","    all_tier3_results = []\n","    pathway_flow_results = []\n","\n","    print(f\"\\nüî¨ Building Tier 2: Dyadic Operations...\")\n","\n","    # TIER 2: Apply dyadic operations to ALL Tier 1 estimates\n","    tier1_processed = 0\n","    total_tier1 = len(df_tier1)\n","\n","    for idx, row in df_tier1.iterrows():\n","        if pd.notna(row['estimate']) and row['estimate'] != 0:\n","            try:\n","                tier2_ops = tier2_dyadic_operations(Decimal(str(row['estimate'])), constants_lib)\n","\n","                for tier2_op in tier2_ops:\n","                    tier2_op.update({\n","                        'source_sequence': row['sequence'],\n","                        'source_base': row['base'],\n","                        'source_k': row['k'],\n","                        'source_seed_idx': row['seed_idx'],\n","                        'source_estimate': row['estimate'],\n","                        'source_error': row['best_match_error'],\n","                        'tier1_constant': row['best_match_name'],\n","                        'pathway': 'tier1‚Üítier2'\n","                    })\n","                    all_tier2_results.append(tier2_op)\n","\n","                tier1_processed += 1\n","                if tier1_processed % 100 == 0:\n","                    print(f\"  Processed {tier1_processed}/{total_tier1} Tier 1 results...\")\n","\n","            except Exception as e:\n","                continue\n","\n","    print(f\"‚úÖ Tier 2 complete: {len(all_tier2_results)} operations generated\")\n","\n","    print(f\"\\nüî¨ Building Tier 3: Complex Operations...\")\n","\n","    # TIER 3: Apply complex operations to best Tier 2 results\n","    if len(all_tier2_results) > 0:\n","        df_tier2 = pd.DataFrame(all_tier2_results)\n","\n","        # Take best results from each sequence-base-k combination for Tier 3\n","        tier2_best = df_tier2.loc[df_tier2.groupby(['source_sequence', 'source_base', 'source_k'])['best_match_error'].idxmin()]\n","\n","        tier2_processed = 0\n","        for idx, row in tier2_best.iterrows():\n","            if pd.notna(row['result']) and row['result'] != 0:\n","                try:\n","                    tier3_ops = tier3_complex_operations(row['result'], constants_lib)\n","\n","                    for tier3_op in tier3_ops:\n","                        tier3_op.update({\n","                            'source_sequence': row['source_sequence'],\n","                            'source_base': row['source_base'],\n","                            'source_k': row['source_k'],\n","                            'tier2_operation': row['operation'],\n","                            'tier2_result': row['result'],\n","                            'tier2_error': row['best_match_error'],\n","                            'pathway': 'tier1‚Üítier2‚Üítier3'\n","                        })\n","                        all_tier3_results.append(tier3_op)\n","\n","                    tier2_processed += 1\n","                    if tier2_processed % 50 == 0:\n","                        print(f\"  Processed {tier2_processed} Tier 2 results...\")\n","\n","                except Exception as e:\n","                    continue\n","\n","        print(f\"‚úÖ Tier 3 complete: {len(all_tier3_results)} operations generated\")\n","\n","    # PATHWAY B: Direct operations on key constants\n","    print(f\"\\nüî¨ Building Pathway B: Direct Constant Operations...\")\n","    key_constants = [\n","        constants_lib['fine_structure'],\n","        constants_lib['phi'],\n","        constants_lib['pi'],\n","        constants_lib['e'],\n","        constants_lib['inv137']\n","    ]\n","\n","    for const_name, const_val in [('fine_structure', constants_lib['fine_structure']),\n","                                  ('phi', constants_lib['phi']),\n","                                  ('pi', constants_lib['pi']),\n","                                  ('e', constants_lib['e']),\n","                                  ('inv137', constants_lib['inv137'])]:\n","        try:\n","            tier2_ops = tier2_dyadic_operations(const_val, constants_lib)\n","            for tier2_op in tier2_ops:\n","                tier2_op.update({\n","                    'source_constant': const_name,\n","                    'source_value': float(const_val),\n","                    'pathway': 'constant‚Üídyadic'\n","                })\n","                pathway_flow_results.append(tier2_op)\n","        except:\n","            continue\n","\n","    print(f\"‚úÖ Pathway B complete: {len(pathway_flow_results)} operations\")\n","\n","    # Convert to DataFrames\n","    df_tier2 = pd.DataFrame(all_tier2_results)\n","    df_tier3 = pd.DataFrame(all_tier3_results)\n","    df_pathways = pd.DataFrame(pathway_flow_results)\n","\n","    print(f\"\\nüìä Computing Statistical Summaries...\")\n","\n","    # Cross-seed statistics\n","    tier1_summary = compute_cross_seed_statistics(df_tier1)\n","\n","    if len(df_tier2) > 0:\n","        tier2_summary = compute_cross_seed_statistics(df_tier2)\n","    else:\n","        tier2_summary = pd.DataFrame()\n","\n","    # Monte Carlo analysis\n","    mc_results = monte_carlo_baseline_analysis(df_tier1, constants_lib)\n","\n","    print(f\"\\nüéµ Generating Visualizations...\")\n","\n","    # Generate comprehensive visualizations\n","    generate_comprehensive_cymatics(constants_lib)\n","    create_precision_heatmaps(df_tier1, df_tier2, df_tier3)\n","\n","    print(f\"\\nüíæ Saving Complete Results...\")\n","\n","    # Save all results\n","    df_tier1.to_csv(f\"{OUTDIR}/tier1_complete.csv\", index=False)\n","    df_tier2.to_csv(f\"{OUTDIR}/tier2_complete.csv\", index=False)\n","    df_tier3.to_csv(f\"{OUTDIR}/tier3_complete.csv\", index=False)\n","    df_pathways.to_csv(f\"{OUTDIR}/pathway_flow_complete.csv\", index=False)\n","    tier1_summary.to_csv(f\"{OUTDIR}/tier1_summary_complete.csv\", index=False)\n","    if len(tier2_summary) > 0:\n","        tier2_summary.to_csv(f\"{OUTDIR}/tier2_summary_complete.csv\", index=False)\n","    mc_results.to_csv(f\"{OUTDIR}/monte_carlo_complete.csv\", index=False)\n","\n","    runtime = time.time() - start_time\n","\n","    print(f\"\\nüéâ COMPLETE COMPREHENSIVE EMERGENCE MAPPING FINISHED!\")\n","    print(f\"‚è±Ô∏è  Runtime: {runtime:.1f} seconds ({runtime/60:.1f} minutes)\")\n","    print(f\"üìä FINAL STATISTICS:\")\n","    print(f\"   Tier 1 results: {len(df_tier1)}\")\n","    print(f\"   Tier 2 results: {len(df_tier2)}\")\n","    print(f\"   Tier 3 results: {len(df_tier3)}\")\n","    print(f\"   Pathway flows: {len(df_pathways)}\")\n","    print(f\"   Monte Carlo baselines: {len(mc_results)}\")\n","    print(f\"üìÅ All results saved to: {OUTDIR}\")\n","\n","    # Show best results from each tier\n","    if len(df_tier2) > 0 and 'best_match_error' in df_tier2.columns:\n","        best_tier2 = df_tier2.loc[df_tier2['best_match_error'].idxmin()]\n","        print(f\"\\nü•á BEST TIER 2 RESULT:\")\n","        print(f\"   {best_tier2['source_sequence']} ‚Üí {best_tier2['operation']}\")\n","        print(f\"   {best_tier2['best_match_name']} (error: {best_tier2['best_match_error']:.2e})\")\n","\n","    if len(df_tier3) > 0 and 'best_match_error' in df_tier3.columns:\n","        best_tier3 = df_tier3.loc[df_tier3['best_match_error'].idxmin()]\n","        print(f\"\\nü•á BEST TIER 3 RESULT:\")\n","        print(f\"   {best_tier3['source_sequence']} ‚Üí {best_tier3['tier2_operation']} ‚Üí {best_tier3['operation']}\")\n","        print(f\"   {best_tier3['best_match_name']} (error: {best_tier3['best_match_error']:.2e})\")\n","\n","    return df_tier1, df_tier2, df_tier3, mc_results\n","\n","# ============= EXECUTE COMPLETE ANALYSIS =============\n","if __name__ == \"__main__\":\n","    try:\n","        df_tier1, df_tier2, df_tier3, mc_results = run_complete_emergence_analysis()\n","\n","        if df_tier1 is not None:\n","            print(f\"\\nüéØ COMPLETE COMPREHENSIVE EMERGENCE MAPPING SUCCESS!\")\n","            print(f\"üî¨ All tiers analyzed with full statistical rigor\")\n","            print(f\"üéµ Cymatics patterns generated\")\n","            print(f\"üìà Precision heatmaps created\")\n","            print(f\"üìä Monte Carlo baselines computed\")\n","            print(f\"üåä Pathway flow analysis complete\")\n","\n","    except Exception as e:\n","        print(f\"‚ùå Error: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()"]},{"cell_type":"code","source":["# === DEBUG SCRIPT: INSPECT ACTUAL CSV STRUCTURE ===\n","import pandas as pd\n","\n","print(\"üîç DEBUGGING: Inspecting actual CSV structure...\")\n","\n","try:\n","    # Load the CSV and inspect its actual structure\n","    df = pd.read_csv('tier1_corrected.csv')\n","\n","    print(f\"‚úÖ CSV loaded successfully\")\n","    print(f\"üìä Shape: {df.shape}\")\n","\n","    print(f\"\\nüìã ACTUAL COLUMN NAMES:\")\n","    for i, col in enumerate(df.columns):\n","        print(f\"  {i}: '{col}'\")\n","\n","    print(f\"\\nüìù DATA TYPES:\")\n","    print(df.dtypes)\n","\n","    print(f\"\\nüëÄ FIRST 3 ROWS:\")\n","    print(df.head(3))\n","\n","    print(f\"\\nüéØ SAMPLE VALUES FROM KEY COLUMNS:\")\n","    if 'sequence' in df.columns:\n","        print(f\"  sequence: {df['sequence'].unique()}\")\n","    else:\n","        print(f\"  ‚ùå 'sequence' column not found\")\n","\n","    if 'base' in df.columns:\n","        print(f\"  base: {sorted(df['base'].unique())}\")\n","    else:\n","        print(f\"  ‚ùå 'base' column not found\")\n","\n","    if 'k' in df.columns:\n","        print(f\"  k: {sorted(df['k'].unique())}\")\n","    else:\n","        print(f\"  ‚ùå 'k' column not found\")\n","\n","    print(f\"\\nüîç CHECKING FOR MISSING/NULL VALUES:\")\n","    print(df.isnull().sum())\n","\n","except Exception as e:\n","    print(f\"‚ùå Error loading CSV: {str(e)}\")\n","    import traceback\n","    traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlmbqqlzJoFs","executionInfo":{"status":"ok","timestamp":1755201356930,"user_tz":240,"elapsed":33,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"a00fd6bc-324f-4da6-b211-a0e9dd99fbb1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["üîç DEBUGGING: Inspecting actual CSV structure...\n","‚úÖ CSV loaded successfully\n","üìä Shape: (864, 12)\n","\n","üìã ACTUAL COLUMN NAMES:\n","  0: 'base'\n","  1: 'k'\n","  2: 'estimate'\n","  3: 'D_value'\n","  4: 'triadic_multiplier'\n","  5: 'best_match_name'\n","  6: 'best_match_value'\n","  7: 'best_match_error'\n","  8: 'best_match_rel_error'\n","  9: 'sequence'\n","  10: 'seed_idx'\n","  11: 'seed_value'\n","\n","üìù DATA TYPES:\n","base                      int64\n","k                         int64\n","estimate                float64\n","D_value                 float64\n","triadic_multiplier        int64\n","best_match_name          object\n","best_match_value        float64\n","best_match_error        float64\n","best_match_rel_error    float64\n","sequence                 object\n","seed_idx                  int64\n","seed_value                int64\n","dtype: object\n","\n","üëÄ FIRST 3 ROWS:\n","   base  k  estimate  D_value  triadic_multiplier    best_match_name  \\\n","0    10  0  0.000010  0.01011                   1  catalan_div100000   \n","1    10  1  0.000030  0.01011                   3       pi_div100000   \n","2    10  2  0.000091  0.01011                   9   catalan_div10000   \n","\n","   best_match_value  best_match_error  best_match_rel_error sequence  \\\n","0          0.000009      9.504452e-07              0.103764        F   \n","1          0.000031      1.085623e-06              0.034556        F   \n","2          0.000092      6.056495e-07              0.006612        F   \n","\n","   seed_idx  seed_value  \n","0         0      123456  \n","1         0      123456  \n","2         0      123456  \n","\n","üéØ SAMPLE VALUES FROM KEY COLUMNS:\n","  sequence: ['F' 'R' 'TM' 'FIBBI']\n","  base: [np.int64(2), np.int64(3), np.int64(10)]\n","  k: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n","\n","üîç CHECKING FOR MISSING/NULL VALUES:\n","base                    0\n","k                       0\n","estimate                0\n","D_value                 0\n","triadic_multiplier      0\n","best_match_name         0\n","best_match_value        0\n","best_match_error        0\n","best_match_rel_error    0\n","sequence                0\n","seed_idx                0\n","seed_value              0\n","dtype: int64\n"]}]}]}