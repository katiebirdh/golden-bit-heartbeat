{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOD+fl0PM/m4DsSWbtjSLJC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from typing import Dict, Any\n","import scipy.stats as stats\n","from scipy import signal\n","from statsmodels.tsa.stattools import acf, pacf\n","\n","# Define a dummy ComprehensiveHypothesisSpace class for testing purposes\n","class ComprehensiveHypothesisSpace:\n","    def __init__(self):\n","        self.window_sizes = [10, 20, 30]\n","        self.mathematical_constants = {'phi': 1.618, 'pi': 3.14159, 'e': 2.718}\n","        self.frequency_bands = [(0, 0.1), (0.1, 0.5)]\n","\n","class ExhaustivePatternDetector:\n","    \"\"\"\n","    Test ALL possible patterns without bias.\n","    \"\"\"\n","\n","    def __init__(self, hypothesis_space: ComprehensiveHypothesisSpace):\n","        self.hypothesis_space = hypothesis_space\n","        self.all_results = []\n","\n","    def detect_all_patterns(self, data: pd.DataFrame) -> Dict[str, Any]:\n","        \"\"\"\n","        Exhaustively test all patterns in the hypothesis space.\n","        \"\"\"\n","        print(\"Starting detect_all_patterns...\")\n","        if data.empty or len(data) < 100:\n","            print(\"Data is empty or too short. Returning empty results.\")\n","            return {}\n","\n","        results = {\n","            'density_patterns': {},\n","            'ratio_patterns': {},\n","            'spectral_patterns': {},\n","            'scaling_patterns': {},\n","            'autocorrelation_patterns': {},\n","            'entropy_patterns': {}\n","        }\n","\n","        # 1. Test density convergence at ALL windows\n","        print(\"Testing density convergence...\")\n","        for window in self.hypothesis_space.window_sizes:\n","            if len(data) < window:\n","                continue\n","\n","            # Add print statement to inspect window value\n","            # print(f\"Testing density convergence for window: {window}\") # Keep this print for now\n","\n","            density_result = self._test_density_convergence(data, window)\n","            if density_result: # Only add if not empty\n","                results['density_patterns'][window] = density_result\n","\n","        # 2. Test ratio patterns for ALL targets\n","        print(\"Testing ratio patterns...\")\n","        ratio_results = self._test_all_ratios(data)\n","        if ratio_results: # Only add if not empty\n","            results['ratio_patterns'] = ratio_results\n","\n","        # 3. Spectral analysis across all frequency bands\n","        print(\"Performing spectral analysis...\")\n","        spectral_results = self._comprehensive_spectral_analysis(data)\n","        if spectral_results: # Only add if not empty\n","            results['spectral_patterns'] = spectral_results\n","\n","        # 4. Scaling analysis (Hurst, DFA, multifractal)\n","        print(\"Performing scaling analysis...\")\n","        scaling_results = self._scaling_analysis(data)\n","        if scaling_results: # Only add if not empty\n","            results['scaling_patterns'] = scaling_results\n","\n","        # 5. Autocorrelation structure\n","        print(\"Performing autocorrelation analysis...\")\n","        autocorr_results = self._autocorrelation_analysis(data)\n","        if autocorr_results: # Only add if not empty\n","            results['autocorrelation_patterns'] = autocorr_results\n","\n","        # 6. Information theoretic measures\n","        print(\"Performing entropy analysis...\")\n","        entropy_results = self._entropy_analysis(data)\n","        if entropy_results: # Only add if not empty\n","            results['entropy_patterns'] = entropy_results\n","\n","        print(f\"Finished detect_all_patterns. Results: {results}\")\n","        return results\n","\n","    def _test_density_convergence(self, data: pd.DataFrame, window: int) -> Dict:\n","        \"\"\"Test if binary patterns converge to specific densities\"\"\"\n","        print(f\"  _test_density_convergence called with window: {window}\")\n","        prices = data['Close'].values\n","\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, pd.Series):\n","            prices = prices.values\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        if len(prices) < 2:\n","            print(f\"    _test_density_convergence: prices too short ({len(prices)}). Returning empty.\")\n","            return {}\n","\n","        binary = (np.diff(prices) > 0).astype(int)\n","\n","        if len(binary) < window:\n","            print(f\"    _test_density_convergence: binary data too short ({len(binary)}) for window ({window}). Returning empty.\")\n","            return {}\n","\n","        densities = []\n","        # Ensure step size is at least 1 using an explicit check\n","        step = window // 4\n","        if step == 0:\n","            step = 1\n","\n","        # print(f\"    _test_density_convergence: range params: 0, {len(binary) - window}, {step}\") # Keep this print for now\n","        for i in range(0, len(binary) - window, step):\n","            segment = binary[i:i+window]\n","            densities.append(np.mean(segment))\n","\n","        if not densities:\n","            print(f\"    _test_density_convergence: no densities calculated. Returning empty.\")\n","            return {}\n","\n","        # Test against ALL mathematical constants\n","        results = {\n","            'observed_mean': float(np.mean(densities)),\n","            'observed_std': float(np.std(densities)),\n","            'n_observations': len(densities)\n","        }\n","\n","        # Test convergence to each constant\n","        for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","            if 0 <= const_value <= 1:  # Density must be in [0,1]\n","                # Added check for std deviation to avoid division by zero\n","                std_dev = np.std(densities)\n","                denominator = std_dev / np.sqrt(len(densities)) + 1e-10\n","                z_score = (np.mean(densities) - const_value) / denominator\n","                p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n","\n","                results[f'convergence_to_{const_name}'] = {\n","                    'z_score': float(z_score),\n","                    'p_value': float(p_value),\n","                    'distance': float(abs(np.mean(densities) - const_value))\n","                }\n","        print(f\"  _test_density_convergence finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _test_all_ratios(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"Test for all possible ratio patterns\"\"\"\n","        print(\"  _test_all_ratios called.\")\n","        results = {}\n","\n","        # Volume ratios\n","        if 'Volume' in data.columns:\n","            volumes = data['Volume'].values\n","            vol_ratios = []\n","            for i in range(len(volumes) - 1):\n","                if volumes[i+1] > 0:\n","                    ratio = volumes[i] / volumes[i+1]\n","                    if 0.01 < ratio < 100:  # Filter extreme outliers\n","                        vol_ratios.append(ratio)\n","\n","            if vol_ratios:\n","                # Convert list to numpy array before analysis\n","                vol_ratios = np.array(vol_ratios)\n","                analysis_result = self._analyze_ratio_distribution(vol_ratios)\n","                if analysis_result: results['volume_ratios'] = analysis_result\n","\n","        # Price ratios (high/low, close/open, etc.)\n","        price_ratios = {\n","            'high_low': data['High'] / data['Low'],\n","            'close_open': data['Close'] / data['Open'],\n","            'close_prev_close': data['Close'] / data['Close'].shift(1)\n","        }\n","\n","        for ratio_name, ratio_values in price_ratios.items():\n","            ratio_values = ratio_values.dropna()\n","            if len(ratio_values) > 100:\n","                analysis_result = self._analyze_ratio_distribution(ratio_values.values)\n","                if analysis_result: results[ratio_name] = analysis_result\n","        print(f\"  _test_all_ratios finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _analyze_ratio_distribution(self, ratios: np.ndarray) -> Dict:\n","        \"\"\"Analyze distribution of ratios against all targets\"\"\"\n","        print(f\"    _analyze_ratio_distribution called with {len(ratios)} ratios.\")\n","        from sklearn.neighbors import KernelDensity\n","\n","        # Ensure ratios is a numpy array\n","        if isinstance(ratios, list):\n","            ratios = np.array(ratios)\n","\n","        # Ensure it's 1D\n","        if len(ratios.shape) > 1:\n","            ratios = ratios.flatten()\n","\n","        # Filter out NaN and infinite values\n","        ratios = ratios[np.isfinite(ratios)]\n","\n","        if len(ratios) == 0:\n","            print(\"      _analyze_ratio_distribution: no finite ratios. Returning empty.\")\n","            return {}\n","\n","        # Find natural peaks in distribution\n","        try:\n","            kde = KernelDensity(bandwidth=0.05, kernel='gaussian')\n","            ratios_reshaped = ratios.reshape(-1, 1)\n","            kde.fit(ratios_reshaped)\n","\n","            x_range = np.linspace(0.5, 3, 500).reshape(-1, 1)\n","            density = np.exp(kde.score_samples(x_range))\n","            peaks, properties = signal.find_peaks(density, height=np.max(density)*0.2)\n","\n","            peak_values = x_range[peaks].flatten()\n","        except Exception as e:\n","            print(f\"      _analyze_ratio_distribution: Error during KDE or peak finding: {e}. Returning empty.\")\n","            return {}\n","\n","\n","        results = {\n","            'distribution_stats': {\n","                'mean': float(np.mean(ratios)),\n","                'std': float(np.std(ratios)),\n","                'median': float(np.median(ratios)),\n","                'mode': float(peak_values[np.argmax(density[peaks])]) if len(peaks) > 0 else np.nan\n","            },\n","            'discovered_peaks': peak_values.tolist(),\n","            'peak_heights': density[peaks].tolist()\n","        }\n","\n","        # Test proximity to each target ratio\n","        for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","            if 0.5 <= const_value <= 3.0:  # Reasonable ratio range\n","                # Find closest peak\n","                if len(peak_values) > 0:\n","                    distances = np.abs(peak_values - const_value)\n","                    min_distance = np.min(distances)\n","\n","                    # Statistical test: is the distribution centered around this constant?\n","                    # Added check for sufficient observations for t-test\n","                    if len(ratios) > 1:\n","                        t_stat, p_value = stats.ttest_1samp(ratios, const_value)\n","                    else:\n","                        t_stat, p_value = np.nan, np.nan # Not enough data for t-test\n","\n","\n","                    results[f'proximity_to_{const_name}'] = {\n","                        'min_peak_distance': float(min_distance),\n","                        't_statistic': float(t_stat) if not np.isnan(t_stat) else None,\n","                        'p_value': float(p_value) if not np.isnan(p_value) else None,\n","                        'mean_distance': float(abs(np.mean(ratios) - const_value))\n","                    }\n","                else:\n","                     results[f'proximity_to_{const_name}'] = {\n","                        'min_peak_distance': np.nan,\n","                        't_statistic': np.nan,\n","                        'p_value': np.nan,\n","                        'mean_distance': float(abs(np.mean(ratios) - const_value)) if len(ratios) > 0 else np.nan\n","                    }\n","        print(f\"    _analyze_ratio_distribution finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _comprehensive_spectral_analysis(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"Perform comprehensive frequency domain analysis\"\"\"\n","        print(\"  _comprehensive_spectral_analysis called.\")\n","        prices = data['Close'].values\n","\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        if len(prices) < 256:\n","            print(f\"    _comprehensive_spectral_analysis: prices too short ({len(prices)}). Returning empty.\")\n","            return {}\n","\n","        # Compute FFT\n","        try:\n","            fft_values = np.fft.fft(prices)\n","            freqs = np.fft.fftfreq(len(prices))\n","            power_spectrum = np.abs(fft_values) ** 2\n","        except Exception as e:\n","            print(f\"    _comprehensive_spectral_analysis: Error during FFT calculation: {e}. Returning empty.\")\n","            return {}\n","\n","\n","        results = {}\n","\n","        # Analyze each frequency band\n","        print(\"    _comprehensive_spectral_analysis: Analyzing frequency bands.\")\n","        for band_name, (low_freq, high_freq) in enumerate(self.hypothesis_space.frequency_bands):\n","            band_mask = (np.abs(freqs) >= low_freq) & (np.abs(freqs) <= high_freq)\n","            if np.any(band_mask):\n","                band_power = np.sum(power_spectrum[band_mask])\n","                total_power = np.sum(power_spectrum)\n","\n","                # Added check for total_power to avoid division by zero\n","                if total_power > 0:\n","                    results[f'band_{low_freq}_{high_freq}'] = {\n","                        'power_ratio': float(band_power / total_power),\n","                        'peak_frequency': float(freqs[band_mask][np.argmax(power_spectrum[band_mask])]) if np.sum(band_mask) > 0 else np.nan\n","                    }\n","                else:\n","                     results[f'band_{low_freq}_{high_freq}'] = {\n","                        'power_ratio': 0.0,\n","                        'peak_frequency': np.nan\n","                    }\n","\n","\n","        # Find dominant frequencies\n","        print(\"    _comprehensive_spectral_analysis: Finding dominant frequencies.\")\n","        try:\n","            peak_indices = signal.find_peaks(power_spectrum[:len(power_spectrum)//2],\n","                                            height=np.max(power_spectrum)*0.1)[0]\n","        except Exception as e:\n","            print(f\"    _comprehensive_spectral_analysis: Error during peak finding: {e}.\")\n","            peak_indices = []\n","\n","\n","        if len(peak_indices) > 0:\n","            dominant_freqs = freqs[peak_indices]\n","            # Added check for dominant_freqs > 0 before division\n","            dominant_periods = 1 / (dominant_freqs + 1e-10)\n","\n","            results['dominant_frequencies'] = {\n","                'frequencies': dominant_freqs[:10].tolist(),  # Top 10\n","                'periods': dominant_periods[:10].tolist(),\n","                'power_values': power_spectrum[peak_indices][:10].tolist()\n","            }\n","\n","            # Check if any periods match our constants\n","            for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","                period_matches = np.abs(dominant_periods - const_value)\n","                if len(period_matches) > 0:\n","                    min_match = np.min(period_matches)\n","                    results[f'period_match_{const_name}'] = float(min_match)\n","        print(f\"  _comprehensive_spectral_analysis finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _scaling_analysis(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"Analyze scaling properties (fractality, self-similarity)\"\"\"\n","        print(\"  _scaling_analysis called.\")\n","        prices = data['Close'].values\n","\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        if len(prices) < 100:\n","            print(f\"    _scaling_analysis: prices too short ({len(prices)}). Returning empty.\")\n","            return {}\n","\n","        results = {}\n","\n","        # 1. Hurst Exponent (multiple methods)\n","        print(\"    _scaling_analysis: Calculating Hurst exponents.\")\n","        try:\n","            # R/S Analysis\n","            hurst_rs = self._calculate_hurst_rs(prices)\n","            results['hurst_rs'] = float(hurst_rs)\n","        except Exception as e:\n","            print(f\"    _scaling_analysis: Error calculating Hurst (R/S): {e}.\")\n","            pass\n","\n","        try:\n","            # DFA (Detrended Fluctuation Analysis)\n","            hurst_dfa = self._calculate_hurst_dfa(prices)\n","            results['hurst_dfa'] = float(hurst_dfa)\n","        except Exception as e:\n","            print(f\"    _scaling_analysis: Error calculating Hurst (DFA): {e}.\")\n","            pass\n","\n","\n","        # 2. Multifractal spectrum\n","        print(\"    _scaling_analysis: Calculating multifractal spectrum.\")\n","        scaling_exponents = []\n","        for q in [-5, -2, -1, 0, 1, 2, 5]:\n","            try:\n","                exponent = self._calculate_scaling_exponent(prices, q)\n","                # Added check for valid exponent\n","                if not np.isnan(exponent) and not np.isinf(exponent):\n","                    scaling_exponents.append((q, float(exponent)))\n","            except Exception as e:\n","                print(f\"    _scaling_analysis: Error calculating scaling exponent for q={q}: {e}.\")\n","                pass\n","\n","        if scaling_exponents:\n","            results['multifractal_spectrum'] = scaling_exponents\n","\n","            # Width of multifractal spectrum (indicates complexity)\n","            exponents_only = [e[1] for e in scaling_exponents]\n","            # Added check for non-empty exponents_only list\n","            if exponents_only:\n","                 results['multifractal_width'] = float(max(exponents_only) - min(exponents_only))\n","            else:\n","                 results['multifractal_width'] = np.nan\n","\n","\n","        # 3. Test for specific scaling ratios\n","        print(\"    _scaling_analysis: Testing for specific scaling ratios.\")\n","        scales = [2, 3, 5, 8, 13, 21, 34, 55, 89, 144]\n","        scale_ratios = []\n","\n","        for scale in scales:\n","            if scale < len(prices):\n","                # Variance at different scales\n","                var_original = np.var(prices)\n","                # Added check for sufficient data for scaled variance calculation\n","                if len(prices) - scale >= scale:\n","                    var_scaled = np.var([np.mean(prices[i:i+scale])\n","                                        for i in range(0, len(prices)-scale, scale)])\n","                    if var_scaled > 0:\n","                        scale_ratios.append(var_original / var_scaled)\n","\n","        if scale_ratios:\n","            results['scale_variance_ratios'] = [float(r) for r in scale_ratios]\n","\n","            # Check if ratios match any constants\n","            for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","                distances = [abs(ratio - const_value) for ratio in scale_ratios]\n","                if distances:\n","                    results[f'scale_ratio_match_{const_name}'] = float(min(distances))\n","        print(f\"  _scaling_analysis finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _calculate_hurst_rs(self, prices: np.ndarray) -> float:\n","        \"\"\"Calculate Hurst exponent using R/S analysis\"\"\"\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        lags = range(2, min(100, len(prices)//2))\n","        # Added check for sufficient data for lags\n","        if len(lags) < 2: return 0.5 # Not enough data for R/S analysis\n","\n","        tau = [np.sqrt(np.std(np.subtract(prices[lag:], prices[:-lag]))) for lag in lags]\n","\n","        if len(tau) > 10:\n","            # Convert lags to list then array for polyfit\n","            lags_array = np.array(list(lags))\n","            tau_array = np.array(tau)\n","            # Added check for non-zero tau_array before log\n","            if np.min(tau_array) > 0:\n","                poly = np.polyfit(np.log(lags_array), np.log(tau_array), 1)\n","                return poly[0] * 2.0\n","        return 0.5\n","\n","    def _calculate_hurst_dfa(self, prices: np.ndarray) -> float:\n","        \"\"\"Calculate Hurst exponent using DFA\"\"\"\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        scales = np.logspace(1, min(3, np.log10(len(prices)//4)), 20).astype(int)\n","        fluct = []\n","\n","        for scale in scales:\n","            if scale < len(prices):\n","                # Divide into segments\n","                segments = len(prices) // scale\n","                fluctuations = []\n","\n","                for i in range(segments):\n","                    segment = prices[i*scale:(i+1)*scale]\n","                    # Detrend using linear fit\n","                    x = np.arange(len(segment))\n","                    # Added check for sufficient data in segment for polyfit\n","                    if len(segment) > 1:\n","                        poly = np.polyfit(x, segment, 1)\n","                        fit = np.polyval(poly, x)\n","                        fluctuations.append(np.sqrt(np.mean((segment - fit)**2)))\n","\n","                if fluctuations:\n","                    fluct.append(np.mean(fluctuations))\n","\n","        if len(fluct) > 5:\n","            # Added check for non-zero scales and fluct before log\n","            if np.min(scales[:len(fluct)]) > 0 and np.min(fluct) > 0:\n","                poly = np.polyfit(np.log(scales[:len(fluct)]), np.log(fluct), 1)\n","                return poly[0]\n","        return 0.5\n","\n","    def _calculate_scaling_exponent(self, prices: np.ndarray, q: float) -> float:\n","        \"\"\"Calculate generalized scaling exponent for multifractal analysis\"\"\"\n","        # Ensure prices is a numpy array\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","\n","        scales = np.logspace(1, min(3, np.log10(len(prices)//4)), 10).astype(int)\n","        fluctuations = []\n","\n","        for scale in scales:\n","            if scale < len(prices):\n","                segments = len(prices) // scale\n","                seg_flucts = []\n","\n","                for i in range(segments):\n","                    segment = prices[i*scale:(i+1)*scale]\n","                    # Added check for sufficient data in segment for std\n","                    if len(segment) > 1:\n","                        seg_flucts.append(np.std(segment))\n","\n","                if seg_flucts and q != 0:\n","                     # Added check for positive seg_flucts before power and mean\n","                    positive_seg_flucts = [f for f in seg_flucts if f > 0]\n","                    if positive_seg_flucts:\n","                        fluctuations.append(np.mean([f**q for f in positive_seg_flucts])**(1/q))\n","                elif seg_flucts:\n","                     # Added check for positive seg_flucts before log and mean\n","                    positive_seg_flucts = [f for f in seg_flucts if f > 0]\n","                    if positive_seg_flucts:\n","                        fluctuations.append(np.exp(np.mean([np.log(f) for f in positive_seg_flucts])))\n","\n","\n","        if len(fluctuations) > 3:\n","            # Added check for non-zero scales and fluctuations before log\n","            if np.min(scales[:len(fluctuations)]) > 0 and np.min(fluctuations) > 0:\n","                poly = np.polyfit(np.log(scales[:len(fluctuations)]), np.log(fluctuations), 1)\n","                return poly[0]\n","        return 0\n","\n","    def _autocorrelation_analysis(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"Analyze autocorrelation structure\"\"\"\n","        print(\"  _autocorrelation_analysis called.\")\n","        returns = data['returns'].dropna().values\n","\n","        if len(returns) < 100:\n","            print(f\"    _autocorrelation_analysis: returns too short ({len(returns)}). Returning empty.\")\n","            return {}\n","\n","        results = {}\n","\n","        try:\n","            # Calculate ACF and PACF\n","            max_lag = min(100, len(returns)//4)\n","            # Added check for sufficient data for acf/pacf\n","            if max_lag > 0 and len(returns) > max_lag:\n","                acf_values = acf(returns, nlags=max_lag)\n","                pacf_values = pacf(returns, nlags=max_lag)\n","            else:\n","                print(f\"    _autocorrelation_analysis: not enough data for acf/pacf calculation (returns={len(returns)}, max_lag={max_lag}). Returning empty.\")\n","                return {}\n","\n","\n","            # Find significant lags\n","            # Added check for non-zero len(returns) before division\n","            confidence_interval = 1.96 / (np.sqrt(len(returns)) + 1e-10)\n","            significant_acf_lags = np.where(np.abs(acf_values) > confidence_interval)[0]\n","            significant_pacf_lags = np.where(np.abs(pacf_values) > confidence_interval)[0]\n","\n","            results['significant_lags'] = {\n","                'acf': significant_acf_lags.tolist(),\n","                'pacf': significant_pacf_lags.tolist()\n","            }\n","\n","            # Check if any significant lags match our constants\n","            for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","                if 1 <= const_value <= max_lag:\n","                    lag_int = int(const_value)\n","                    # Added bounds checking for acf_values and pacf_values access\n","                    if lag_int < len(acf_values):\n","                         results[f'lag_{const_name}_acf'] = float(acf_values[lag_int])\n","                    if lag_int < len(pacf_values):\n","                         results[f'lag_{const_name}_pacf'] = float(pacf_values[lag_int])\n","\n","\n","            # Ljung-Box test for autocorrelation\n","            print(\"    _autocorrelation_analysis: Performing Ljung-Box test.\")\n","            try:\n","                from statsmodels.stats.diagnostic import acorr_ljungbox\n","                # Added check for sufficient data for Ljung-Box test\n","                lb_lags = min(40, len(returns)//5)\n","                if lb_lags > 0 and len(returns) > lb_lags:\n","                    lb_stats = acorr_ljungbox(returns, lags=lb_lags, return_df=True)\n","\n","                    results['ljung_box'] = {\n","                        'min_p_value': float(lb_stats['lb_pvalue'].min()),\n","                        'autocorrelation_present': lb_stats['lb_pvalue'].min() < 0.05\n","                    }\n","                else:\n","                    print(f\"    _autocorrelation_analysis: not enough data for Ljung-Box test (returns={len(returns)}, lags={lb_lags}). Skipping.\")\n","\n","            except Exception as e:\n","                print(f\"    _autocorrelation_analysis: Error during Ljung-Box test: {e}. Skipping.\")\n","                pass\n","\n","        except Exception as e:\n","            print(f\"  Autocorrelation analysis error: {e}\")\n","\n","        print(f\"  _autocorrelation_analysis finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _entropy_analysis(self, data: pd.DataFrame) -> Dict:\n","        \"\"\"Calculate various entropy measures\"\"\"\n","        print(\"  _entropy_analysis called.\")\n","        prices = data['Close'].values\n","        returns = data['returns'].dropna().values\n","\n","        # Ensure arrays\n","        if isinstance(prices, list):\n","            prices = np.array(prices)\n","        if isinstance(returns, list):\n","            returns = np.array(returns)\n","\n","        if len(returns) < 100:\n","            print(f\"    _entropy_analysis: returns too short ({len(returns)}). Returning empty.\")\n","            return {}\n","\n","        results = {}\n","\n","        # 1. Shannon entropy of returns\n","        print(\"    _entropy_analysis: Calculating Shannon entropy.\")\n","        # Added check for sufficient data for histogram\n","        if len(returns) > 1:\n","            hist, bins = np.histogram(returns, bins=50, density=True)\n","            hist = hist[hist > 0]  # Remove zeros\n","            # Added check for non-empty hist before log\n","            if len(hist) > 0:\n","                 shannon_entropy = -np.sum(hist * np.log2(hist)) / len(hist)\n","                 results['shannon_entropy'] = float(shannon_entropy)\n","            else:\n","                 results['shannon_entropy'] = 0.0 # Or np.nan, depending on desired behavior\n","        else:\n","            print(\"    _entropy_analysis: not enough data for Shannon entropy histogram. Skipping.\")\n","\n","\n","        # 2. Approximate entropy\n","        print(\"    _entropy_analysis: Calculating Approximate entropy.\")\n","        # Added check for sufficient data for approximate entropy\n","        if len(prices) > 2:\n","            approx_entropy = self._approximate_entropy(prices, 2, 0.2 * np.std(prices))\n","            # Added check for valid entropy value\n","            if not np.isnan(approx_entropy) and not np.isinf(approx_entropy):\n","                 results['approximate_entropy'] = float(approx_entropy)\n","            else:\n","                 results['approximate_entropy'] = np.nan\n","        else:\n","            print(\"    _entropy_analysis: not enough data for Approximate entropy. Skipping.\")\n","\n","\n","        # 3. Sample entropy\n","        print(\"    _entropy_analysis: Calculating Sample entropy.\")\n","        # Added check for sufficient data for sample entropy\n","        if len(prices) > 2:\n","            sample_entropy = self._sample_entropy(prices, 2, 0.2 * np.std(prices))\n","            # Added check for valid entropy value\n","            if not np.isnan(sample_entropy) and not np.isinf(sample_entropy):\n","                 results['sample_entropy'] = float(sample_entropy)\n","            else:\n","                 results['sample_entropy'] = np.nan\n","        else:\n","            print(\"    _entropy_analysis: not enough data for Sample entropy. Skipping.\")\n","\n","        # 4. Permutation entropy\n","        print(\"    _entropy_analysis: Calculating Permutation entropy.\")\n","        # Define m and delay before using them in the calculation\n","        m = 3\n","        delay = 1\n","        # Added check for sufficient data for permutation entropy\n","        min_data_perm_entropy = delay * (m - 1) + 1 # Corrected formula: delay * (m - 1) + 1\n","        if len(prices) >= min_data_perm_entropy:\n","            perm_entropy = self._permutation_entropy(prices, m, delay)\n","            # Added check for valid entropy value\n","            if not np.isnan(perm_entropy) and not np.isinf(perm_entropy):\n","                results['permutation_entropy'] = float(perm_entropy)\n","            else:\n","                results['permutation_entropy'] = np.nan\n","        else:\n","            print(f\"    _entropy_analysis: not enough data for Permutation entropy ({len(prices)} < {min_data_perm_entropy}). Skipping.\")\n","            results['permutation_entropy'] = np.nan # Or some other indicator\n","\n","\n","        # Check if any entropy values match constants\n","        print(\"    _entropy_analysis: Checking entropy matches with constants.\")\n","        # Filter out None/NaN values before calculating distances\n","        valid_entropy_values = [ent for ent in [results.get('shannon_entropy'), results.get('approximate_entropy'), results.get('sample_entropy'), results.get('permutation_entropy')] if ent is not None and not np.isnan(ent)]\n","\n","        if valid_entropy_values:\n","            for const_name, const_value in self.hypothesis_space.mathematical_constants.items():\n","                if 0 < const_value < 10:  # Reasonable entropy range\n","                    distances = [abs(ent - const_value) for ent in valid_entropy_values]\n","                    results[f'entropy_match_{const_name}'] = float(min(distances))\n","        else:\n","            print(\"    _entropy_analysis: no valid entropy values calculated to check against constants.\")\n","\n","\n","        print(f\"  _entropy_analysis finished. Results: {len(results)} entries.\")\n","        return results\n","\n","    def _approximate_entropy(self, U: np.ndarray, m: int, r: float) -> float:\n","        \"\"\"Calculate approximate entropy\"\"\"\n","        # Ensure U is a numpy array\n","        if isinstance(U, list):\n","            U = np.array(U)\n","\n","        # Added check for sufficient data\n","        if len(U) < m + 1:\n","            return np.nan # Not enough data\n","\n","        def _maxdist(xi, xj, m):\n","            return max([abs(float(xi[k]) - float(xj[k])) for k in range(m)])\n","\n","        def _phi(m):\n","            patterns = np.array([U[i:i+m] for i in range(len(U) - m + 1)])\n","            C = np.zeros(len(patterns))\n","\n","            for i in range(len(patterns)):\n","                matching = np.sum([1 for j in range(len(patterns))\n","                                  if _maxdist(patterns[i], patterns[j], m) <= r])\n","                C[i] = matching / len(patterns)\n","\n","            # Added check for non-zero C before log\n","            C_positive = C[C > 0]\n","            if len(C_positive) > 0:\n","                 return np.sum(np.log(C_positive)) / len(C)\n","            else:\n","                 return np.nan # Cannot calculate log if all C are zero\n","\n","\n","        try:\n","            phi_m = _phi(m)\n","            phi_m1 = _phi(m + 1)\n","            # Added checks for valid phi_m and phi_m1\n","            if not np.isnan(phi_m) and not np.isnan(phi_m1) and phi_m > 0:\n","                return -np.log(phi_m1 / phi_m)\n","            else:\n","                return np.nan\n","        except Exception as e:\n","            print(f\"      _approximate_entropy: Error during calculation: {e}.\")\n","            return np.nan\n","\n","    def _sample_entropy(self, U: np.ndarray, m: int, r: float) -> float:\n","        \"\"\"Calculate sample entropy\"\"\"\n","        # Ensure U is a numpy array\n","        if isinstance(U, list):\n","            U = np.array(U)\n","\n","        # Added check for sufficient data\n","        if len(U) < m + 2: # Sample entropy requires m+2 data points\n","             return np.nan # Not enough data\n","\n","        def _maxdist(xi, xj, m):\n","            return max([abs(float(xi[k]) - float(xj[k])) for k in range(m)])\n","\n","        def _phi(m):\n","            patterns = np.array([U[i:i+m] for i in range(len(U) - m + 1)])\n","            matching = 0\n","\n","            for i in range(len(patterns)):\n","                for j in range(i+1, len(patterns)):\n","                    if _maxdist(patterns[i], patterns[j], m) <= r:\n","                        matching += 1\n","\n","            # Added check for sufficient patterns before division\n","            num_pairs = len(patterns) * (len(patterns) - 1) / 2\n","            return matching / num_pairs if num_pairs > 0 else 0\n","\n","        try:\n","            phi_m = _phi(m)\n","            phi_m1 = _phi(m + 1)\n","            # Added checks for valid phi_m and phi_m1 before log\n","            if phi_m > 0 and phi_m1 > 0:\n","                return -np.log(phi_m1 / phi_m)\n","            else:\n","                return np.nan # Cannot calculate log if phi_m or phi_m1 is zero or negative\n","        except Exception as e:\n","            print(f\"      _sample_entropy: Error during calculation: {e}.\")\n","            return np.nan\n","\n","    def _permutation_entropy(self, U: np.ndarray, m: int, delay: int) -> float:\n","        \"\"\"Calculate permutation entropy\"\"\"\n","        from itertools import permutations\n","\n","        # Ensure U is a numpy array\n","        if isinstance(U, list):\n","            U = np.array(U)\n","        n = len(U)\n","        # Added check for sufficient data\n","        min_data_points = delay * (m - 1) + 1\n","        if n < min_data_points:\n","            return np.nan # Not enough data\n","\n","        permutations_list = list(permutations(range(m)))\n","        c = np.zeros(len(permutations_list))\n","\n","        for i in range(n - delay * (m - 1)):\n","            # Added check for valid slice indices\n","            start = i\n","            end = i + delay * m\n","            step = delay\n","            if end <= n and step > 0:\n","                sorted_indices = np.argsort(U[start:end:step])\n","                for j, perm in enumerate(permutations_list):\n","                    if tuple(sorted_indices) == perm:\n","                        c[j] += 1\n","                        break\n","            else:\n","                # This case should ideally not be reached if n >= min_data_points and delay > 0\n","                print(f\"      _permutation_entropy: Invalid slice indices or step ({start}:{end}:{step}). Skipping segment.\")\n","\n","\n","        c = c[c > 0]\n","        if len(c) == 0:\n","            return 0\n","        c = c / c.sum()\n","        # Added check for non-zero c before log\n","        if np.min(c) > 0:\n","             return -np.sum(c * np.log2(c))\n","        else:\n","             # This case should not be reached if c = c[c > 0] and len(c) > 0\n","             print(\"      _permutation_entropy: encountered zero values after filtering for log calculation.\")\n","             return np.nan # Or handle as appropriate\n","\n","# Add a test case to check if the class is producing output\n","print(\"\\n--- Running test case ---\")\n","# Create dummy data\n","dummy_data = pd.DataFrame({\n","    'Close': np.random.rand(200),\n","    'Open': np.random.rand(200),\n","    'High': np.random.rand(200) + 0.1,\n","    'Low': np.random.rand(200) - 0.1,\n","    'Volume': np.random.randint(100, 1000, 200)\n","})\n","# Add a 'returns' column (needed for autocorrelation/entropy)\n","dummy_data['returns'] = dummy_data['Close'].pct_change()\n","\n","# Create dummy hypothesis space\n","dummy_hypothesis_space = ComprehensiveHypothesisSpace()\n","\n","# Instantiate and run the detector\n","dummy_detector = ExhaustivePatternDetector(dummy_hypothesis_space)\n","test_results = dummy_detector.detect_all_patterns(dummy_data)\n","\n","print(\"Test case finished. Results:\")\n","print(test_results)\n","print(\"--- Test case finished ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72YPY-yHZ-gU","executionInfo":{"status":"ok","timestamp":1754670252574,"user_tz":240,"elapsed":361,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"8eb25e74-2ae5-4188-ff42-b6493085abbd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Running test case ---\n","Starting detect_all_patterns...\n","Testing density convergence...\n","  _test_density_convergence called with window: 10\n","  _test_density_convergence finished. Results: 3 entries.\n","  _test_density_convergence called with window: 20\n","  _test_density_convergence finished. Results: 3 entries.\n","  _test_density_convergence called with window: 30\n","  _test_density_convergence finished. Results: 3 entries.\n","Testing ratio patterns...\n","  _test_all_ratios called.\n","    _analyze_ratio_distribution called with 199 ratios.\n","    _analyze_ratio_distribution finished. Results: 5 entries.\n","    _analyze_ratio_distribution called with 200 ratios.\n","    _analyze_ratio_distribution finished. Results: 5 entries.\n","    _analyze_ratio_distribution called with 200 ratios.\n","    _analyze_ratio_distribution finished. Results: 5 entries.\n","    _analyze_ratio_distribution called with 199 ratios.\n","    _analyze_ratio_distribution finished. Results: 5 entries.\n","  _test_all_ratios finished. Results: 4 entries.\n","Performing spectral analysis...\n","  _comprehensive_spectral_analysis called.\n","    _comprehensive_spectral_analysis: prices too short (200). Returning empty.\n","Performing scaling analysis...\n","  _scaling_analysis called.\n","    _scaling_analysis: Calculating Hurst exponents.\n","    _scaling_analysis: Calculating multifractal spectrum.\n","    _scaling_analysis: Testing for specific scaling ratios.\n","  _scaling_analysis finished. Results: 8 entries.\n","Performing autocorrelation analysis...\n","  _autocorrelation_analysis called.\n","    _autocorrelation_analysis: Performing Ljung-Box test.\n","  _autocorrelation_analysis finished. Results: 8 entries.\n","Performing entropy analysis...\n","  _entropy_analysis called.\n","    _entropy_analysis: Calculating Shannon entropy.\n","    _entropy_analysis: Calculating Approximate entropy.\n","    _entropy_analysis: Calculating Sample entropy.\n","    _entropy_analysis: Calculating Permutation entropy.\n","    _entropy_analysis: Checking entropy matches with constants.\n","  _entropy_analysis finished. Results: 7 entries.\n","Finished detect_all_patterns. Results: {'density_patterns': {10: {'observed_mean': 0.48210526315789476, 'observed_std': 0.08457814495410806, 'n_observations': 95}, 20: {'observed_mean': 0.4847222222222223, 'observed_std': 0.05871239274402292, 'n_observations': 36}, 30: {'observed_mean': 0.47733333333333333, 'observed_std': 0.041867515914953544, 'n_observations': 25}}, 'ratio_patterns': {'volume_ratios': {'distribution_stats': {'mean': 1.4355213316892608, 'std': 1.284230406291365, 'median': 1.0703125, 'mode': 0.9909819639278556}, 'discovered_peaks': [0.530060120240481, 0.8106212424849699, 0.9909819639278556, 1.2665330661322645, 1.5470941883767535, 1.907815631262525, 2.103206412825651, 2.7094188376753507], 'peak_heights': [0.5798267390436763, 0.45449756736398356, 0.626315246111952, 0.41666845286479204, 0.42219918367221454, 0.3055219664505352, 0.14270079859267815, 0.19450287367979216], 'proximity_to_phi': {'min_peak_distance': 0.0709058116232466, 't_statistic': -1.9994094926033812, 'p_value': 0.046931240465140564, 'mean_distance': 0.18247866831073933}, 'proximity_to_e': {'min_peak_distance': 0.008581162324649227, 't_statistic': -14.052053575464006, 'p_value': 1.4401145643607262e-31, 'mean_distance': 1.2824786683107392}}, 'high_low': {'distribution_stats': {'mean': 2.7572880488281455, 'std': 25.684516045220352, 'median': 1.3090846260152182, 'mode': 1.2264529058116231}, 'discovered_peaks': [0.655310621242485, 0.8557114228456913, 1.036072144288577, 1.2264529058116231, 1.5521042084168337, 1.6823647294589177, 1.9779559118236472, 2.0831663326653307, 2.3086172344689375], 'peak_heights': [0.4775340473637032, 0.44056358271516916, 0.42137253591386076, 0.5268962535450034, 0.3341081942206518, 0.2903234323393437, 0.16157242078754583, 0.1566027798761951, 0.21163524334878037], 'proximity_to_phi': {'min_peak_distance': 0.06436472945891758, 't_statistic': 0.6257324717083047, 'p_value': 0.532207409124306, 'mean_distance': 1.1392880488281454}, 'proximity_to_e': {'min_peak_distance': 0.40938276553106245, 't_statistic': 0.02157821977253127, 'p_value': 0.982806028252323, 'mean_distance': 0.039288048828145516}}, 'close_open': {'distribution_stats': {'mean': 2.8815410698972386, 'std': 8.423462600217237, 'median': 0.9664966487887969, 'mode': 0.7154308617234468}, 'discovered_peaks': [0.5701402805611222, 0.7154308617234468, 1.0711422845691383, 1.2615230460921842, 1.6723446893787575, 1.8376753507014028, 2.0080160320641283, 2.3286573146292584, 2.8547094188376754], 'peak_heights': [0.4361440736974509, 0.534065270690735, 0.45528861280880295, 0.49556228610370134, 0.24916408530897335, 0.21134454254959392, 0.13300762876939617, 0.13976890002321773, 0.14229233678008676], 'proximity_to_phi': {'min_peak_distance': 0.05434468937875736, 't_statistic': 2.1160467041244084, 'p_value': 0.03558399471785036, 'mean_distance': 1.2635410698972385}, 'proximity_to_e': {'min_peak_distance': 0.1367094188376754, 't_statistic': 0.27388151457014037, 'p_value': 0.7844597468184703, 'mean_distance': 0.1635410698972386}}, 'close_prev_close': {'distribution_stats': {'mean': 10.73919977368401, 'std': 53.11711270004738, 'median': 0.9426464947041845, 'mode': 0.845691382765531}, 'discovered_peaks': [0.6953907815631263, 0.845691382765531, 1.1212424849699398, 1.7124248496993988, 2.0230460921843685, 2.2484969939879758], 'peak_heights': [0.5508205425082299, 0.6022476693087954, 0.4114587729858885, 0.2023222418878236, 0.1575576438554647, 0.20542918957506973], 'proximity_to_phi': {'min_peak_distance': 0.09442484969939868, 't_statistic': 2.416295822134204, 'p_value': 0.01658672372933735, 'mean_distance': 9.12119977368401}, 'proximity_to_e': {'min_peak_distance': 0.4695030060120242, 't_statistic': 2.1248949680474287, 'p_value': 0.034835344716375914, 'mean_distance': 8.02119977368401}}}, 'spectral_patterns': {}, 'scaling_patterns': {'hurst_rs': -0.00031456792411884023, 'hurst_dfa': 0.07256509758008858, 'multifractal_spectrum': [(-5, 0.08246829011070851), (-2, 0.061438779994443325), (-1, 0.054770042172688674), (0, 0.04837652573225885), (1, 0.04229549295201955), (2, 0.036547021109800085), (5, 0.021297341971607996)], 'multifractal_width': 0.06117094813910051, 'scale_variance_ratios': [2.2188713628972976, 3.255358718677845, 6.764366348389938, 8.193719079009725, 10.697397227885974, 42.54253289701724, 67.64120455265194, 138.5233131111662, 511.567377781534], 'scale_ratio_match_phi': 0.6008713628972975, 'scale_ratio_match_pi': 0.1137687186778451, 'scale_ratio_match_e': 0.4991286371027024}, 'autocorrelation_patterns': {'significant_lags': {'acf': [0], 'pacf': [0, 42, 45]}, 'lag_phi_acf': -0.03700291760830408, 'lag_phi_pacf': -0.037189801030568216, 'lag_pi_acf': -0.03206247465591657, 'lag_pi_pacf': -0.03444768717537545, 'lag_e_acf': -0.023302453014733496, 'lag_e_pacf': -0.02495662448313092, 'ljung_box': {'min_p_value': 0.528246131869898, 'autocorrelation_present': False}}, 'entropy_patterns': {'shannon_entropy': 0.03646659880377448, 'approximate_entropy': nan, 'sample_entropy': 2.282085289786643, 'permutation_entropy': 2.5749960244709893, 'entropy_match_phi': 0.6640852897866429, 'entropy_match_pi': 0.5665939755290106, 'entropy_match_e': 0.14300397552901067}}\n","Test case finished. Results:\n","{'density_patterns': {10: {'observed_mean': 0.48210526315789476, 'observed_std': 0.08457814495410806, 'n_observations': 95}, 20: {'observed_mean': 0.4847222222222223, 'observed_std': 0.05871239274402292, 'n_observations': 36}, 30: {'observed_mean': 0.47733333333333333, 'observed_std': 0.041867515914953544, 'n_observations': 25}}, 'ratio_patterns': {'volume_ratios': {'distribution_stats': {'mean': 1.4355213316892608, 'std': 1.284230406291365, 'median': 1.0703125, 'mode': 0.9909819639278556}, 'discovered_peaks': [0.530060120240481, 0.8106212424849699, 0.9909819639278556, 1.2665330661322645, 1.5470941883767535, 1.907815631262525, 2.103206412825651, 2.7094188376753507], 'peak_heights': [0.5798267390436763, 0.45449756736398356, 0.626315246111952, 0.41666845286479204, 0.42219918367221454, 0.3055219664505352, 0.14270079859267815, 0.19450287367979216], 'proximity_to_phi': {'min_peak_distance': 0.0709058116232466, 't_statistic': -1.9994094926033812, 'p_value': 0.046931240465140564, 'mean_distance': 0.18247866831073933}, 'proximity_to_e': {'min_peak_distance': 0.008581162324649227, 't_statistic': -14.052053575464006, 'p_value': 1.4401145643607262e-31, 'mean_distance': 1.2824786683107392}}, 'high_low': {'distribution_stats': {'mean': 2.7572880488281455, 'std': 25.684516045220352, 'median': 1.3090846260152182, 'mode': 1.2264529058116231}, 'discovered_peaks': [0.655310621242485, 0.8557114228456913, 1.036072144288577, 1.2264529058116231, 1.5521042084168337, 1.6823647294589177, 1.9779559118236472, 2.0831663326653307, 2.3086172344689375], 'peak_heights': [0.4775340473637032, 0.44056358271516916, 0.42137253591386076, 0.5268962535450034, 0.3341081942206518, 0.2903234323393437, 0.16157242078754583, 0.1566027798761951, 0.21163524334878037], 'proximity_to_phi': {'min_peak_distance': 0.06436472945891758, 't_statistic': 0.6257324717083047, 'p_value': 0.532207409124306, 'mean_distance': 1.1392880488281454}, 'proximity_to_e': {'min_peak_distance': 0.40938276553106245, 't_statistic': 0.02157821977253127, 'p_value': 0.982806028252323, 'mean_distance': 0.039288048828145516}}, 'close_open': {'distribution_stats': {'mean': 2.8815410698972386, 'std': 8.423462600217237, 'median': 0.9664966487887969, 'mode': 0.7154308617234468}, 'discovered_peaks': [0.5701402805611222, 0.7154308617234468, 1.0711422845691383, 1.2615230460921842, 1.6723446893787575, 1.8376753507014028, 2.0080160320641283, 2.3286573146292584, 2.8547094188376754], 'peak_heights': [0.4361440736974509, 0.534065270690735, 0.45528861280880295, 0.49556228610370134, 0.24916408530897335, 0.21134454254959392, 0.13300762876939617, 0.13976890002321773, 0.14229233678008676], 'proximity_to_phi': {'min_peak_distance': 0.05434468937875736, 't_statistic': 2.1160467041244084, 'p_value': 0.03558399471785036, 'mean_distance': 1.2635410698972385}, 'proximity_to_e': {'min_peak_distance': 0.1367094188376754, 't_statistic': 0.27388151457014037, 'p_value': 0.7844597468184703, 'mean_distance': 0.1635410698972386}}, 'close_prev_close': {'distribution_stats': {'mean': 10.73919977368401, 'std': 53.11711270004738, 'median': 0.9426464947041845, 'mode': 0.845691382765531}, 'discovered_peaks': [0.6953907815631263, 0.845691382765531, 1.1212424849699398, 1.7124248496993988, 2.0230460921843685, 2.2484969939879758], 'peak_heights': [0.5508205425082299, 0.6022476693087954, 0.4114587729858885, 0.2023222418878236, 0.1575576438554647, 0.20542918957506973], 'proximity_to_phi': {'min_peak_distance': 0.09442484969939868, 't_statistic': 2.416295822134204, 'p_value': 0.01658672372933735, 'mean_distance': 9.12119977368401}, 'proximity_to_e': {'min_peak_distance': 0.4695030060120242, 't_statistic': 2.1248949680474287, 'p_value': 0.034835344716375914, 'mean_distance': 8.02119977368401}}}, 'spectral_patterns': {}, 'scaling_patterns': {'hurst_rs': -0.00031456792411884023, 'hurst_dfa': 0.07256509758008858, 'multifractal_spectrum': [(-5, 0.08246829011070851), (-2, 0.061438779994443325), (-1, 0.054770042172688674), (0, 0.04837652573225885), (1, 0.04229549295201955), (2, 0.036547021109800085), (5, 0.021297341971607996)], 'multifractal_width': 0.06117094813910051, 'scale_variance_ratios': [2.2188713628972976, 3.255358718677845, 6.764366348389938, 8.193719079009725, 10.697397227885974, 42.54253289701724, 67.64120455265194, 138.5233131111662, 511.567377781534], 'scale_ratio_match_phi': 0.6008713628972975, 'scale_ratio_match_pi': 0.1137687186778451, 'scale_ratio_match_e': 0.4991286371027024}, 'autocorrelation_patterns': {'significant_lags': {'acf': [0], 'pacf': [0, 42, 45]}, 'lag_phi_acf': -0.03700291760830408, 'lag_phi_pacf': -0.037189801030568216, 'lag_pi_acf': -0.03206247465591657, 'lag_pi_pacf': -0.03444768717537545, 'lag_e_acf': -0.023302453014733496, 'lag_e_pacf': -0.02495662448313092, 'ljung_box': {'min_p_value': 0.528246131869898, 'autocorrelation_present': False}}, 'entropy_patterns': {'shannon_entropy': 0.03646659880377448, 'approximate_entropy': nan, 'sample_entropy': 2.282085289786643, 'permutation_entropy': 2.5749960244709893, 'entropy_match_phi': 0.6640852897866429, 'entropy_match_pi': 0.5665939755290106, 'entropy_match_e': 0.14300397552901067}}\n","--- Test case finished ---\n"]}]}]}