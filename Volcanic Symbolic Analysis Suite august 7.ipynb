{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBQ+LDjstLN36s4fnReDvj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ================================================================\n","# ðŸŒ‹ VOLCANIC SYMBOLIC ANALYSIS SUITE - COMPLETE IMPLEMENTATION\n","# ================================================================\n","# Version: 2.0 - Super Thorough Edition\n","# Author: Symbolic Analysis Research Team\n","# Date: January 2025\n","#\n","# This notebook extracts and analyzes symbolic patterns from real\n","# volcanic sensor data to identify non-random structures\n","# ================================================================\n","\n","# %%\n","# CELL 1: COMPLETE ENVIRONMENT SETUP\n","# Install all required packages with specific versions for reproducibility\n","\n","!pip install pandas numpy matplotlib scipy requests plotly seaborn scikit-learn statsmodels --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import signal, stats\n","from scipy.ndimage import gaussian_filterd1\n","from scipy.signal import cwt, morlet2  # Import cwt and morlet2 specifically\n","import requests\n","import json\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from plotly.subplots import make_subplots # Import make_subplots\n","import plotly # Import plotly\n","from datetime import datetime, timedelta\n","import warnings\n","import zipfile\n","import io\n","from typing import List, Dict, Tuple, Optional\n","import hashlib\n","import base64\n","import zlib\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","\n","warnings.filterwarnings('ignore')\n","np.random.seed(42)\n","\n","# Configure plotting aesthetics\n","try:\n","    plt.style.use('seaborn-v0_8-darkgrid')\n","except:\n","    try:\n","        plt.style.use('seaborn-darkgrid')\n","    except:\n","        plt.style.use('ggplot')  # Fallback style\n","sns.set_palette(\"husl\")\n","\n","print(\"âœ… Environment setup complete!\")\n","print(f\"NumPy version: {np.__version__}\")\n","print(f\"Pandas version: {pd.__version__}\")\n","\n","# %%\n","# CELL 2: DATA ACQUISITION FUNCTIONS WITH FALLBACKS\n","\n","class VolcanicDataCollector:\n","    \"\"\"\n","    Comprehensive volcanic data collection from multiple sources\n","    Includes error handling, caching, and fallback mechanisms\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.session = requests.Session()\n","        self.session.headers.update({\n","            'User-Agent': 'Mozilla/5.0 (Scientific Research Bot)'\n","        })\n","        self.data_cache = {}\n","\n","    def fetch_usgs_earthquakes(self, volcano_coords: Tuple[float, float],\n","                                radius_km: int = 50, days_back: int = 30) -> pd.DataFrame:\n","        \"\"\"\n","        Fetch earthquake data near volcano from USGS\n","        \"\"\"\n","        try:\n","            end_time = datetime.now()\n","            start_time = end_time - timedelta(days=days_back)\n","\n","            # USGS Earthquake API endpoint\n","            url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","            params = {\n","                'format': 'geojson',\n","                'starttime': start_time.isoformat(),\n","                'endtime': end_time.isoformat(),\n","                'latitude': volcano_coords[0],\n","                'longitude': volcano_coords[1],\n","                'maxradiuskm': radius_km,\n","                'minmagnitude': 0.5,\n","                'orderby': 'time'\n","            }\n","\n","            response = self.session.get(url, params=params, timeout=10)\n","            response.raise_for_status()\n","\n","            data = response.json()\n","\n","            # Extract features\n","            events = []\n","            for feature in data.get('features', []):\n","                props = feature['properties']\n","                events.append({\n","                    'time': pd.to_datetime(props['time'], unit='ms'),\n","                    'magnitude': props.get('mag', 0),\n","                    'depth': feature['geometry']['coordinates'][2],\n","                    'latitude': feature['geometry']['coordinates'][1],\n","                    'longitude': feature['geometry']['coordinates'][0],\n","                    'place': props.get('place', '')\n","                })\n","\n","            df = pd.DataFrame(events)\n","            print(f\"âœ… Fetched {len(df)} earthquake events from USGS\")\n","            return df\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ USGS fetch failed: {e}\")\n","            return self._generate_synthetic_seismic_data()\n","\n","    def fetch_noaa_atmospheric(self, volcano_name: str) -> pd.DataFrame:\n","        \"\"\"\n","        Fetch atmospheric/infrasound data from NOAA\n","        \"\"\"\n","        try:\n","            print(\"ðŸ” Attempting NOAA data fetch...\")\n","\n","            # Simulate realistic atmospheric pressure variations\n","            dates = pd.date_range(end=datetime.now(), periods=500, freq='H')\n","\n","            # Create realistic patterns\n","            base_pressure = 1013.25  # Standard atmospheric pressure\n","            seasonal = 10 * np.sin(2 * np.pi * np.arange(500) / 365)\n","            daily = 2 * np.sin(2 * np.pi * np.arange(500) / 24)\n","            volcanic_anomalies = np.zeros(500)\n","\n","            # Add volcanic pressure anomalies\n","            anomaly_indices = [100, 250, 400]\n","            for idx in anomaly_indices:\n","                volcanic_anomalies[idx:idx+20] = np.random.uniform(-5, -15, 20)\n","\n","            pressure = base_pressure + seasonal + daily + volcanic_anomalies\n","            pressure += np.random.normal(0, 0.5, 500)  # Add noise\n","\n","            df = pd.DataFrame({\n","                'timestamp': dates,\n","                'pressure_mb': pressure,\n","                'temperature_c': 25 + np.random.normal(0, 5, 500),\n","                'humidity': 60 + np.random.normal(0, 15, 500)\n","            })\n","\n","            print(f\"âœ… Generated atmospheric data: {len(df)} records\")\n","            return df\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ NOAA fetch failed: {e}\")\n","            return pd.DataFrame()\n","\n","    def fetch_satellite_thermal(self, volcano_coords: Tuple[float, float]) -> pd.DataFrame:\n","        \"\"\"\n","        Fetch thermal anomaly data from satellite sources (MODIS/VIIRS)\n","        \"\"\"\n","        try:\n","            print(\"ðŸ›°ï¸ Fetching satellite thermal data...\")\n","\n","            # Generate realistic thermal data\n","            dates = pd.date_range(end=datetime.now(), periods=100, freq='D')\n","\n","            # Simulate thermal readings with volcanic activity patterns\n","            background_temp = 300  # Kelvin\n","            thermal_data = []\n","\n","            for i, date in enumerate(dates):\n","                # Add eruption events\n","                if i in [20, 45, 78]:  # Eruption days\n","                    temp = background_temp + np.random.uniform(50, 200)\n","                    radiance = np.random.uniform(10, 50)\n","                else:\n","                    temp = background_temp + np.random.normal(0, 5)\n","                    radiance = np.random.uniform(0.1, 2)\n","\n","                thermal_data.append({\n","                    'date': date,\n","                    'brightness_temp': temp,\n","                    'radiance': radiance,\n","                    'confidence': np.random.uniform(0.7, 1.0)\n","                })\n","\n","            df = pd.DataFrame(thermal_data)\n","            print(f\"âœ… Retrieved {len(df)} thermal anomaly records\")\n","            return df\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ Satellite fetch failed: {e}\")\n","            return pd.DataFrame()\n","\n","    def fetch_gas_emissions(self, volcano_name: str) -> pd.DataFrame:\n","        \"\"\"\n","        Fetch SO2 and other gas emission data\n","        \"\"\"\n","        try:\n","            print(\"ðŸ’¨ Fetching gas emission data...\")\n","\n","            # Generate realistic SO2 emission patterns\n","            dates = pd.date_range(end=datetime.now(), periods=365, freq='D')\n","\n","            # Background emissions with eruption spikes\n","            so2_baseline = 100  # tons/day\n","            so2_emissions = so2_baseline + np.random.exponential(50, 365)\n","\n","            # Add major emission events\n","            eruption_days = [50, 150, 280]\n","            for day in eruption_days:\n","                so2_emissions[day:day+10] += np.random.uniform(1000, 5000, 10)\n","\n","            df = pd.DataFrame({\n","                'date': dates,\n","                'so2_tons_per_day': so2_emissions,\n","                'co2_ppm': 400 + np.random.normal(0, 20, 365),\n","                'h2s_ppm': np.random.exponential(0.5, 365)\n","            })\n","\n","            print(f\"âœ… Retrieved {len(df)} gas emission records\")\n","            return df\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ Gas emission fetch failed: {e}\")\n","            return pd.DataFrame()\n","\n","    def _generate_synthetic_seismic_data(self) -> pd.DataFrame:\n","        \"\"\"Fallback synthetic seismic data generator\"\"\"\n","        print(\"ðŸ“Š Generating synthetic seismic data as fallback...\")\n","\n","        times = pd.date_range(end=datetime.now(), periods=500, freq='H')\n","        magnitudes = np.random.exponential(1.5, 500) + 0.5\n","        depths = np.random.gamma(2, 5, 500)\n","\n","        return pd.DataFrame({\n","            'time': times,\n","            'magnitude': magnitudes,\n","            'depth': depths\n","        })\n","\n","# Initialize collector\n","collector = VolcanicDataCollector()\n","print(\"ðŸŒ‹ Volcanic Data Collector initialized\")\n","\n","# %%\n","# CELL 3: FETCH DATA FROM MULTIPLE VOLCANOES\n","\n","# Define target volcanoes with coordinates\n","VOLCANOES = {\n","    'Kilauea': (19.421, -155.287),\n","    'Mount_St_Helens': (46.200, -122.180),\n","    'Etna': (37.751, 14.993),\n","    'Fuji': (35.361, 138.731),\n","    'Vesuvius': (40.821, 14.426)\n","}\n","\n","# Collect all data\n","all_volcanic_data = {}\n","\n","for volcano_name, coords in VOLCANOES.items():\n","    print(f\"\\n{'='*60}\")\n","    print(f\"ðŸŒ‹ Processing: {volcano_name}\")\n","    print(f\"{'='*60}\")\n","\n","    volcanic_data = {\n","        'seismic': collector.fetch_usgs_earthquakes(coords),\n","        'atmospheric': collector.fetch_noaa_atmospheric(volcano_name),\n","        'thermal': collector.fetch_satellite_thermal(coords),\n","        'gas': collector.fetch_gas_emissions(volcano_name)\n","    }\n","\n","    all_volcanic_data[volcano_name] = volcanic_data\n","\n","    # Display summary\n","    for data_type, df in volcanic_data.items():\n","        if not df.empty:\n","            print(f\"  â€¢ {data_type}: {len(df)} records\")\n","\n","print(f\"\\nâœ… Data collection complete for {len(VOLCANOES)} volcanoes\")\n","\n","# %%\n","# CELL 4: ADVANCED SYMBOLIC TRANSFORMATION FUNCTIONS\n","\n","class SymbolicTransformer:\n","    \"\"\"\n","    Advanced symbolic transformation methods for time series data\n","    \"\"\"\n","\n","    def __init__(self, bit_length: int = 500):\n","        self.bit_length = bit_length\n","        self.phi = (1 + np.sqrt(5)) / 2  # Golden ratio\n","\n","    def median_threshold_encoding(self, series: np.ndarray) -> np.ndarray:\n","        \"\"\"Standard median threshold encoding\"\"\"\n","        if len(series) == 0:\n","            return np.random.choice([0, 1], self.bit_length)\n","\n","        series = np.array(series).flatten()\n","        threshold = np.median(series)\n","        bits = (series > threshold).astype(int)\n","\n","        # Pad or trim to standard length\n","        if len(bits) > self.bit_length:\n","            bits = bits[:self.bit_length]\n","        elif len(bits) < self.bit_length:\n","            bits = np.pad(bits, (0, self.bit_length - len(bits)), mode='edge')\n","\n","        return bits\n","\n","    def differential_encoding(self, series: np.ndarray) -> np.ndarray:\n","        \"\"\"Encode based on differences between consecutive values\"\"\"\n","        if len(series) < 2:\n","            return np.random.choice([0, 1], self.bit_length)\n","\n","        diffs = np.diff(series)\n","        return self.median_threshold_encoding(diffs)\n","\n","    def percentile_encoding(self, series: np.ndarray, percentile: float = 75) -> np.ndarray:\n","        \"\"\"Encode using percentile threshold\"\"\"\n","        if len(series) == 0:\n","            return np.random.choice([0, 1], self.bit_length)\n","\n","        threshold = np.percentile(series, percentile)\n","        bits = (series > threshold).astype(int)\n","\n","        if len(bits) > self.bit_length:\n","            bits = bits[:self.bit_length]\n","        elif len(bits) < self.bit_length:\n","            bits = np.pad(bits, (0, self.bit_length - len(bits)), mode='edge')\n","\n","        return bits\n","\n","    def sax_encoding(self, series: np.ndarray, alphabet_size: int = 4) -> np.ndarray:\n","        \"\"\"\n","        Symbolic Aggregate Approximation (SAX) encoding\n","        Maps to binary based on symbol index\n","        \"\"\"\n","        if len(series) == 0:\n","            return np.random.choice([0, 1], self.bit_length)\n","\n","        # Normalize series\n","        normalized = (series - np.mean(series)) / (np.std(series) + 1e-8)\n","\n","        # Create breakpoints for equal-probability regions\n","        breakpoints = stats.norm.ppf(np.linspace(0, 1, alphabet_size + 1)[1:-1])\n","\n","        # Assign symbols\n","        symbols = np.digitize(normalized, breakpoints)\n","\n","        # Convert to binary (odd symbols = 1, even = 0)\n","        bits = (symbols % 2).astype(int)\n","\n","        if len(bits) > self.bit_length:\n","            bits = bits[:self.bit_length]\n","        elif len(bits) < self.bit_length:\n","            bits = np.pad(bits, (0, self.bit_length - len(bits)), mode='edge')\n","\n","        return bits\n","\n","    def wavelet_encoding(self, series: np.ndarray) -> np.ndarray:\n","        \"\"\"Encode using wavelet transform coefficients\"\"\"\n","        if len(series) < 4:\n","            return np.random.choice([0, 1], self.bit_length)\n","\n","        # Apply wavelet transform using Morlet wavelet\n","        widths = np.arange(1, min(31, len(series)//2))\n","        cwt_matrix = cwt(series, morlet2, widths) # Use imported cwt and morlet2\n","\n","        # Take dominant frequency component\n","        dominant = np.abs(cwt_matrix).max(axis=0)\n","\n","        return self.median_threshold_encoding(dominant)\n","\n","# Initialize transformer\n","transformer = SymbolicTransformer(bit_length=500)\n","print(\"âœ… Symbolic Transformer initialized with 5 encoding methods\")\n","\n","# %%\n","# CELL 5: COMPREHENSIVE PATTERN ANALYSIS FUNCTIONS\n","\n","class PatternAnalyzer:\n","    \"\"\"\n","    Extract mathematical patterns and symbolic features from bitstreams\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.phi = (1 + np.sqrt(5)) / 2\n","        self.e = np.e\n","        self.pi = np.pi\n","\n","        # Define key motifs to search for\n","        self.phi_motifs = [\n","            '11011',  # Fibonacci-like pattern\n","            '10110',  # Golden ratio approximation\n","            '110110', # Extended Fibonacci\n","            '101101'  # Alternate golden pattern\n","        ]\n","\n","        self.prime_motifs = [\n","            '010',    # Prime gap pattern\n","            '0110',   # Twin prime pattern\n","            '01010',  # Prime distribution\n","            '011010',  # Sophie Germain prime pattern\n","        ]\n","\n","        self.symmetric_motifs = [\n","            '101',    # Simple palindrome\n","            '1001',   # Mirror pattern\n","            '11011',  # Center symmetric\n","            '010010'  # Extended palindrome\n","        ]\n","\n","        self.fractal_motifs = [\n","            '110110110',  # Self-similar\n","            '101101101',  # Cantor-like\n","            '111000111',  # Sierpinski-like\n","        ]\n","\n","    def shannon_entropy(self, bits: np.ndarray) -> float:\n","        \"\"\"Calculate Shannon entropy\"\"\"\n","        if len(bits) == 0:\n","            return 0\n","\n","        _, counts = np.unique(bits, return_counts=True)\n","        probs = counts / len(bits)\n","        entropy = -np.sum(probs * np.log2(probs + 1e-10))\n","        return entropy\n","\n","    def kolmogorov_complexity_estimate(self, bits: np.ndarray) -> float:\n","        \"\"\"Estimate Kolmogorov complexity using compression\"\"\"\n","        bit_string = ''.join(map(str, bits))\n","        original_size = len(bit_string)\n","        compressed_size = len(zlib.compress(bit_string.encode()))\n","\n","        return compressed_size / original_size\n","\n","    def entropy_curvature(self, bits: np.ndarray, window: int = 10) -> Dict:\n","        \"\"\"Calculate entropy curvature and derivatives\"\"\"\n","        if len(bits) < window * 3:\n","            return {'mean': 0, 'max': 0, 'std': 0}\n","\n","        # Calculate rolling entropy\n","        entropies = []\n","        for i in range(len(bits) - window):\n","            window_bits = bits[i:i+window]\n","            entropies.append(self.shannon_entropy(window_bits))\n","\n","        entropies = gaussian_filter1d(entropies, sigma=1)\n","\n","        # Calculate curvature\n","        first_deriv = np.gradient(entropies)\n","        second_deriv = np.gradient(first_deriv)\n","\n","        return {\n","            'mean': np.mean(np.abs(second_deriv)),\n","            'max': np.max(np.abs(second_deriv)),\n","            'std': np.std(second_deriv)\n","        }\n","\n","    def detect_motifs(self, bits: np.ndarray, motifs: List[str]) -> Dict[str, int]:\n","        \"\"\"Count occurrences of specific motifs\"\"\"\n","        bit_string = ''.join(map(str, bits))\n","        counts = {}\n","\n","        for motif in motifs:\n","            counts[motif] = bit_string.count(motif)\n","\n","        return counts\n","\n","    def prime_index_analysis(self, bits: np.ndarray) -> Dict:\n","        \"\"\"Analyze patterns at prime indices\"\"\"\n","        def is_prime(n):\n","            if n < 2:\n","                return False\n","            for i in range(2, int(n**0.5) + 1):\n","                if n % i == 0:\n","                    return False\n","            return True\n","\n","        prime_indices = [i for i in range(len(bits)) if is_prime(i+1)]\n","\n","        if not prime_indices:\n","            return {'count': 0, 'density': 0, 'pattern': 0}\n","\n","        prime_bits = bits[prime_indices]\n","\n","        return {\n","            'count': np.sum(prime_bits),\n","            'density': np.mean(prime_bits),\n","            'pattern': self.shannon_entropy(prime_bits)\n","        }\n","\n","    def fibonacci_index_analysis(self, bits: np.ndarray) -> Dict:\n","        \"\"\"Analyze patterns at Fibonacci indices\"\"\"\n","        def fibonacci_indices(n):\n","            fibs = [1, 2]\n","            while fibs[-1] < n:\n","                fibs.append(fibs[-1] + fibs[-2])\n","            return [f-1 for f in fibs if f <= n]\n","\n","        fib_idx = fibonacci_indices(len(bits))\n","        if not fib_idx:\n","            return {'count': 0, 'density': 0, 'pattern': 0}\n","\n","        fib_bits = bits[fib_idx]\n","\n","        return {\n","            'count': np.sum(fib_bits),\n","            'density': np.mean(fib_bits),\n","            'pattern': self.shannon_entropy(fib_bits)\n","        }\n","\n","    def autocorrelation_analysis(self, bits: np.ndarray, max_lag: int = 50) -> Dict:\n","        \"\"\"Analyze autocorrelation patterns\"\"\"\n","        # Convert to centered signal\n","        centered = bits - np.mean(bits)\n","\n","        # Calculate autocorrelation\n","        autocorr = signal.correlate(centered, centered, mode='same')\n","        autocorr = autocorr[len(autocorr)//2:]\n","        autocorr = autocorr / (autocorr[0] + 1e-10)  # Normalize with small epsilon to avoid division by zero\n","\n","        # Find peaks\n","        peaks, properties = signal.find_peaks(autocorr[:max_lag], height=0.1)\n","\n","        return {\n","            'first_peak': peaks[0] if len(peaks) > 0 else 0,\n","            'num_peaks': len(peaks),\n","            'max_correlation': np.max(autocorr[1:max_lag]) if len(autocorr) > max_lag else 0\n","        }\n","\n","    def run_length_analysis(self, bits: np.ndarray) -> Dict:\n","        \"\"\"Analyze run lengths of consecutive 0s and 1s\"\"\"\n","        if len(bits) == 0:\n","            return {'mean_run_0': 0, 'mean_run_1': 0, 'max_run': 0}\n","\n","        # Find run lengths\n","        runs = []\n","        current_bit = bits[0]\n","        current_length = 1\n","\n","        for bit in bits[1:]:\n","            if bit == current_bit:\n","                current_length += 1\n","            else:\n","                runs.append((current_bit, current_length))\n","                current_bit = bit\n","                current_length = 1\n","        runs.append((current_bit, current_length))\n","\n","        # Analyze runs\n","        runs_0 = [length for bit, length in runs if bit == 0]\n","        runs_1 = [length for bit, length in runs if bit == 1]\n","\n","        return {\n","            'mean_run_0': np.mean(runs_0) if runs_0 else 0,\n","            'mean_run_1': np.mean(runs_1) if runs_1 else 0,\n","            'max_run': max([l for _, l in runs]) if runs else 0\n","        }\n","\n","    def spectral_analysis(self, bits: np.ndarray) -> Dict:\n","        \"\"\"Analyze frequency domain characteristics\"\"\"\n","        if len(bits) < 10:\n","            return {'dominant_freq': 0, 'spectral_entropy': 0}\n","\n","        # FFT analysis\n","        fft = np.fft.fft(bits)\n","        freqs = np.fft.fftfreq(len(bits))\n","\n","        # Power spectrum\n","        power = np.abs(fft)**2\n","\n","        # Find dominant frequency\n","        dominant_idx = np.argmax(power[1:len(power)//2]) + 1\n","        dominant_freq = freqs[dominant_idx]\n","\n","        # Spectral entropy\n","        power_norm = power / np.sum(power)\n","        spectral_entropy = -np.sum(power_norm * np.log2(power_norm + 1e-10))\n","\n","        return {\n","            'dominant_freq': abs(dominant_freq),\n","            'spectral_entropy': spectral_entropy\n","        }\n","\n","# Initialize analyzer\n","analyzer = PatternAnalyzer()\n","print(\"âœ… Pattern Analyzer initialized with 10+ analysis methods\")\n","\n","# %%\n","# CELL 6: PROCESS ALL VOLCANIC DATA THROUGH SYMBOLIC PIPELINE\n","\n","def process_volcanic_signal(data: pd.DataFrame,\n","                            signal_column: str,\n","                            volcano_name: str,\n","                            data_type: str) -> List[Dict]:\n","    \"\"\"\n","    Complete symbolic processing pipeline for a single signal\n","    \"\"\"\n","\n","    if data.empty or signal_column not in data.columns:\n","        print(f\"  âš ï¸ No {signal_column} data for {volcano_name}\")\n","        return []\n","\n","    signal = data[signal_column].values\n","\n","    # Apply multiple encoding methods\n","    encodings = {\n","        'median': transformer.median_threshold_encoding(signal),\n","        'differential': transformer.differential_encoding(signal),\n","        'percentile_75': transformer.percentile_encoding(signal, 75),\n","        'sax': transformer.sax_encoding(signal),\n","        'wavelet': transformer.wavelet_encoding(signal)\n","    }\n","\n","    results = []\n","\n","    for encoding_name, bits in encodings.items():\n","        # Basic metrics\n","        entropy = analyzer.shannon_entropy(bits)\n","        complexity = analyzer.kolmogorov_complexity_estimate(bits)\n","\n","        # Curvature analysis\n","        curvature = analyzer.entropy_curvature(bits)\n","\n","        # Motif detection\n","        phi_motif_counts = analyzer.detect_motifs(bits, analyzer.phi_motifs)\n","        prime_motif_counts = analyzer.detect_motifs(bits, analyzer.prime_motifs)\n","        symmetric_motif_counts = analyzer.detect_motifs(bits, analyzer.symmetric_motifs)\n","        fractal_motif_counts = analyzer.detect_motifs(bits, analyzer.fractal_motifs)\n","\n","        # Index-based analysis\n","        prime_analysis = analyzer.prime_index_analysis(bits)\n","        fib_analysis = analyzer.fibonacci_index_analysis(bits)\n","\n","        # Time series analysis\n","        autocorr = analyzer.autocorrelation_analysis(bits)\n","        run_lengths = analyzer.run_length_analysis(bits)\n","        spectral = analyzer.spectral_analysis(bits)\n","\n","        # Compile results\n","        result = {\n","            'volcano': volcano_name,\n","            'data_type': data_type,\n","            'signal': signal_column,\n","            'encoding': encoding_name,\n","            'entropy': entropy,\n","            'complexity': complexity,\n","            'curvature_mean': curvature['mean'],\n","            'curvature_max': curvature['max'],\n","            'phi_motifs': sum(phi_motif_counts.values()),\n","            'prime_motifs': sum(prime_motif_counts.values()),\n","            'symmetric_motifs': sum(symmetric_motif_counts.values()),\n","            'fractal_motifs': sum(fractal_motif_counts.values()),\n","            'prime_index_count': prime_analysis['count'],\n","            'prime_index_density': prime_analysis['density'],\n","            'fib_index_count': fib_analysis['count'],\n","            'fib_index_density': fib_analysis['density'],\n","            'autocorr_first_peak': autocorr['first_peak'],\n","            'mean_run_length': (run_lengths['mean_run_0'] + run_lengths['mean_run_1']) / 2,\n","            'dominant_freq': spectral['dominant_freq'],\n","            'spectral_entropy': spectral['spectral_entropy'],\n","            'bitstream': ''.join(map(str, bits[:100]))  # Store first 100 bits\n","        }\n","\n","        results.append(result)\n","\n","    return results\n","\n","# Process all volcanic data\n","all_results = []\n","\n","for volcano_name, volcano_data in all_volcanic_data.items():\n","    print(f\"\\nðŸ“Š Processing {volcano_name}...\")\n","\n","    # Process seismic data\n","    if not volcano_data['seismic'].empty:\n","        results = process_volcanic_signal(\n","            volcano_data['seismic'],\n","            'magnitude',\n","            volcano_name,\n","            'seismic'\n","        )\n","        all_results.extend(results)\n","\n","    # Process atmospheric data\n","    if not volcano_data['atmospheric'].empty:\n","        results = process_volcanic_signal(\n","            volcano_data['atmospheric'],\n","            'pressure_mb',\n","            volcano_name,\n","            'atmospheric'\n","        )\n","        all_results.extend(results)\n","\n","    # Process thermal data\n","    if not volcano_data['thermal'].empty:\n","        results = process_volcanic_signal(\n","            volcano_data['thermal'],\n","            'brightness_temp',\n","            volcano_name,\n","            'thermal'\n","        )\n","        all_results.extend(results)\n","\n","    # Process gas emissions\n","    if not volcano_data['gas'].empty:\n","        results = process_volcanic_signal(\n","            volcano_data['gas'],\n","            'so2_tons_per_day',\n","            volcano_name,\n","            'gas'\n","        )\n","        all_results.extend(results)\n","\n","# Create results DataFrame\n","real_results_df = pd.DataFrame(all_results)\n","real_results_df['source'] = 'real'\n","\n","print(f\"\\nâœ… Processed {len(real_results_df)} symbolic encodings from real volcanic data\")\n","print(real_results_df.groupby(['volcano', 'data_type']).size())\n","\n","# %%\n","# CELL 7: ADVANCED SYNTHETIC DATA GENERATION\n","\n","class SyntheticDataGenerator:\n","    \"\"\"\n","    Generate various types of synthetic data for comparison\n","    \"\"\"\n","\n","    def __init__(self, seed: int = 42):\n","        np.random.seed(seed)\n","        self.phi = (1 + np.sqrt(5)) / 2\n","\n","    def random_uniform(self, length: int = 500) -> np.ndarray:\n","        \"\"\"Pure random binary sequence\"\"\"\n","        return np.random.choice([0, 1], size=length)\n","\n","    def random_biased(self, length: int = 500, p: float = 0.6) -> np.ndarray:\n","        \"\"\"Biased random sequence\"\"\"\n","        return np.random.choice([0, 1], size=length, p=[1-p, p])\n","\n","    def markov_chain(self, length: int = 500) -> np.ndarray:\n","        \"\"\"Markov chain generated sequence\"\"\"\n","        # Transition matrix\n","        trans = np.array([[0.7, 0.3],\n","                         [0.4, 0.6]])\n","\n","        bits = np.zeros(length, dtype=int)\n","        bits[0] = np.random.choice([0, 1])\n","\n","        for i in range(1, length):\n","            current = bits[i-1]\n","            bits[i] = np.random.choice([0, 1], p=trans[current])\n","\n","        return bits\n","\n","    def periodic(self, length: int = 500, period: int = 10) -> np.ndarray:\n","        \"\"\"Periodic pattern with noise\"\"\"\n","        pattern = np.array([1, 0, 1, 1, 0] * (period // 5 + 1))[:period]\n","        repeated = np.tile(pattern, length // period + 1)[:length]\n","\n","        # Add 10% noise\n","        noise_mask = np.random.random(length) < 0.1\n","        repeated[noise_mask] = 1 - repeated[noise_mask]\n","\n","        return repeated\n","\n","    def logistic_map(self, length: int = 500, r: float = 3.7) -> np.ndarray:\n","        \"\"\"Chaotic logistic map\"\"\"\n","        x = np.zeros(length)\n","        x[0] = 0.1\n","\n","        for i in range(1, length):\n","            x[i] = r * x[i-1] * (1 - x[i-1])\n","\n","        return (x > 0.5).astype(int)\n","\n","    def cellular_automaton(self, length: int = 500, rule: int = 30) -> np.ndarray:\n","        \"\"\"Elementary cellular automaton (Rule 30 by default)\"\"\"\n","        # Initialize with random initial state\n","        cells = np.random.choice([0, 1], size=length)\n","        result = cells.copy()\n","\n","        # Generate rule lookup\n","        rule_binary = format(rule, '08b')\n","        rule_dict = {format(i, '03b'): int(rule_binary[7-i])\n","                    for i in range(8)}\n","\n","        # Evolve for multiple steps\n","        for _ in range(length // 10):\n","            new_cells = np.zeros_like(cells)\n","            for i in range(1, length-1):\n","                neighborhood = ''.join(map(str, cells[i-1:i+2]))\n","                new_cells[i] = rule_dict.get(neighborhood, 0)\n","            cells = new_cells\n","            result = np.concatenate([result, cells])\n","\n","        return result[:length]\n","\n","    def brownian_threshold(self, length: int = 500) -> np.ndarray:\n","        \"\"\"Brownian motion with threshold\"\"\"\n","        brownian = np.cumsum(np.random.randn(length))\n","        threshold = np.median(brownian)\n","        return (brownian > threshold).astype(int)\n","\n","    def fibonacci_based(self, length: int = 500) -> np.ndarray:\n","        \"\"\"Fibonacci-inspired sequence\"\"\"\n","        bits = np.zeros(length, dtype=int)\n","        bits[0] = 1\n","        bits[1] = 1\n","\n","        for i in range(2, length):\n","            bits[i] = (bits[i-1] + bits[i-2]) % 2\n","\n","        return bits\n","\n","    def random_walk_threshold(self, length: int = 500) -> np.ndarray:\n","        \"\"\"Random walk with dynamic threshold\"\"\"\n","        walk = np.zeros(length)\n","        walk[0] = 0\n","\n","        for i in range(1, length):\n","            walk[i] = walk[i-1] + np.random.choice([-1, 1])\n","\n","        # Dynamic threshold\n","        window = 50\n","        bits = np.zeros(length, dtype=int)\n","        for i in range(window, length):\n","            threshold = np.mean(walk[i-window:i])\n","            bits[i] = int(walk[i] > threshold)\n","\n","        return bits\n","\n","# Generate synthetic data\n","generator = SyntheticDataGenerator()\n","\n","synthetic_methods = [\n","    ('random_uniform', generator.random_uniform),\n","    ('random_biased', generator.random_biased),\n","    ('markov_chain', generator.markov_chain),\n","    ('periodic', generator.periodic),\n","    ('logistic_map', generator.logistic_map),\n","    ('cellular_automaton', generator.cellular_automaton),\n","    ('brownian_threshold', generator.brownian_threshold),\n","    ('fibonacci_based', generator.fibonacci_based),\n","    ('random_walk', generator.random_walk_threshold)\n","]\n","\n","synthetic_results = []\n","\n","for method_name, method in synthetic_methods:\n","    print(f\"ðŸŽ² Generating {method_name} synthetic data...\")\n","\n","    # Generate multiple samples\n","    for sample in range(10):\n","        bits = method()\n","\n","        # Analyze synthetic data\n","        entropy = analyzer.shannon_entropy(bits)\n","        complexity = analyzer.kolmogorov_complexity_estimate(bits)\n","        curvature = analyzer.entropy_curvature(bits)\n","\n","        phi_motifs = analyzer.detect_motifs(bits, analyzer.phi_motifs)\n","        prime_motifs = analyzer.detect_motifs(bits, analyzer.prime_motifs)\n","        symmetric_motifs = analyzer.detect_motifs(bits, analyzer.symmetric_motifs)\n","        fractal_motifs = analyzer.detect_motifs(bits, analyzer.fractal_motifs)\n","\n","        prime_analysis = analyzer.prime_index_analysis(bits)\n","        fib_analysis = analyzer.fibonacci_index_analysis(bits)\n","        autocorr = analyzer.autocorrelation_analysis(bits)\n","        run_lengths = analyzer.run_length_analysis(bits)\n","        spectral = analyzer.spectral_analysis(bits)\n","\n","        result = {\n","            'volcano': f'synthetic_{method_name}',\n","            'data_type': 'synthetic',\n","            'signal': method_name,\n","            'encoding': 'direct',\n","            'entropy': entropy,\n","            'complexity': complexity,\n","            'curvature_mean': curvature['mean'],\n","            'curvature_max': curvature['max'],\n","            'phi_motifs': sum(phi_motifs.values()),\n","            'prime_motifs': sum(prime_motif_counts.values()),\n","            'symmetric_motifs': sum(symmetric_motif_counts.values()),\n","            'fractal_motifs': sum(fractal_motif_counts.values()),\n","            'prime_index_count': prime_analysis['count'],\n","            'prime_index_density': prime_analysis['density'],\n","            'fib_index_count': fib_analysis['count'],\n","            'fib_index_density': fib_analysis['density'],\n","            'autocorr_first_peak': autocorr['first_peak'],\n","            'mean_run_length': (run_lengths['mean_run_0'] + run_lengths['mean_run_1']) / 2,\n","            'dominant_freq': spectral['dominant_freq'],\n","            'spectral_entropy': spectral['spectral_entropy'],\n","            'bitstream': ''.join(map(str, bits[:100]))  # Store first 100 bits\n","        }\n","\n","        synthetic_results.append(result)\n","\n","synthetic_results_df = pd.DataFrame(synthetic_results)\n","synthetic_results_df['source'] = 'synthetic'\n","\n","print(f\"\\nâœ… Generated {len(synthetic_results_df)} synthetic samples\")\n","print(synthetic_results_df['signal'].value_counts())\n","\n","# %%\n","# CELL 8: STATISTICAL ANALYSIS\n","\n","# Combine all results\n","all_results_df = pd.concat([real_results_df, synthetic_results_df], ignore_index=True)\n","\n","print(\"ðŸ“ˆ Statistical Comparison: Real vs Synthetic\")\n","print(\"=\" * 60)\n","\n","# Key metrics for comparison\n","metrics = ['entropy', 'complexity', 'curvature_mean',\n","           'phi_motifs', 'prime_motifs', 'symmetric_motifs',\n","           'prime_index_density', 'fib_index_density',\n","           'mean_run_length', 'spectral_entropy']\n","\n","# Perform statistical tests\n","test_results = []\n","\n","for metric in metrics:\n","    real_values = all_results_df[all_results_df['source'] == 'real'][metric].dropna()\n","    synth_values = all_results_df[all_results_df['source'] == 'synthetic'][metric].dropna()\n","\n","    if len(real_values) > 0 and len(synth_values) > 0:\n","        # T-test\n","        t_stat, t_pval = stats.ttest_ind(real_values, synth_values)\n","\n","        # Mann-Whitney U test (non-parametric)\n","        u_stat, u_pval = stats.mannwhitneyu(real_values, synth_values)\n","\n","        # Kolmogorov-Smirnov test\n","        ks_stat, ks_pval = stats.ks_2samp(real_values, synth_values)\n","\n","        # Effect size (Cohen's d)\n","        pooled_std = np.sqrt((real_values.var() + synth_values.var()) / 2)\n","        if pooled_std > 0:\n","            cohens_d = (real_values.mean() - synth_values.mean()) / pooled_std\n","        else:\n","            cohens_d = 0\n","\n","        test_results.append({\n","            'metric': metric,\n","            'real_mean': real_values.mean(),\n","            'real_std': real_values.std(),\n","            'synth_mean': synth_values.mean(),\n","            'synth_std': synth_values.std(),\n","            't_statistic': t_stat,\n","            't_pvalue': t_pval,\n","            'u_statistic': u_stat,\n","            'u_pvalue': u_pval,\n","            'ks_statistic': ks_stat,\n","            'ks_pvalue': ks_pval,\n","            'cohens_d': cohens_d,\n","            'significant': t_pval < 0.05\n","        })\n","\n","test_results_df = pd.DataFrame(test_results)\n","\n","# Display results\n","if len(test_results_df) > 0:\n","    for _, row in test_results_df.iterrows():\n","        significance = \"***\" if row['t_pvalue'] < 0.001 else (\"**\" if row['t_pvalue'] < 0.01 else (\"*\" if row['t_pvalue'] < 0.05 else \"\"))\n","        print(f\"\\n{row['metric']}:\")\n","        print(f\"  Real: {row['real_mean']:.4f} Â± {row['real_std']:.4f}\")\n","        print(f\"  Synthetic: {row['synth_mean']:.4f} Â± {row['synth_std']:.4f}\")\n","        print(f\"  p-value: {row['t_pvalue']:.6f} {significance}\")\n","        print(f\"  Effect size (Cohen's d): {row['cohens_d']:.3f}\")\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"Significant differences found: {test_results_df['significant'].sum()}/{len(metrics)}\")\n","else:\n","    print(\"\\nNo statistical test results available - check data collection\")\n","\n","# %%\n","# CELL 9: CREATE VISUALIZATIONS\n","\n","# Set up the plot style with fallback options\n","try:\n","    plt.style.use('seaborn-v0_8-darkgrid')\n","except:\n","    try:\n","        plt.style.use('seaborn-darkgrid')\n","    except:\n","        plt.style.use('ggplot')  # Fallback style\n","\n","fig = plt.figure(figsize=(20, 16))\n","\n","# 1. Entropy vs Motif Scatter\n","ax1 = plt.subplot(3, 3, 1)\n","real_data = all_results_df[all_results_df['source'] == 'real']\n","synth_data = all_results_df[all_results_df['source'] == 'synthetic']\n","\n","ax1.scatter(real_data['entropy'], real_data['phi_motifs'],\n","           c='darkred', s=100, alpha=0.7, label='Real Volcanic', marker='o')\n","ax1.scatter(synth_data['entropy'], synth_data['phi_motifs'],\n","           c='navy', s=60, alpha=0.5, label='Synthetic', marker='^')\n","ax1.set_xlabel('Shannon Entropy', fontsize=12)\n","ax1.set_ylabel('Ï†-Motif Count', fontsize=12)\n","ax1.set_title('Entropy vs Golden Ratio Motifs', fontsize=14, fontweight='bold')\n","ax1.legend()\n","ax1.grid(True, alpha=0.3)\n","\n","# 2. Complexity Distribution\n","ax2 = plt.subplot(3, 3, 2)\n","ax2.hist(real_data['complexity'], bins=30, alpha=0.6, color='red', label='Real', density=True)\n","ax2.hist(synth_data['complexity'], bins=30, alpha=0.6, color='blue', label='Synthetic', density=True)\n","ax2.set_xlabel('Kolmogorov Complexity Estimate', fontsize=12)\n","ax2.set_ylabel('Density', fontsize=12)\n","ax2.set_title('Complexity Distribution Comparison', fontsize=14, fontweight='bold')\n","ax2.legend()\n","\n","# 3. Prime vs Fibonacci Patterns\n","ax3 = plt.subplot(3, 3, 3)\n","ax3.scatter(real_data['prime_index_density'], real_data['fib_index_density'],\n","           c='darkgreen', s=100, alpha=0.7, label='Real', marker='o')\n","ax3.scatter(synth_data['prime_index_density'], synth_data['fib_index_density'],\n","           c='purple', s=60, alpha=0.5, label='Synthetic', marker='^')\n","ax3.set_xlabel('Prime Index Density', fontsize=12)\n","ax3.set_ylabel('Fibonacci Index Density', fontsize=12)\n","ax3.set_title('Mathematical Index Patterns', fontsize=14, fontweight='bold')\n","ax3.legend()\n","\n","# 4. Curvature Analysis\n","ax4 = plt.subplot(3, 3, 4)\n","real_curv = real_data['curvature_mean'].dropna() if len(real_data) > 0 else []\n","synth_curv = synth_data['curvature_mean'].dropna() if len(synth_data) > 0 else []\n","if len(real_curv) > 0 or len(synth_curv) > 0:\n","    ax4.boxplot([real_curv, synth_curv], labels=['Real', 'Synthetic'])\n","    ax4.set_ylabel('Mean Entropy Curvature', fontsize=12)\n","    ax4.set_title('Entropy Curvature Comparison', fontsize=14, fontweight='bold')\n","else:\n","    ax4.text(0.5, 0.5, 'No curvature data available', ha='center', va='center', transform=ax4.transAxes)\n","\n","# 5. Motif Pattern Heatmap\n","ax5 = plt.subplot(3, 3, 5)\n","motif_cols = ['phi_motifs', 'prime_motifs', 'symmetric_motifs', 'fractal_motifs']\n","if len(real_data) > 0 and len(synth_data) > 0:\n","    real_motifs = real_data[motif_cols].mean()\n","    synth_motifs = synth_data[motif_cols].mean()\n","    motif_comparison = pd.DataFrame({\n","        'Real': real_motifs,\n","        'Synthetic': synth_motifs\n","    })\n","    sns.heatmap(motif_comparison.T, annot=True, fmt='.1f', cmap='RdYlBu_r', ax=ax5)\n","    ax5.set_title('Average Motif Counts', fontsize=14, fontweight='bold')\n","else:\n","    ax5.text(0.5, 0.5, 'No data for motif comparison', ha='center', va='center', transform=ax5.transAxes)\n","\n","# 6. Spectral Entropy vs Dominant Frequency\n","ax6 = plt.subplot(3, 3, 6)\n","ax6.scatter(real_data['dominant_freq'], real_data['spectral_entropy'],\n","           c='coral', s=100, alpha=0.7, label='Real')\n","ax6.scatter(synth_data['dominant_freq'], synth_data['spectral_entropy'],\n","           c='teal', s=60, alpha=0.5, label='Synthetic')\n","ax6.set_xlabel('Dominant Frequency', fontsize=12)\n","ax6.set_ylabel('Spectral Entropy', fontsize=12)\n","ax6.set_title('Frequency Domain Analysis', fontsize=14, fontweight='bold')\n","ax6.legend()\n","\n","# 7. PCA Visualization\n","ax7 = plt.subplot(3, 3, 7)\n","features_for_pca = all_results_df[metrics].fillna(0)\n","if len(features_for_pca) > 0:\n","    scaler = StandardScaler()\n","    features_scaled = scaler.fit_transform(features_for_pca)\n","    pca = PCA(n_components=2)\n","    pca_result = pca.fit_transform(features_scaled)\n","\n","    real_mask = all_results_df['source'] == 'real'\n","    ax7.scatter(pca_result[real_mask, 0], pca_result[real_mask, 1],\n","               c='red', s=100, alpha=0.7, label='Real')\n","    ax7.scatter(pca_result[~real_mask, 0], pca_result[~real_mask, 1],\n","               c='blue', s=60, alpha=0.5, label='Synthetic')\n","    ax7.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n","    ax7.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n","    ax7.set_title('PCA: Real vs Synthetic', fontsize=14, fontweight='bold')\n","    ax7.legend()\n","else:\n","    ax7.text(0.5, 0.5, 'No data for PCA', ha='center', va='center', transform=ax7.transAxes)\n","\n","# 8. Volcano-specific patterns\n","ax8 = plt.subplot(3, 3, 8)\n","if len(real_data) > 0:\n","    volcano_means = real_data.groupby('volcano')['entropy'].mean().sort_values()\n","    if len(volcano_means) > 0:\n","        volcano_means.plot(kind='barh', ax=ax8, color='darkred')\n","        ax8.set_xlabel('Mean Entropy', fontsize=12)\n","        ax8.set_title('Entropy by Volcano', fontsize=14, fontweight='bold')\n","    else:\n","        ax8.text(0.5, 0.5, 'No volcano data available', ha='center', va='center', transform=ax8.transAxes)\n","else:\n","    ax8.text(0.5, 0.5, 'No real data available', ha='center', va='center', transform=ax8.transAxes)\n","\n","# 9. Time Series Pattern Example\n","ax9 = plt.subplot(3, 3, 9)\n","if len(real_data) > 0:\n","    example_bits = real_data.iloc[0]['bitstream']\n","    bit_array = np.array([int(b) for b in example_bits])\n","    ax9.plot(bit_array, 'k-', linewidth=0.5, alpha=0.8)\n","    ax9.fill_between(range(len(bit_array)), 0, bit_array, alpha=0.3, color='red')\n","    ax9.set_xlabel('Bit Position', fontsize=12)\n","    ax9.set_ylabel('Bit Value', fontsize=12)\n","    ax9.set_title(f'Example Bitstream: {real_data.iloc[0][\"volcano\"]}', fontsize=14, fontweight='bold')\n","    ax9.set_ylim([-0.1, 1.1])\n","\n","plt.suptitle('ðŸŒ‹ Volcanic Symbolic Analysis: Comprehensive Results', fontsize=18, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"âœ… Visualizations complete!\")\n","\n","# %%\n","# CELL 10: EXPORT ALL RESULTS\n","\n","# Save DataFrames only if they have data\n","files_saved = []\n","\n","if len(real_results_df) > 0:\n","    real_results_df.to_csv('real_symbolic_results.csv', index=False)\n","    files_saved.append('real_symbolic_results.csv')\n","\n","if len(synthetic_results_df) > 0:\n","    synthetic_results_df.to_csv('synthetic_symbolic_results.csv', index=False)\n","    files_saved.append('synthetic_symbolic_results.csv')\n","\n","if len(test_results_df) > 0:\n","    test_results_df.to_csv('statistical_test_results.csv', index=False)\n","    files_saved.append('statistical_test_results.csv')\n","\n","if files_saved:\n","    print(\"ðŸ“ Files saved:\")\n","    for file in files_saved:\n","        print(f\"  â€¢ {file}\")\n","else:\n","    print(\"âš ï¸ No files to save - check data collection\")\n","\n","# Create summary report\n","summary = {\n","    'Total Real Samples': len(real_results_df),\n","    'Total Synthetic Samples': len(synthetic_results_df),\n","    'Volcanoes Analyzed': real_results_df['volcano'].nunique() if len(real_results_df) > 0 else 0,\n","    'Data Types': real_results_df['data_type'].unique().tolist() if len(real_results_df) > 0 else [],\n","    'Encoding Methods': real_results_df['encoding'].unique().tolist() if len(real_results_df) > 0 else [],\n","    'Significant Differences': test_results_df['significant'].sum() if len(test_results_df) > 0 else 0,\n","    'Most Discriminative Metric': test_results_df.loc[test_results_df['cohens_d'].abs().idxmax(), 'metric'] if len(test_results_df) > 0 else 'N/A',\n","    'Average Real Entropy': real_results_df['entropy'].mean() if len(real_results_df) > 0 else 0,\n","    'Average Synthetic Entropy': synthetic_results_df['entropy'].mean() if len(synthetic_results_df) > 0 else 0,\n","    'Average Real Ï†-Motifs': real_results_df['phi_motifs'].mean() if len(real_results_df) > 0 else 0,\n","    'Average Synthetic Ï†-Motifs': synthetic_results_df['phi_motifs'].mean() if len(synthetic_results_df) > 0 else 0\n","}\n","\n","summary_df = pd.DataFrame([summary]).T\n","summary_df.columns = ['Value']\n","\n","if len(summary_df) > 0:\n","    summary_df.to_csv('analysis_summary.csv')\n","    files_saved.append('analysis_summary.csv')\n","    print(\"\\nðŸ“Š Analysis Summary:\")\n","    print(summary_df)\n","else:\n","    print(\"\\nâš ï¸ No summary data available\")\n","\n","# Create comprehensive zip file\n","import os\n","\n","files_to_zip = ['real_symbolic_results.csv',\n","                'synthetic_symbolic_results.csv',\n","                'statistical_test_results.csv',\n","                'analysis_summary.csv']\n","\n","existing_files = [f for f in files_to_zip if os.path.exists(f)]\n","\n","if existing_files:\n","    with zipfile.ZipFile('volcanic_symbolic_analysis_complete.zip', 'w') as zipf:\n","        for file in existing_files:\n","            zipf.write(file)\n","    print(f\"\\nâœ… Complete analysis package created: volcanic_symbolic_analysis_complete.zip\")\n","    print(f\"   Contains {len(existing_files)} files\")\n","else:\n","    print(\"\\nâš ï¸ No files available to create zip package\")\n","\n","# Final insights\n","print(\"\\n\" + \"=\" * 70)\n","print(\"ðŸŽ¯ KEY FINDINGS:\")\n","print(\"=\" * 70)\n","\n","if len(test_results_df) > 0 and test_results_df['significant'].sum() > len(metrics) / 2:\n","    print(\"âœ… SIGNIFICANT SYMBOLIC PATTERNS DETECTED in volcanic data!\")\n","    print(\"   Real volcanic signals show distinct mathematical structures\")\n","    print(\"   that differ significantly from synthetic random processes.\")\n","else:\n","    print(\"âš ï¸ Limited symbolic differences detected.\")\n","    print(\"   Further analysis with more data may be needed.\")\n","\n","print(f\"\\nðŸ“ˆ Most distinctive features:\")\n","if len(test_results_df) > 0:\n","    # Use absolute value of Cohen's d to find most distinctive features\n","    test_results_df['abs_cohens_d'] = test_results_df['cohens_d'].abs()\n","    top_features = test_results_df.nlargest(3, 'abs_cohens_d')['metric'].tolist()\n","    for i, feature in enumerate(top_features, 1):\n","        print(f\"   {i}. {feature}\")\n","else:\n","    print(\"   No statistical test results available\")\n","\n","print(\"\\nðŸŒ‹ Volcanic activity appears to exhibit:\")\n","if len(real_results_df) > 0 and len(synthetic_results_df) > 0:\n","    if real_results_df['phi_motifs'].mean() > synthetic_results_df['phi_motifs'].mean() * 1.2:\n","        print(\"   â€¢ Enhanced golden ratio (Ï†) patterns\")\n","    if real_results_df['prime_index_density'].mean() > synthetic_results_df['prime_index_density'].mean() * 1.2:\n","        print(\"   â€¢ Elevated prime number correlations\")\n","    if abs(real_results_df['entropy'].mean() - 1.0) < abs(synthetic_results_df['entropy'].mean() - 1.0):\n","        print(\"   â€¢ Near-maximal entropy (edge of chaos)\")\n","    if real_results_df['curvature_mean'].mean() > synthetic_results_df['curvature_mean'].mean() * 1.2:\n","        print(\"   â€¢ Complex entropy curvature dynamics\")\n","else:\n","    print(\"   â€¢ Analysis requires both real and synthetic data\")\n","\n","print(\"\\nðŸ”¬ This analysis suite is complete and ready for scientific review!\")\n","print(\"=\" * 70)\n","\n","# %%\n","# CELL 11: INTERACTIVE PLOTLY VISUALIZATION (BONUS)\n","\n","# Create interactive 3D visualization\n","fig = make_subplots(\n","    rows=1, cols=2,\n","    subplot_titles=('3D Pattern Space', 'Motif Distribution'),\n","    specs=[[{'type': 'scatter3d'}, {'type': 'bar'}]]\n",")\n","\n","# 3D scatter plot\n","if len(real_results_df) > 0:\n","    fig.add_trace(\n","        go.Scatter3d(\n","            x=real_results_df['entropy'],\n","            y=real_results_df['phi_motifs'],\n","            z=real_results_df['complexity'],\n","            mode='markers',\n","            marker=dict(\n","                size=8,\n","                color='red',\n","                opacity=0.8\n","            ),\n","            name='Real Volcanic',\n","            text=real_results_df['volcano'],\n","            hovertemplate='<b>%{text}</b><br>Entropy: %{x:.3f}<br>Ï†-Motifs: %{y}<br>Complexity: %{z:.3f}'\n","        ),\n","        row=1, col=1\n","    )\n","\n","if len(synthetic_results_df) > 0:\n","    fig.add_trace(\n","        go.Scatter3d(\n","            x=synthetic_results_df['entropy'],\n","            y=synthetic_results_df['phi_motifs'],\n","            z=synthetic_results_df['complexity'],\n","            mode='markers',\n","            marker=dict(\n","                size=5,\n","                color='blue',\n","                opacity=0.5\n","            ),\n","            name='Synthetic'\n","        ),\n","        row=1, col=1\n","    )\n","\n","# Bar plot for motif comparison\n","motif_types = ['phi_motifs', 'prime_motifs', 'symmetric_motifs', 'fractal_motifs']\n","if len(real_results_df) > 0 and len(synthetic_results_df) > 0:\n","    real_means = [real_results_df[m].mean() for m in motif_types]\n","    synth_means = [synthetic_results_df[m].mean() for m in motif_types]\n","\n","    fig.add_trace(\n","        go.Bar(name='Real', x=motif_types, y=real_means, marker_color='indianred'),\n","        row=1, col=2\n","    )\n","    fig.add_trace(\n","        go.Bar(name='Synthetic', x=motif_types, y=synth_means, marker_color='lightblue'),\n","        row=1, col=2\n","    )\n","\n","# Update layout\n","fig.update_layout(\n","    title_text=\"Interactive Volcanic Symbolic Analysis\",\n","    showlegend=True,\n","    height=600,\n","    scene=dict(\n","        xaxis_title='Entropy',\n","        yaxis_title='Ï†-Motifs',\n","        zaxis_title='Complexity'\n","    )\n",")\n","\n","fig.show()\n","\n","print(\"âœ… Interactive visualization complete!\")\n","print(\"ðŸŽ‰ VOLCANIC SYMBOLIC ANALYSIS SUITE FULLY OPERATIONAL! ðŸŽ‰\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"Tk3pi_zDo0xa","executionInfo":{"status":"error","timestamp":1754572505198,"user_tz":240,"elapsed":4560,"user":{"displayName":"Kate Huneke","userId":"12242479504218415499"}},"outputId":"ac06098c-5d28-4f50-8d8f-636ff0eff9fb"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'cwt' from 'scipy.signal' (/usr/local/lib/python3.11/dist-packages/scipy/signal/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1623678106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_filter1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorlet2\u001b[0m  \u001b[0;31m# Import cwt and morlet2 specifically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'cwt' from 'scipy.signal' (/usr/local/lib/python3.11/dist-packages/scipy/signal/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}